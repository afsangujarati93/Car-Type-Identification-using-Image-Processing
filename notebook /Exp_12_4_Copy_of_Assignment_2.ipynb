{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exp 12 4 Copy of Assignment 2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "f0ayvowQNK3C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Referred Material\n",
        "\n",
        "**-Loading and transforming data **\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "\n",
        "\n",
        "-**Intro to pytorch **\n",
        "\n",
        "https://medium.com/ml2vec/intro-to-pytorch-with-image-classification-on-a-fashion-clothes-dataset-e589682df0c5\n",
        "\n",
        "\n",
        "-**Image preprocessing over view: **\n",
        "\n",
        "https://becominghuman.ai/image-data-pre-processing-for-neural-networks-498289068258\n",
        " \n",
        " (*Try this*) https://pytorch.org/docs/stable/torchvision/transforms.html\n",
        " \n",
        " (*Try this*) https://jdhao.github.io/2017/11/06/resize-image-to-square-with-padding/\n",
        " \n",
        " -**List of things to try w.r.t pre-processing**\n",
        " \n",
        "\n",
        "*   Square images + batch size 10  (**Done**)\n",
        "*   Square images + bw + batch size 100 (**Done**)\n",
        "*   Square images + batch size 100  (**Done**)\n",
        "*   Random flips and rotation  (**Done**)\n",
        "*   Five crop images + batch size 100  (**Done**)\n",
        "*   Other transformation techniques  (**Done**)\n",
        "*   Without normalization  (**Done**)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "m29L6L_EYtQt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Mounting the google drive for loading Dataset sand saving other files"
      ]
    },
    {
      "metadata": {
        "id": "kiT0P1Zh0T4O",
        "colab_type": "code",
        "outputId": "2474217f-554b-4205-f8ce-ea3130e4e3e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8eJz_lmGZDF4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Installing and loading necessary modules"
      ]
    },
    {
      "metadata": {
        "id": "L5xPhzElgxSe",
        "colab_type": "code",
        "outputId": "9ab1c2f7-d1a7-46c7-fb22-fac46fc467f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install --no-cache-dir -I pillow\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "from skimage import transform\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import pandas as pd;\n",
        "import numpy as np;\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random;\n",
        "import math;\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import io, transform\n",
        "from IPython.display import clear_output, display\n",
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "import torchvision.transforms.functional as F\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import pickle"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.0.0)\n",
            "Collecting pillow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 2.9MB/s \n",
            "\u001b[?25hInstalling collected packages: pillow\n",
            "Successfully installed pillow-5.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aQHizK52OvDl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2e96b934-0077-410f-eb3f-b90c8d9510fd"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JMN54-l0Y2Zt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Loading Dataset to pandas dataframe and splitting it into train, test and validation sets"
      ]
    },
    {
      "metadata": {
        "id": "jpn74UIA1LG-",
        "colab_type": "code",
        "outputId": "03caad50-9359-4e87-8c98-302203de1e18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "cell_type": "code",
      "source": [
        "# with open('/content/gdrive/My Drive/Dal Masters/CSCI 6515 - ML for Big Data/Assignment 2/Data/train_corrected/train_cars.csv', 'r') as f:\n",
        "#   print(f.read())  \n",
        "\n",
        "file_dir = \"/content/gdrive/My Drive/Dal Masters/CSCI 6515 - ML for Big Data/Assignment 2/Data/train_corrected/\"\n",
        "img_dir = file_dir + \"train/\"\n",
        "sq_img_dir = file_dir + \"train_sq/\"\n",
        "file_name = file_dir + \"train_cars.csv\"\n",
        "sep_datasets = file_dir + \"sep datasets/\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_entire_dataset = pd.read_csv(file_name)\n",
        "\n",
        "print(df_entire_dataset.columns)\n",
        "unique_car_type = df_entire_dataset.target.unique()\n",
        "\n",
        "print(unique_car_type)\n",
        "\n",
        "unique_car_type_dict = {}\n",
        "df_entire_dataset[\"num_target\"] = df_entire_dataset[\"target\"]\n",
        "for index, per_car_type in enumerate(unique_car_type):\n",
        "  df_entire_dataset[\"num_target\"] = df_entire_dataset[\"num_target\"].replace(per_car_type, index)\n",
        "\n",
        "# print(df_entire_dataset)\n",
        "train_valid, test = train_test_split(df_entire_dataset, test_size=0.05, random_state =10, stratify=df_entire_dataset[\"num_target\"])\n",
        "train, valid = train_test_split(train_valid, test_size=0.05, random_state=10, stratify=train_valid[\"num_target\"])\n",
        "\n",
        "train.reset_index(inplace = True, drop=True)\n",
        "valid.reset_index(inplace = True, drop=True)\n",
        "test.reset_index(inplace = True, drop=True)\n",
        "\n",
        "train_data_file = sep_datasets + \"train_dataset.csv\"\n",
        "train.to_csv(train_data_file)\n",
        "valid_data_file = sep_datasets + \"valid_dataset.csv\"\n",
        "valid.to_csv(valid_data_file)\n",
        "test_data_file = sep_datasets + \"test_dataset.csv\"\n",
        "test.to_csv(test_data_file)\n",
        "\n",
        "\n",
        "print(df_entire_dataset.groupby(\"target\").size())\n",
        "# print(train.groupby(\"target\").size())\n",
        "# print(valid.groupby(\"target\").size())\n",
        "# print(test.groupby(\"target\").size())\n",
        "print(train.size)\n",
        "print(valid.size)\n",
        "print(test.size)\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['image_name', 'target'], dtype='object')\n",
            "['sedan' 'truck' 'dedicated agricultural vehicle' 'jeep' 'crane truck'\n",
            " 'prime mover' 'cement mixer' 'hatchback' 'minivan' 'pickup' 'van'\n",
            " 'light truck' 'bus' 'tanker' 'minibus']\n",
            "target\n",
            "bus                                 53\n",
            "cement mixer                        17\n",
            "crane truck                         16\n",
            "dedicated agricultural vehicle       5\n",
            "hatchback                         3080\n",
            "jeep                               865\n",
            "light truck                        164\n",
            "minibus                             25\n",
            "minivan                            586\n",
            "pickup                             435\n",
            "prime mover                         44\n",
            "sedan                             5783\n",
            "tanker                               3\n",
            "truck                              179\n",
            "van                                362\n",
            "dtype: int64\n",
            "31452\n",
            "1656\n",
            "1743\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pKm-1x3KZLsE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Class to load and transform the dataset"
      ]
    },
    {
      "metadata": {
        "id": "LRWlulvT6gB_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#referred from https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "class CarTypeDataset(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, pd_dataframe, root_dir, transform=None, sq_image = False, image_channel = \"RGB\", find_edges = False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pd_dataframe (dataframe): Pandas dataframe with the respectve data\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.cartype_frame = pd_dataframe\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.sq_image = sq_image\n",
        "        self.image_channel = image_channel\n",
        "        self.find_edges = find_edges\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.cartype_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.root_dir + self.cartype_frame.iloc[idx, 0]\n",
        "        # read the image which returns numerical transformation of image plot using plt.imshow\n",
        "        \n",
        "        image = io.imread(img_name)\n",
        "        actual_image = image \n",
        "        \n",
        "        if self.find_edges: \n",
        "          image = cv2.Canny(image,10,100, L2gradient= True)\n",
        "          \n",
        "        \n",
        "        pil_image = Image.fromarray(image)\n",
        "        \n",
        "        if self.sq_image: \n",
        "          pil_image = CarTypeDataset.make_square(pil_image)\n",
        "        \n",
        "#         if self.image_channel:\n",
        "        pil_image = pil_image.convert(self.image_channel)\n",
        "\n",
        "        image = pil_image\n",
        "        num_car_type = self.cartype_frame.iloc[idx, 2]\n",
        "        car_type = self.cartype_frame.iloc[idx, 1]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            \n",
        "        sample = {'image': image, 'label': num_car_type}\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "    def make_square(im, min_size=80, fill_color=(0, 0, 0, 0)):\n",
        "        x, y = im.size\n",
        "        min_size = x if x > y else y \n",
        "        size = max(min_size, x, y)\n",
        "        new_im = Image.new('RGB', (size, size), fill_color)\n",
        "        val_x = int((size - x) / 2)\n",
        "        val_y = int((size - y) / 2)\n",
        "\n",
        "        new_im.paste(im, (val_x, val_y))\n",
        "        return new_im"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z_dCHX5hXP-i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Calculating Mean and Standard Deviation across all the train images data\n",
        "\n",
        "(Outputs are commented below the print statements)"
      ]
    },
    {
      "metadata": {
        "id": "37RiX6XnU9Y2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# composed = transforms.Compose([\n",
        "#         transforms.ToTensor()\n",
        "#     ])\n",
        "\n",
        "\n",
        "# car_type_train = CarTypeDataset(pd_dataframe=train,\n",
        "#                                     root_dir=img_dir,\n",
        "#                                     transform = composed)\n",
        "\n",
        "\n",
        "# tensor_mean_list = []\n",
        "# tensor_std_list = []\n",
        "\n",
        "# for index in range(0,len(car_type_train)):\n",
        "# #   print(str(index) + \" of \" + str(len(car_type_train)))\n",
        "#   this_car_type = car_type_train[index]\n",
        "#   this_mean = this_car_type[\"image\"].mean(1).mean(1)\n",
        "#   this_std = this_car_type[\"image\"].std(1).std(1)\n",
        "#   if index % 1000 == 0:\n",
        "#     print(str(index) + \" of \" + str(len(car_type_train)))\n",
        "    \n",
        "#     print(this_mean)\n",
        "#     print(this_std)\n",
        "#   tensor_mean_list.append(this_mean)\n",
        "#   tensor_std_list.append(this_std)\n",
        "  \n",
        "# tensor_mean_tuple = tuple(tensor_mean_list)\n",
        "# tensor_std_tuple = tuple(tensor_std_list)\n",
        "\n",
        "# # print(tensor_mean_tuple)\n",
        "\n",
        "# image_means = torch.stack(tensor_mean_tuple)\n",
        "# print(image_means.mean(0))\n",
        "# # tensor([0.4961, 0.5154, 0.5685])\n",
        "\n",
        "# image_std = torch.stack(tensor_std_tuple)\n",
        "# print(image_std.mean(0))\n",
        "# # tensor([0.0538, 0.0556, 0.0510])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TSqClVOB3MBo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Edge Dectection Sample Code\n",
        "(The sample outputs are included in the report)"
      ]
    },
    {
      "metadata": {
        "id": "B3yhsGxR3JIJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import cv2\n",
        "# import numpy as np\n",
        "# from matplotlib import pyplot as plt\n",
        "# from random import * \n",
        "\n",
        "# def plot_edges(car_type):\n",
        "\n",
        "#   car_type_df = train.loc[train['target'] == car_type]\n",
        "#   car_img = car_type_df.iloc[randint(0,len(car_type_df))]\n",
        "#   image = car_img[\"image_name\"]\n",
        "#   img_name = img_dir + image\n",
        "#   io_image  = io.imread(img_name)\n",
        "#   print(type(io_image))\n",
        "\n",
        "#   edges_1 = cv2.Canny(io_image,10,100, L2gradient= True)\n",
        "\n",
        "#   plt.subplot(121),plt.imshow(edges_1)\n",
        "#   plt.title('Edge Image ' + car_type), plt.xticks([]), plt.yticks([])\n",
        "#   plt.subplot(122)\n",
        "#   plt.imshow(io_image)\n",
        "#   plt.title('Oriignal Image ' + car_type), plt.xticks([]), plt.yticks([])\n",
        "\n",
        "#   plt.show()\n",
        "  \n",
        "  \n",
        "# plot_edges(\"sedan\")\n",
        "# plot_edges(\"truck\")\n",
        "# plot_edges(\"jeep\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aB3dw_BUK1or",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model_extra_char = input(\"Enter experiment Number ..\")\n",
        "model_extra_char = \"12\"\n",
        "\n",
        "num_epochs = 300;\n",
        "batch_size = 10;\n",
        "learning_rate = 0.001;\n",
        "find_edges = False\n",
        "in_kernel_size = (3,2)\n",
        "hd_kernel_size = (3,2)\n",
        "neuron_count = 32\n",
        "max_neuron_count = 2\n",
        "cnv_count = 15\n",
        "full_count = 10\n",
        "\n",
        "sq_image = False\n",
        "acc_score = 0\n",
        "\n",
        "\n",
        "# model_extra_char = input(\"Enter that extra character to apply to the best model name\")\n",
        "model_save_path = file_dir + \"model_file_experiment_\"  + str(model_extra_char) +\".model\"\n",
        "losses_save_path = file_dir + \"losses/losses_file_experiment_\" + str(model_extra_char) +\".csv\"\n",
        "\n",
        "resize_height = 72\n",
        "resize_width = 30\n",
        "rotation_degree= 10\n",
        "#           transforms.RandomRotation(rotation_degree),\n",
        "if find_edges:\n",
        "  data_transform = transforms.Compose([\n",
        "          transforms.Resize((resize_height,resize_width)),\n",
        "          transforms.RandomHorizontalFlip(),\n",
        "          transforms.RandomVerticalFlip(),\n",
        "\n",
        "          transforms.ToTensor()\n",
        "\n",
        "      ])\n",
        "else:\n",
        "  data_transform = transforms.Compose([\n",
        "          transforms.Resize((resize_height,resize_width)),\n",
        "          transforms.RandomHorizontalFlip(),\n",
        "          transforms.RandomVerticalFlip(),\n",
        "\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize((0.4961, 0.5154, 0.5685), (0.0538, 0.0556, 0.0510))\n",
        "      ])\n",
        "\n",
        "\n",
        "car_type_train_norm = CarTypeDataset(pd_dataframe=train,\n",
        "                                    root_dir=img_dir,\n",
        "                                    transform = data_transform, \n",
        "                                    sq_image = sq_image, \n",
        "                                    find_edges = find_edges)\n",
        "\n",
        "\n",
        "dataset_loader = torch.utils.data.DataLoader(car_type_train_norm,\n",
        "                                             batch_size=batch_size, shuffle=True,\n",
        "                                             num_workers=12)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LzPN8EchhjzB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "          \n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, neuron_count, kernel_size=in_kernel_size, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(neuron_count),\n",
        "            nn.MaxPool2d(max_neuron_count))\n",
        "  \n",
        "        self.layer_hd = nn.Sequential(\n",
        "            nn.Conv2d(neuron_count, neuron_count, kernel_size=hd_kernel_size, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(neuron_count),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(max_neuron_count))\n",
        "\n",
        "        self.fcS = nn.Linear(192, neuron_count)\n",
        "        self.fc_c = nn.Linear(neuron_count, neuron_count)      \n",
        "        self.fcL = nn.Linear(neuron_count, 15)\n",
        "        \n",
        "    def forward(self, x, cnv_count= 1, full_count = 2):\n",
        "        out = self.layer1(x)     #1 \n",
        "        \n",
        "        for i in range(cnv_count-1):\n",
        "          out = self.layer_hd(out) #2\n",
        "        \n",
        "        out = out.view(out.size(0), -1)\n",
        "    \n",
        "        out = self.fcS(out)  #First\n",
        "        for i in range(full_count-2):\n",
        "          out = self.fc_c(out) #1\n",
        "\n",
        "        out = self.fcL(out)  #Last\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3uGZHD17hmVD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#instance of the Conv Net\n",
        "cnn = CNN();\n",
        "cnn.to(device)\n",
        "#loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss();\n",
        "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "16kZUe0Axb30",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def valid_score(acc_score, cnn, data_transform):\n",
        "  car_type_valid_norm = CarTypeDataset(pd_dataframe=valid,\n",
        "                                      root_dir=img_dir,\n",
        "                                      transform = data_transform,\n",
        "                                       sq_image = sq_image, \n",
        "                                      find_edges = find_edges\n",
        "                                      )\n",
        "\n",
        "  valid_loader = torch.utils.data.DataLoader(car_type_valid_norm,\n",
        "                                               batch_size=batch_size, shuffle=True,\n",
        "                                               num_workers=8)\n",
        "  cnn.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for i, this_loader in enumerate(valid_loader):\n",
        "      images = Variable(this_loader[\"image\"]).to(device)\n",
        "      outputs = cnn(images, cnv_count, full_count)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += this_loader[\"label\"].size(0)\n",
        "      correct += (predicted == this_loader[\"label\"].to(device)).sum()\n",
        "    \n",
        "  this_acc_score = (100 * correct / total)\n",
        "  if this_acc_score > acc_score:\n",
        "    acc_score = this_acc_score\n",
        "    torch.save(cnn, model_save_path)\n",
        "    print(\"Saved the model to \" + model_save_path)\n",
        "  print('Test Accuracy of the model on the %i test images: %.4f %%' % (len(car_type_valid_norm), (100 * correct / total)) )\n",
        "  return acc_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AGVb6d9-IvXm",
        "colab_type": "code",
        "outputId": "aa8f2ab8-62b6-446c-87fb-5ab43a251512",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5556
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Size Data loader\")\n",
        "print(len(dataset_loader))\n",
        "data_loader_len = len(dataset_loader)\n",
        "losses = [];\n",
        "for epoch in range(num_epochs):\n",
        "    if (epoch+1) % 5 == 0:\n",
        "      acc_score =  valid_score(acc_score, cnn, data_transform)\n",
        "      \n",
        "    for i, this_loader in enumerate(dataset_loader):\n",
        "        images = Variable(this_loader[\"image\"]).to(device)\n",
        "        labels = Variable(this_loader[\"label\"]).to(device)\n",
        "        \n",
        "        # Forward + Backward + Optimize\n",
        "        optimizer.zero_grad()\n",
        "        outputs = cnn(images, cnv_count, full_count)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.data.item());\n",
        "        \n",
        "        with open(losses_save_path, 'wb') as fp:\n",
        "          pickle.dump(losses, fp)\n",
        "        \n",
        "        if (i+1) % int(data_loader_len/3) == 0:\n",
        "            print ('Epoch : %d/%d, Iter : %d/%d,  Loss: %.4f' \n",
        "                   %(epoch+1, num_epochs, i+1, len(train)//batch_size, loss.data.item()))\n",
        "          \n",
        "    "
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size Data loader\n",
            "1049\n",
            "Epoch : 1/300, Iter : 349/1048,  Loss: 1.6792\n",
            "Epoch : 1/300, Iter : 698/1048,  Loss: 1.3063\n",
            "Epoch : 1/300, Iter : 1047/1048,  Loss: 0.9084\n",
            "Epoch : 2/300, Iter : 349/1048,  Loss: 1.8047\n",
            "Epoch : 2/300, Iter : 698/1048,  Loss: 1.6843\n",
            "Epoch : 2/300, Iter : 1047/1048,  Loss: 1.6580\n",
            "Epoch : 3/300, Iter : 349/1048,  Loss: 2.1661\n",
            "Epoch : 3/300, Iter : 698/1048,  Loss: 0.7523\n",
            "Epoch : 3/300, Iter : 1047/1048,  Loss: 0.7532\n",
            "Epoch : 4/300, Iter : 349/1048,  Loss: 1.2631\n",
            "Epoch : 4/300, Iter : 698/1048,  Loss: 1.0844\n",
            "Epoch : 4/300, Iter : 1047/1048,  Loss: 2.3533\n",
            "Saved the model to /content/gdrive/My Drive/Dal Masters/CSCI 6515 - ML for Big Data/Assignment 2/Data/train_corrected/model_file_experiment_12.model\n",
            "Test Accuracy of the model on the 552 test images: 40.0000 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:250: UserWarning: Couldn't retrieve source code for container of type CNN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch : 5/300, Iter : 349/1048,  Loss: 0.8182\n",
            "Epoch : 5/300, Iter : 698/1048,  Loss: 1.6885\n",
            "Epoch : 5/300, Iter : 1047/1048,  Loss: 1.3840\n",
            "Epoch : 6/300, Iter : 349/1048,  Loss: 1.3691\n",
            "Epoch : 6/300, Iter : 698/1048,  Loss: 1.4917\n",
            "Epoch : 6/300, Iter : 1047/1048,  Loss: 1.0823\n",
            "Epoch : 7/300, Iter : 349/1048,  Loss: 1.2123\n",
            "Epoch : 7/300, Iter : 698/1048,  Loss: 0.7635\n",
            "Epoch : 7/300, Iter : 1047/1048,  Loss: 1.4645\n",
            "Epoch : 8/300, Iter : 349/1048,  Loss: 0.5415\n",
            "Epoch : 8/300, Iter : 698/1048,  Loss: 0.9742\n",
            "Epoch : 8/300, Iter : 1047/1048,  Loss: 0.2937\n",
            "Epoch : 9/300, Iter : 349/1048,  Loss: 1.1368\n",
            "Epoch : 9/300, Iter : 698/1048,  Loss: 0.7667\n",
            "Epoch : 9/300, Iter : 1047/1048,  Loss: 1.2639\n",
            "Saved the model to /content/gdrive/My Drive/Dal Masters/CSCI 6515 - ML for Big Data/Assignment 2/Data/train_corrected/model_file_experiment_12.model\n",
            "Test Accuracy of the model on the 552 test images: 63.0000 %\n",
            "Epoch : 10/300, Iter : 349/1048,  Loss: 0.8886\n",
            "Epoch : 10/300, Iter : 698/1048,  Loss: 1.2880\n",
            "Epoch : 10/300, Iter : 1047/1048,  Loss: 0.7168\n",
            "Epoch : 11/300, Iter : 349/1048,  Loss: 0.9881\n",
            "Epoch : 11/300, Iter : 698/1048,  Loss: 1.4664\n",
            "Epoch : 11/300, Iter : 1047/1048,  Loss: 0.8204\n",
            "Epoch : 12/300, Iter : 349/1048,  Loss: 1.6121\n",
            "Epoch : 12/300, Iter : 698/1048,  Loss: 0.6779\n",
            "Epoch : 12/300, Iter : 1047/1048,  Loss: 1.2825\n",
            "Epoch : 13/300, Iter : 349/1048,  Loss: 1.5987\n",
            "Epoch : 13/300, Iter : 698/1048,  Loss: 1.7025\n",
            "Epoch : 13/300, Iter : 1047/1048,  Loss: 0.8044\n",
            "Epoch : 14/300, Iter : 349/1048,  Loss: 0.9877\n",
            "Epoch : 14/300, Iter : 698/1048,  Loss: 1.5733\n",
            "Epoch : 14/300, Iter : 1047/1048,  Loss: 0.9430\n",
            "Test Accuracy of the model on the 552 test images: 63.0000 %\n",
            "Epoch : 15/300, Iter : 349/1048,  Loss: 1.4913\n",
            "Epoch : 15/300, Iter : 698/1048,  Loss: 0.9694\n",
            "Epoch : 15/300, Iter : 1047/1048,  Loss: 0.8552\n",
            "Epoch : 16/300, Iter : 349/1048,  Loss: 0.7234\n",
            "Epoch : 16/300, Iter : 698/1048,  Loss: 1.7071\n",
            "Epoch : 16/300, Iter : 1047/1048,  Loss: 1.2218\n",
            "Epoch : 17/300, Iter : 349/1048,  Loss: 0.7175\n",
            "Epoch : 17/300, Iter : 698/1048,  Loss: 1.5520\n",
            "Epoch : 17/300, Iter : 1047/1048,  Loss: 0.2677\n",
            "Epoch : 18/300, Iter : 349/1048,  Loss: 1.2434\n",
            "Epoch : 18/300, Iter : 698/1048,  Loss: 0.6983\n",
            "Epoch : 18/300, Iter : 1047/1048,  Loss: 1.4112\n",
            "Epoch : 19/300, Iter : 349/1048,  Loss: 0.6014\n",
            "Epoch : 19/300, Iter : 698/1048,  Loss: 1.1777\n",
            "Epoch : 19/300, Iter : 1047/1048,  Loss: 0.5079\n",
            "Saved the model to /content/gdrive/My Drive/Dal Masters/CSCI 6515 - ML for Big Data/Assignment 2/Data/train_corrected/model_file_experiment_12.model\n",
            "Test Accuracy of the model on the 552 test images: 64.0000 %\n",
            "Epoch : 20/300, Iter : 349/1048,  Loss: 0.6878\n",
            "Epoch : 20/300, Iter : 698/1048,  Loss: 1.8102\n",
            "Epoch : 20/300, Iter : 1047/1048,  Loss: 0.6621\n",
            "Epoch : 21/300, Iter : 349/1048,  Loss: 1.2319\n",
            "Epoch : 21/300, Iter : 698/1048,  Loss: 1.3530\n",
            "Epoch : 21/300, Iter : 1047/1048,  Loss: 1.1288\n",
            "Epoch : 22/300, Iter : 349/1048,  Loss: 0.7459\n",
            "Epoch : 22/300, Iter : 698/1048,  Loss: 0.8349\n",
            "Epoch : 22/300, Iter : 1047/1048,  Loss: 0.8603\n",
            "Epoch : 23/300, Iter : 349/1048,  Loss: 1.1691\n",
            "Epoch : 23/300, Iter : 698/1048,  Loss: 1.1003\n",
            "Epoch : 23/300, Iter : 1047/1048,  Loss: 1.9987\n",
            "Epoch : 24/300, Iter : 349/1048,  Loss: 1.1574\n",
            "Epoch : 24/300, Iter : 698/1048,  Loss: 0.4596\n",
            "Epoch : 24/300, Iter : 1047/1048,  Loss: 0.8094\n",
            "Saved the model to /content/gdrive/My Drive/Dal Masters/CSCI 6515 - ML for Big Data/Assignment 2/Data/train_corrected/model_file_experiment_12.model\n",
            "Test Accuracy of the model on the 552 test images: 65.0000 %\n",
            "Epoch : 25/300, Iter : 349/1048,  Loss: 1.8253\n",
            "Epoch : 25/300, Iter : 698/1048,  Loss: 0.8018\n",
            "Epoch : 25/300, Iter : 1047/1048,  Loss: 0.8694\n",
            "Epoch : 26/300, Iter : 349/1048,  Loss: 0.7847\n",
            "Epoch : 26/300, Iter : 698/1048,  Loss: 1.0940\n",
            "Epoch : 26/300, Iter : 1047/1048,  Loss: 0.7799\n",
            "Epoch : 27/300, Iter : 349/1048,  Loss: 0.5468\n",
            "Epoch : 27/300, Iter : 698/1048,  Loss: 0.3865\n",
            "Epoch : 27/300, Iter : 1047/1048,  Loss: 1.8688\n",
            "Epoch : 28/300, Iter : 349/1048,  Loss: 0.8587\n",
            "Epoch : 28/300, Iter : 698/1048,  Loss: 1.1969\n",
            "Epoch : 28/300, Iter : 1047/1048,  Loss: 0.8638\n",
            "Epoch : 29/300, Iter : 349/1048,  Loss: 0.7615\n",
            "Epoch : 29/300, Iter : 698/1048,  Loss: 0.9000\n",
            "Epoch : 29/300, Iter : 1047/1048,  Loss: 1.3517\n",
            "Test Accuracy of the model on the 552 test images: 63.0000 %\n",
            "Epoch : 30/300, Iter : 349/1048,  Loss: 1.4918\n",
            "Epoch : 30/300, Iter : 698/1048,  Loss: 1.1127\n",
            "Epoch : 30/300, Iter : 1047/1048,  Loss: 0.5120\n",
            "Epoch : 31/300, Iter : 349/1048,  Loss: 1.8018\n",
            "Epoch : 31/300, Iter : 698/1048,  Loss: 0.6584\n",
            "Epoch : 31/300, Iter : 1047/1048,  Loss: 0.7588\n",
            "Epoch : 32/300, Iter : 349/1048,  Loss: 1.0860\n",
            "Epoch : 32/300, Iter : 698/1048,  Loss: 0.7997\n",
            "Epoch : 32/300, Iter : 1047/1048,  Loss: 0.9554\n",
            "Epoch : 33/300, Iter : 349/1048,  Loss: 1.1247\n",
            "Epoch : 33/300, Iter : 698/1048,  Loss: 0.3508\n",
            "Epoch : 33/300, Iter : 1047/1048,  Loss: 0.9397\n",
            "Epoch : 34/300, Iter : 349/1048,  Loss: 1.0734\n",
            "Epoch : 34/300, Iter : 698/1048,  Loss: 1.3417\n",
            "Epoch : 34/300, Iter : 1047/1048,  Loss: 1.1226\n",
            "Test Accuracy of the model on the 552 test images: 65.0000 %\n",
            "Epoch : 35/300, Iter : 349/1048,  Loss: 1.2966\n",
            "Epoch : 35/300, Iter : 698/1048,  Loss: 0.8776\n",
            "Epoch : 35/300, Iter : 1047/1048,  Loss: 0.8442\n",
            "Epoch : 36/300, Iter : 349/1048,  Loss: 0.9939\n",
            "Epoch : 36/300, Iter : 698/1048,  Loss: 0.6482\n",
            "Epoch : 36/300, Iter : 1047/1048,  Loss: 0.3776\n",
            "Epoch : 37/300, Iter : 349/1048,  Loss: 1.2650\n",
            "Epoch : 37/300, Iter : 698/1048,  Loss: 1.1606\n",
            "Epoch : 37/300, Iter : 1047/1048,  Loss: 0.3463\n",
            "Epoch : 38/300, Iter : 349/1048,  Loss: 1.5627\n",
            "Epoch : 38/300, Iter : 698/1048,  Loss: 1.1175\n",
            "Epoch : 38/300, Iter : 1047/1048,  Loss: 0.5994\n",
            "Epoch : 39/300, Iter : 349/1048,  Loss: 1.2113\n",
            "Epoch : 39/300, Iter : 698/1048,  Loss: 1.0861\n",
            "Epoch : 39/300, Iter : 1047/1048,  Loss: 1.2514\n",
            "Test Accuracy of the model on the 552 test images: 64.0000 %\n",
            "Epoch : 40/300, Iter : 349/1048,  Loss: 0.7328\n",
            "Epoch : 40/300, Iter : 698/1048,  Loss: 0.7412\n",
            "Epoch : 40/300, Iter : 1047/1048,  Loss: 0.8103\n",
            "Epoch : 41/300, Iter : 349/1048,  Loss: 0.6039\n",
            "Epoch : 41/300, Iter : 698/1048,  Loss: 1.0032\n",
            "Epoch : 41/300, Iter : 1047/1048,  Loss: 0.6519\n",
            "Epoch : 42/300, Iter : 349/1048,  Loss: 0.5893\n",
            "Epoch : 42/300, Iter : 698/1048,  Loss: 0.5746\n",
            "Epoch : 42/300, Iter : 1047/1048,  Loss: 1.1525\n",
            "Epoch : 43/300, Iter : 349/1048,  Loss: 0.6915\n",
            "Epoch : 43/300, Iter : 698/1048,  Loss: 1.6996\n",
            "Epoch : 43/300, Iter : 1047/1048,  Loss: 0.5712\n",
            "Epoch : 44/300, Iter : 349/1048,  Loss: 0.8406\n",
            "Epoch : 44/300, Iter : 698/1048,  Loss: 1.3949\n",
            "Epoch : 44/300, Iter : 1047/1048,  Loss: 0.6528\n",
            "Test Accuracy of the model on the 552 test images: 64.0000 %\n",
            "Epoch : 45/300, Iter : 349/1048,  Loss: 0.8753\n",
            "Epoch : 45/300, Iter : 698/1048,  Loss: 0.6741\n",
            "Epoch : 45/300, Iter : 1047/1048,  Loss: 0.6917\n",
            "Epoch : 46/300, Iter : 349/1048,  Loss: 0.4674\n",
            "Epoch : 46/300, Iter : 698/1048,  Loss: 1.0404\n",
            "Epoch : 46/300, Iter : 1047/1048,  Loss: 1.0965\n",
            "Epoch : 47/300, Iter : 349/1048,  Loss: 0.9601\n",
            "Epoch : 47/300, Iter : 698/1048,  Loss: 1.1471\n",
            "Epoch : 47/300, Iter : 1047/1048,  Loss: 0.8181\n",
            "Epoch : 48/300, Iter : 349/1048,  Loss: 0.4644\n",
            "Epoch : 48/300, Iter : 698/1048,  Loss: 0.5118\n",
            "Epoch : 48/300, Iter : 1047/1048,  Loss: 1.2417\n",
            "Epoch : 49/300, Iter : 349/1048,  Loss: 0.7091\n",
            "Epoch : 49/300, Iter : 698/1048,  Loss: 1.2049\n",
            "Epoch : 49/300, Iter : 1047/1048,  Loss: 0.8572\n",
            "Saved the model to /content/gdrive/My Drive/Dal Masters/CSCI 6515 - ML for Big Data/Assignment 2/Data/train_corrected/model_file_experiment_12.model\n",
            "Test Accuracy of the model on the 552 test images: 66.0000 %\n",
            "Epoch : 50/300, Iter : 349/1048,  Loss: 0.5025\n",
            "Epoch : 50/300, Iter : 698/1048,  Loss: 0.6866\n",
            "Epoch : 50/300, Iter : 1047/1048,  Loss: 0.8098\n",
            "Epoch : 51/300, Iter : 349/1048,  Loss: 1.3880\n",
            "Epoch : 51/300, Iter : 698/1048,  Loss: 1.0774\n",
            "Epoch : 51/300, Iter : 1047/1048,  Loss: 0.5671\n",
            "Epoch : 52/300, Iter : 349/1048,  Loss: 0.8522\n",
            "Epoch : 52/300, Iter : 698/1048,  Loss: 0.9787\n",
            "Epoch : 52/300, Iter : 1047/1048,  Loss: 1.1686\n",
            "Epoch : 53/300, Iter : 349/1048,  Loss: 1.4748\n",
            "Epoch : 53/300, Iter : 698/1048,  Loss: 0.5932\n",
            "Epoch : 53/300, Iter : 1047/1048,  Loss: 0.6052\n",
            "Epoch : 54/300, Iter : 349/1048,  Loss: 0.5482\n",
            "Epoch : 54/300, Iter : 698/1048,  Loss: 1.2054\n",
            "Epoch : 54/300, Iter : 1047/1048,  Loss: 0.5016\n",
            "Saved the model to /content/gdrive/My Drive/Dal Masters/CSCI 6515 - ML for Big Data/Assignment 2/Data/train_corrected/model_file_experiment_12.model\n",
            "Test Accuracy of the model on the 552 test images: 68.0000 %\n",
            "Epoch : 55/300, Iter : 349/1048,  Loss: 1.3246\n",
            "Epoch : 55/300, Iter : 698/1048,  Loss: 0.5838\n",
            "Epoch : 55/300, Iter : 1047/1048,  Loss: 0.8192\n",
            "Epoch : 56/300, Iter : 349/1048,  Loss: 0.9900\n",
            "Epoch : 56/300, Iter : 698/1048,  Loss: 0.7536\n",
            "Epoch : 56/300, Iter : 1047/1048,  Loss: 0.6537\n",
            "Epoch : 57/300, Iter : 349/1048,  Loss: 0.9607\n",
            "Epoch : 57/300, Iter : 698/1048,  Loss: 0.7525\n",
            "Epoch : 57/300, Iter : 1047/1048,  Loss: 0.3897\n",
            "Epoch : 58/300, Iter : 349/1048,  Loss: 0.6289\n",
            "Epoch : 58/300, Iter : 698/1048,  Loss: 0.7594\n",
            "Epoch : 58/300, Iter : 1047/1048,  Loss: 0.9541\n",
            "Epoch : 59/300, Iter : 349/1048,  Loss: 1.6010\n",
            "Epoch : 59/300, Iter : 698/1048,  Loss: 1.0458\n",
            "Epoch : 59/300, Iter : 1047/1048,  Loss: 1.7776\n",
            "Test Accuracy of the model on the 552 test images: 66.0000 %\n",
            "Epoch : 60/300, Iter : 349/1048,  Loss: 0.6278\n",
            "Epoch : 60/300, Iter : 698/1048,  Loss: 0.5498\n",
            "Epoch : 60/300, Iter : 1047/1048,  Loss: 0.5586\n",
            "Epoch : 61/300, Iter : 349/1048,  Loss: 0.9428\n",
            "Epoch : 61/300, Iter : 698/1048,  Loss: 0.8530\n",
            "Epoch : 61/300, Iter : 1047/1048,  Loss: 0.9558\n",
            "Epoch : 62/300, Iter : 349/1048,  Loss: 1.3724\n",
            "Epoch : 62/300, Iter : 698/1048,  Loss: 0.5173\n",
            "Epoch : 62/300, Iter : 1047/1048,  Loss: 0.5878\n",
            "Epoch : 63/300, Iter : 349/1048,  Loss: 1.3583\n",
            "Epoch : 63/300, Iter : 698/1048,  Loss: 0.9707\n",
            "Epoch : 63/300, Iter : 1047/1048,  Loss: 1.3734\n",
            "Epoch : 64/300, Iter : 349/1048,  Loss: 1.1546\n",
            "Epoch : 64/300, Iter : 698/1048,  Loss: 0.7549\n",
            "Epoch : 64/300, Iter : 1047/1048,  Loss: 0.8181\n",
            "Saved the model to /content/gdrive/My Drive/Dal Masters/CSCI 6515 - ML for Big Data/Assignment 2/Data/train_corrected/model_file_experiment_12.model\n",
            "Test Accuracy of the model on the 552 test images: 69.0000 %\n",
            "Epoch : 65/300, Iter : 349/1048,  Loss: 0.7688\n",
            "Epoch : 65/300, Iter : 698/1048,  Loss: 1.3634\n",
            "Epoch : 65/300, Iter : 1047/1048,  Loss: 0.5517\n",
            "Epoch : 66/300, Iter : 349/1048,  Loss: 0.7796\n",
            "Epoch : 66/300, Iter : 698/1048,  Loss: 0.6137\n",
            "Epoch : 66/300, Iter : 1047/1048,  Loss: 0.6621\n",
            "Epoch : 67/300, Iter : 349/1048,  Loss: 0.7656\n",
            "Epoch : 67/300, Iter : 698/1048,  Loss: 0.8353\n",
            "Epoch : 67/300, Iter : 1047/1048,  Loss: 0.4773\n",
            "Epoch : 68/300, Iter : 349/1048,  Loss: 0.2196\n",
            "Epoch : 68/300, Iter : 698/1048,  Loss: 1.4873\n",
            "Epoch : 68/300, Iter : 1047/1048,  Loss: 0.8544\n",
            "Epoch : 69/300, Iter : 349/1048,  Loss: 1.2856\n",
            "Epoch : 69/300, Iter : 698/1048,  Loss: 0.6650\n",
            "Epoch : 69/300, Iter : 1047/1048,  Loss: 0.2406\n",
            "Test Accuracy of the model on the 552 test images: 68.0000 %\n",
            "Epoch : 70/300, Iter : 349/1048,  Loss: 0.4835\n",
            "Epoch : 70/300, Iter : 698/1048,  Loss: 0.6988\n",
            "Epoch : 70/300, Iter : 1047/1048,  Loss: 1.4027\n",
            "Epoch : 71/300, Iter : 349/1048,  Loss: 0.6255\n",
            "Epoch : 71/300, Iter : 698/1048,  Loss: 0.2759\n",
            "Epoch : 71/300, Iter : 1047/1048,  Loss: 0.5832\n",
            "Epoch : 72/300, Iter : 349/1048,  Loss: 0.9365\n",
            "Epoch : 72/300, Iter : 698/1048,  Loss: 0.2519\n",
            "Epoch : 72/300, Iter : 1047/1048,  Loss: 0.4845\n",
            "Epoch : 73/300, Iter : 349/1048,  Loss: 0.6461\n",
            "Epoch : 73/300, Iter : 698/1048,  Loss: 0.8313\n",
            "Epoch : 73/300, Iter : 1047/1048,  Loss: 0.2876\n",
            "Epoch : 74/300, Iter : 349/1048,  Loss: 0.9654\n",
            "Epoch : 74/300, Iter : 698/1048,  Loss: 0.9491\n",
            "Epoch : 74/300, Iter : 1047/1048,  Loss: 1.7804\n",
            "Test Accuracy of the model on the 552 test images: 68.0000 %\n",
            "Epoch : 75/300, Iter : 349/1048,  Loss: 0.7247\n",
            "Epoch : 75/300, Iter : 698/1048,  Loss: 0.3234\n",
            "Epoch : 75/300, Iter : 1047/1048,  Loss: 1.2309\n",
            "Epoch : 76/300, Iter : 349/1048,  Loss: 0.6899\n",
            "Epoch : 76/300, Iter : 698/1048,  Loss: 0.8803\n",
            "Epoch : 76/300, Iter : 1047/1048,  Loss: 0.7664\n",
            "Epoch : 77/300, Iter : 349/1048,  Loss: 0.7546\n",
            "Epoch : 77/300, Iter : 698/1048,  Loss: 0.8931\n",
            "Epoch : 77/300, Iter : 1047/1048,  Loss: 1.0051\n",
            "Epoch : 78/300, Iter : 349/1048,  Loss: 0.7714\n",
            "Epoch : 78/300, Iter : 698/1048,  Loss: 0.7949\n",
            "Epoch : 78/300, Iter : 1047/1048,  Loss: 0.9499\n",
            "Epoch : 79/300, Iter : 349/1048,  Loss: 0.7517\n",
            "Epoch : 79/300, Iter : 698/1048,  Loss: 1.2099\n",
            "Epoch : 79/300, Iter : 1047/1048,  Loss: 0.8846\n",
            "Test Accuracy of the model on the 552 test images: 67.0000 %\n",
            "Epoch : 80/300, Iter : 349/1048,  Loss: 0.6866\n",
            "Epoch : 80/300, Iter : 698/1048,  Loss: 1.2795\n",
            "Epoch : 80/300, Iter : 1047/1048,  Loss: 1.2802\n",
            "Epoch : 81/300, Iter : 349/1048,  Loss: 0.6741\n",
            "Epoch : 81/300, Iter : 698/1048,  Loss: 0.4793\n",
            "Epoch : 81/300, Iter : 1047/1048,  Loss: 0.3420\n",
            "Epoch : 82/300, Iter : 349/1048,  Loss: 0.5106\n",
            "Epoch : 82/300, Iter : 698/1048,  Loss: 1.1332\n",
            "Epoch : 82/300, Iter : 1047/1048,  Loss: 0.5393\n",
            "Epoch : 83/300, Iter : 349/1048,  Loss: 0.7129\n",
            "Epoch : 83/300, Iter : 698/1048,  Loss: 1.0617\n",
            "Epoch : 83/300, Iter : 1047/1048,  Loss: 0.9578\n",
            "Epoch : 84/300, Iter : 349/1048,  Loss: 1.0967\n",
            "Epoch : 84/300, Iter : 698/1048,  Loss: 1.0722\n",
            "Epoch : 84/300, Iter : 1047/1048,  Loss: 0.9935\n",
            "Test Accuracy of the model on the 552 test images: 68.0000 %\n",
            "Epoch : 85/300, Iter : 349/1048,  Loss: 0.8822\n",
            "Epoch : 85/300, Iter : 698/1048,  Loss: 0.9767\n",
            "Epoch : 85/300, Iter : 1047/1048,  Loss: 0.6591\n",
            "Epoch : 86/300, Iter : 349/1048,  Loss: 0.4587\n",
            "Epoch : 86/300, Iter : 698/1048,  Loss: 0.8670\n",
            "Epoch : 86/300, Iter : 1047/1048,  Loss: 0.7839\n",
            "Epoch : 87/300, Iter : 349/1048,  Loss: 0.4237\n",
            "Epoch : 87/300, Iter : 698/1048,  Loss: 0.4957\n",
            "Epoch : 87/300, Iter : 1047/1048,  Loss: 0.5330\n",
            "Epoch : 88/300, Iter : 349/1048,  Loss: 0.8057\n",
            "Epoch : 88/300, Iter : 698/1048,  Loss: 0.8892\n",
            "Epoch : 88/300, Iter : 1047/1048,  Loss: 0.7968\n",
            "Epoch : 89/300, Iter : 349/1048,  Loss: 0.4053\n",
            "Epoch : 89/300, Iter : 698/1048,  Loss: 1.4245\n",
            "Epoch : 89/300, Iter : 1047/1048,  Loss: 0.5364\n",
            "Test Accuracy of the model on the 552 test images: 66.0000 %\n",
            "Epoch : 90/300, Iter : 349/1048,  Loss: 0.9023\n",
            "Epoch : 90/300, Iter : 698/1048,  Loss: 1.1386\n",
            "Epoch : 90/300, Iter : 1047/1048,  Loss: 0.6751\n",
            "Epoch : 91/300, Iter : 349/1048,  Loss: 1.3287\n",
            "Epoch : 91/300, Iter : 698/1048,  Loss: 0.7283\n",
            "Epoch : 91/300, Iter : 1047/1048,  Loss: 1.0602\n",
            "Epoch : 92/300, Iter : 349/1048,  Loss: 0.9781\n",
            "Epoch : 92/300, Iter : 698/1048,  Loss: 0.8184\n",
            "Epoch : 92/300, Iter : 1047/1048,  Loss: 0.8701\n",
            "Epoch : 93/300, Iter : 349/1048,  Loss: 1.0965\n",
            "Epoch : 93/300, Iter : 698/1048,  Loss: 1.3367\n",
            "Epoch : 93/300, Iter : 1047/1048,  Loss: 0.5027\n",
            "Epoch : 94/300, Iter : 349/1048,  Loss: 1.0684\n",
            "Epoch : 94/300, Iter : 698/1048,  Loss: 0.8471\n",
            "Epoch : 94/300, Iter : 1047/1048,  Loss: 0.6864\n",
            "Test Accuracy of the model on the 552 test images: 66.0000 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-cd2b174fe221>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses_save_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m           \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader_len\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "DSVlzWTMvgZO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model_save_path = file_dir + \"model_file_\" + str(find_edges) + \"_1.model\"\n",
        "model_save_path = file_dir + \"model_file_experiment\"  + str(model_extra_char) +\"_1.model\"\n",
        "torch.save(cnn, model_save_path)\n",
        "print(\"Saved the model to \" + model_save_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zUm4UsnDxt_E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "car_type_test = CarTypeDataset(pd_dataframe=test,\n",
        "                                    root_dir=img_dir,\n",
        "                                    transform = data_transform,\n",
        "                                     sq_image = sq_image, \n",
        "                                    find_edges = find_edges\n",
        "                                    )\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(car_type_test,\n",
        "                                             batch_size=batch_size, shuffle=True,\n",
        "                                             num_workers=4)\n",
        "\n",
        "cnn.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "for i, this_loader in enumerate(test_loader):\n",
        "    images = Variable(this_loader[\"image\"])\n",
        "    outputs = cnn(images)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += this_loader[\"label\"].size(0)\n",
        "    correct += (predicted == this_loader[\"label\"]).sum()\n",
        "print('Test Accuracy of the model on the 10000 test images: %.4f %%' % (100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "usEoTQQA0j-v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "outputId": "ed1eb798-ff74-412a-91b8-c0bf77861f69"
      },
      "cell_type": "code",
      "source": [
        "print(len(losses))\n",
        "# losses_in_epochs = losses[0::60]\n",
        "losses_in_epochs = losses\n",
        "# plt.xkcd();\n",
        "plt.rcdefaults()\n",
        "plt.figure();\n",
        "plt.title(\"Experiment \" + str(model_extra_char))\n",
        "plt.xlabel('Iterations #');\n",
        "plt.ylabel('Loss');\n",
        "plt.plot(losses_in_epochs);\n",
        "plt.show();"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "98742\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAHHCAYAAABKudlQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n42aGMcYYY2btP7ApfVdsrTACAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fb5b437fef0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}