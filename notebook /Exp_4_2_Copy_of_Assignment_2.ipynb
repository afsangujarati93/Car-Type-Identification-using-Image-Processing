{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exp 4 2 Copy of Assignment 2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "f0ayvowQNK3C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Referred Material\n",
        "\n",
        "**-Loading and transforming data **\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "\n",
        "\n",
        "-**Intro to pytorch **\n",
        "\n",
        "https://medium.com/ml2vec/intro-to-pytorch-with-image-classification-on-a-fashion-clothes-dataset-e589682df0c5\n",
        "\n",
        "\n",
        "-**Image preprocessing over view: **\n",
        "\n",
        "https://becominghuman.ai/image-data-pre-processing-for-neural-networks-498289068258\n",
        " \n",
        " (*Try this*) https://pytorch.org/docs/stable/torchvision/transforms.html\n",
        " \n",
        " (*Try this*) https://jdhao.github.io/2017/11/06/resize-image-to-square-with-padding/\n",
        " \n",
        " -**List of things to try w.r.t pre-processing**\n",
        " \n",
        "\n",
        "*   Square images + batch size 10  (**Done**)\n",
        "*   Square images + bw + batch size 100 (**Done**)\n",
        "*   Square images + batch size 100  (**Done**)\n",
        "*   Random flips and rotation  (**Done**)\n",
        "*   Five crop images + batch size 100  (**Done**)\n",
        "*   Other transformation techniques  (**Done**)\n",
        "*   Without normalization  (**Done**)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "5Srdi9daJoSs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # memory footprint support libraries/code\n",
        "# !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "# !pip install gputil\n",
        "# !pip install psutil\n",
        "# !pip install humanize\n",
        "# import psutil\n",
        "# import humanize\n",
        "# import os\n",
        "# import GPUtil as GPU\n",
        "# GPUs = GPU.getGPUs()\n",
        "# # XXX: only one GPU on Colab and isn’t guaranteed\n",
        "# gpu = GPUs[0]\n",
        "# def printm():\n",
        "#  process = psutil.Process(os.getpid())\n",
        "#  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "#  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "# printm()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m29L6L_EYtQt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Mounting the google drive for loading Dataset sand saving other files"
      ]
    },
    {
      "metadata": {
        "id": "kiT0P1Zh0T4O",
        "colab_type": "code",
        "outputId": "e0d4c90c-e059-4e64-c791-0cd86eacf4a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8eJz_lmGZDF4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Installing and loading necessary modules"
      ]
    },
    {
      "metadata": {
        "id": "L5xPhzElgxSe",
        "colab_type": "code",
        "outputId": "6a37d54f-0219-4fd8-9fed-94780ca1651f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install --no-cache-dir -I pillow\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "from skimage import transform\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import pandas as pd;\n",
        "import numpy as np;\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random;\n",
        "import math;\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import io, transform\n",
        "from IPython.display import clear_output, display\n",
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "import torchvision.transforms.functional as F\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import pickle"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.0.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Collecting pillow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 32.0MB/s \n",
            "\u001b[?25hInstalling collected packages: pillow\n",
            "Successfully installed pillow-5.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0Ozy7Q_dKAQk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "08779a22-e387-4c2f-dfa1-e939a636623e"
      },
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JMN54-l0Y2Zt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Loading Dataset to pandas dataframe and splitting it into train, test and validation sets"
      ]
    },
    {
      "metadata": {
        "id": "jpn74UIA1LG-",
        "colab_type": "code",
        "outputId": "1c6c1f36-751c-4577-d79c-69169480081b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "cell_type": "code",
      "source": [
        "# with open('/content/gdrive/My Drive/Dal Masters/CSCI 6515 - ML for Big Data/Assignment 2/Data/train_corrected/train_cars.csv', 'r') as f:\n",
        "#   print(f.read())  \n",
        "\n",
        "file_dir = \"/content/gdrive/My Drive/Dal Masters/CSCI 6515 - ML for Big Data/Assignment 2/Data/train_corrected/\"\n",
        "img_dir = file_dir + \"train/\"\n",
        "sq_img_dir = file_dir + \"train_sq/\"\n",
        "file_name = file_dir + \"train_cars.csv\"\n",
        "sep_datasets = file_dir + \"sep datasets/\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_entire_dataset = pd.read_csv(file_name)\n",
        "\n",
        "print(df_entire_dataset.columns)\n",
        "unique_car_type = df_entire_dataset.target.unique()\n",
        "\n",
        "print(unique_car_type)\n",
        "\n",
        "unique_car_type_dict = {}\n",
        "df_entire_dataset[\"num_target\"] = df_entire_dataset[\"target\"]\n",
        "for index, per_car_type in enumerate(unique_car_type):\n",
        "  df_entire_dataset[\"num_target\"] = df_entire_dataset[\"num_target\"].replace(per_car_type, index)\n",
        "\n",
        "# print(df_entire_dataset)\n",
        "train_valid, test = train_test_split(df_entire_dataset, test_size=0.05, random_state =10, stratify=df_entire_dataset[\"num_target\"])\n",
        "train, valid = train_test_split(train_valid, test_size=0.05, random_state=10, stratify=train_valid[\"num_target\"])\n",
        "\n",
        "train.reset_index(inplace = True, drop=True)\n",
        "valid.reset_index(inplace = True, drop=True)\n",
        "test.reset_index(inplace = True, drop=True)\n",
        "\n",
        "train_data_file = sep_datasets + \"train_dataset.csv\"\n",
        "train.to_csv(train_data_file)\n",
        "valid_data_file = sep_datasets + \"valid_dataset.csv\"\n",
        "valid.to_csv(valid_data_file)\n",
        "test_data_file = sep_datasets + \"test_dataset.csv\"\n",
        "test.to_csv(test_data_file)\n",
        "\n",
        "\n",
        "print(df_entire_dataset.groupby(\"target\").size())\n",
        "# print(train.groupby(\"target\").size())\n",
        "# print(valid.groupby(\"target\").size())\n",
        "# print(test.groupby(\"target\").size())\n",
        "print(train.size)\n",
        "print(valid.size)\n",
        "print(test.size)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['image_name', 'target'], dtype='object')\n",
            "['sedan' 'truck' 'dedicated agricultural vehicle' 'jeep' 'crane truck'\n",
            " 'prime mover' 'cement mixer' 'hatchback' 'minivan' 'pickup' 'van'\n",
            " 'light truck' 'bus' 'tanker' 'minibus']\n",
            "target\n",
            "bus                                 53\n",
            "cement mixer                        17\n",
            "crane truck                         16\n",
            "dedicated agricultural vehicle       5\n",
            "hatchback                         3080\n",
            "jeep                               865\n",
            "light truck                        164\n",
            "minibus                             25\n",
            "minivan                            586\n",
            "pickup                             435\n",
            "prime mover                         44\n",
            "sedan                             5783\n",
            "tanker                               3\n",
            "truck                              179\n",
            "van                                362\n",
            "dtype: int64\n",
            "31452\n",
            "1656\n",
            "1743\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pKm-1x3KZLsE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Class to load and transform the dataset"
      ]
    },
    {
      "metadata": {
        "id": "LRWlulvT6gB_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#referred from https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "class CarTypeDataset(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, pd_dataframe, root_dir, transform=None, sq_image = False, image_channel = \"RGB\", find_edges = False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pd_dataframe (dataframe): Pandas dataframe with the respectve data\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.cartype_frame = pd_dataframe\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.sq_image = sq_image\n",
        "        self.image_channel = image_channel\n",
        "        self.find_edges = find_edges\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.cartype_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.root_dir + self.cartype_frame.iloc[idx, 0]\n",
        "        # read the image which returns numerical transformation of image plot using plt.imshow\n",
        "        \n",
        "        image = io.imread(img_name)\n",
        "        actual_image = image \n",
        "        \n",
        "        if self.find_edges: \n",
        "          image = cv2.Canny(image,10,100, L2gradient= True)\n",
        "          \n",
        "        \n",
        "        pil_image = Image.fromarray(image)\n",
        "        \n",
        "        if self.sq_image: \n",
        "          pil_image = CarTypeDataset.make_square(pil_image)\n",
        "        \n",
        "#         if self.image_channel:\n",
        "        pil_image = pil_image.convert(self.image_channel)\n",
        "\n",
        "        image = pil_image\n",
        "        num_car_type = self.cartype_frame.iloc[idx, 2]\n",
        "        car_type = self.cartype_frame.iloc[idx, 1]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            \n",
        "        sample = {'image': image, 'label': num_car_type}\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "    def make_square(im, min_size=80, fill_color=(0, 0, 0, 0)):\n",
        "        x, y = im.size\n",
        "        min_size = x if x > y else y \n",
        "        size = max(min_size, x, y)\n",
        "        new_im = Image.new('RGB', (size, size), fill_color)\n",
        "        val_x = int((size - x) / 2)\n",
        "        val_y = int((size - y) / 2)\n",
        "\n",
        "        new_im.paste(im, (val_x, val_y))\n",
        "        return new_im"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z_dCHX5hXP-i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Calculating Mean and Standard Deviation across all the train images data\n",
        "\n",
        "(Outputs are commented below the print statements)"
      ]
    },
    {
      "metadata": {
        "id": "37RiX6XnU9Y2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# composed = transforms.Compose([\n",
        "#         transforms.ToTensor()\n",
        "#     ])\n",
        "\n",
        "\n",
        "# car_type_train = CarTypeDataset(pd_dataframe=train,\n",
        "#                                     root_dir=img_dir,\n",
        "#                                     transform = composed)\n",
        "\n",
        "\n",
        "# tensor_mean_list = []\n",
        "# tensor_std_list = []\n",
        "\n",
        "# for index in range(0,len(car_type_train)):\n",
        "# #   print(str(index) + \" of \" + str(len(car_type_train)))\n",
        "#   this_car_type = car_type_train[index]\n",
        "#   this_mean = this_car_type[\"image\"].mean(1).mean(1)\n",
        "#   this_std = this_car_type[\"image\"].std(1).std(1)\n",
        "#   if index % 1000 == 0:\n",
        "#     print(str(index) + \" of \" + str(len(car_type_train)))\n",
        "    \n",
        "#     print(this_mean)\n",
        "#     print(this_std)\n",
        "#   tensor_mean_list.append(this_mean)\n",
        "#   tensor_std_list.append(this_std)\n",
        "  \n",
        "# tensor_mean_tuple = tuple(tensor_mean_list)\n",
        "# tensor_std_tuple = tuple(tensor_std_list)\n",
        "\n",
        "# # print(tensor_mean_tuple)\n",
        "\n",
        "# image_means = torch.stack(tensor_mean_tuple)\n",
        "# print(image_means.mean(0))\n",
        "# # tensor([0.4961, 0.5154, 0.5685])\n",
        "\n",
        "# image_std = torch.stack(tensor_std_tuple)\n",
        "# print(image_std.mean(0))\n",
        "# # tensor([0.0538, 0.0556, 0.0510])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TSqClVOB3MBo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Edge Dectection Sample Code\n",
        "(The sample outputs are included in the report)"
      ]
    },
    {
      "metadata": {
        "id": "B3yhsGxR3JIJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import cv2\n",
        "# import numpy as np\n",
        "# from matplotlib import pyplot as plt\n",
        "# from random import * \n",
        "\n",
        "# def plot_edges(car_type):\n",
        "\n",
        "#   car_type_df = train.loc[train['target'] == car_type]\n",
        "#   car_img = car_type_df.iloc[randint(0,len(car_type_df))]\n",
        "#   image = car_img[\"image_name\"]\n",
        "#   img_name = img_dir + image\n",
        "#   io_image  = io.imread(img_name)\n",
        "#   print(type(io_image))\n",
        "\n",
        "#   edges_1 = cv2.Canny(io_image,10,100, L2gradient= True)\n",
        "\n",
        "#   plt.subplot(121),plt.imshow(edges_1)\n",
        "#   plt.title('Edge Image ' + car_type), plt.xticks([]), plt.yticks([])\n",
        "#   plt.subplot(122)\n",
        "#   plt.imshow(io_image)\n",
        "#   plt.title('Oriignal Image ' + car_type), plt.xticks([]), plt.yticks([])\n",
        "\n",
        "#   plt.show()\n",
        "  \n",
        "  \n",
        "# plot_edges(\"sedan\")\n",
        "# plot_edges(\"truck\")\n",
        "# plot_edges(\"jeep\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aB3dw_BUK1or",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_epochs = 300;\n",
        "batch_size = 1;\n",
        "learning_rate = 0.001;\n",
        "find_edges = False\n",
        "kernel_size = (5,5)\n",
        "neuron_count = 32\n",
        "\n",
        "sq_image = False\n",
        "acc_score = 0\n",
        "model_extra_char = \"4\"\n",
        "# model_extra_char = input(\"Enter that extra character to apply to the best model name\")\n",
        "model_save_path = file_dir + \"model_file_experiment_\" + str(model_extra_char) +\".model\"\n",
        "losses_save_path = file_dir + \"losses/losses_file_experiment_\" + str(model_extra_char) +\".csv\"\n",
        "\n",
        "resize_height = 72\n",
        "resize_width = 30\n",
        "rotation_degree= 10\n",
        "\n",
        "if find_edges:\n",
        "  data_transform = transforms.Compose([\n",
        "          transforms.Resize((resize_height,resize_width)),\n",
        "          transforms.RandomHorizontalFlip(),\n",
        "          transforms.RandomVerticalFlip(),\n",
        "          transforms.RandomRotation(rotation_degree),\n",
        "          transforms.ToTensor()\n",
        "\n",
        "      ])\n",
        "else:\n",
        "  data_transform = transforms.Compose([\n",
        "          transforms.Resize((resize_height,resize_width)),\n",
        "          transforms.RandomHorizontalFlip(),\n",
        "          transforms.RandomVerticalFlip(),\n",
        "          transforms.RandomRotation(rotation_degree),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize((0.4961, 0.5154, 0.5685), (0.0538, 0.0556, 0.0510))\n",
        "      ])\n",
        "\n",
        "\n",
        "car_type_train_norm = CarTypeDataset(pd_dataframe=train,\n",
        "                                    root_dir=img_dir,\n",
        "                                    transform = data_transform, \n",
        "                                    sq_image = sq_image, \n",
        "                                    find_edges = find_edges)\n",
        "\n",
        "\n",
        "dataset_loader = torch.utils.data.DataLoader(car_type_train_norm,\n",
        "                                             batch_size=batch_size, shuffle=True,\n",
        "                                             num_workers=12)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LzPN8EchhjzB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "          \n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, neuron_count, kernel_size=kernel_size, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(neuron_count),\n",
        "            nn.MaxPool2d(2))\n",
        "  \n",
        "        self.layer_hd = nn.Sequential(\n",
        "            nn.Conv2d(neuron_count, neuron_count, kernel_size=kernel_size, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(neuron_count),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "\n",
        "        self.fcS = nn.Linear(64, neuron_count)\n",
        "        self.fc_c = nn.Linear(neuron_count, neuron_count)      \n",
        "        self.fcL = nn.Linear(neuron_count, 15)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)     #1 \n",
        "        out = self.layer_hd(out) #2\n",
        "        out = self.layer_hd(out) #3\n",
        "        out = self.layer_hd(out) #4\n",
        "        out = self.layer_hd(out) #5\n",
        "        \n",
        "\n",
        "        \n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fcS(out)  #First\n",
        "        out = self.fc_c(out) #1\n",
        "        out = self.fcL(out)  #Last\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3uGZHD17hmVD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#instance of the Conv Net\n",
        "cnn = CNN();\n",
        "cnn.to(device)\n",
        "#loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss();\n",
        "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "16kZUe0Axb30",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def valid_score(acc_score, cnn, data_transform):\n",
        "  car_type_valid_norm = CarTypeDataset(pd_dataframe=valid,\n",
        "                                      root_dir=img_dir,\n",
        "                                      transform = data_transform,\n",
        "                                       sq_image = sq_image, \n",
        "                                      find_edges = find_edges\n",
        "                                      )\n",
        "\n",
        "  valid_loader = torch.utils.data.DataLoader(car_type_valid_norm,\n",
        "                                               batch_size=batch_size, shuffle=True,\n",
        "                                               num_workers=8)\n",
        "  cnn.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for i, this_loader in enumerate(valid_loader):\n",
        "      images = Variable(this_loader[\"image\"].to(device))\n",
        "\n",
        "      outputs = cnn(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += this_loader[\"label\"].size(0)\n",
        "      correct += (predicted == this_loader[\"label\"].to(device)).sum()\n",
        "    \n",
        "  this_acc_score = (100 * correct / total)\n",
        "  if this_acc_score > acc_score:\n",
        "    acc_score = this_acc_score\n",
        "    torch.save(cnn, model_save_path)\n",
        "    print(\"Saved the model to \" + model_save_path)\n",
        "  print('Test Accuracy of the model on the %i test images: %.4f %%' % (len(car_type_valid_norm), (100 * correct / total)) )\n",
        "  return acc_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AGVb6d9-IvXm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 31243
        },
        "outputId": "c5f1883f-3a2f-42d0-f5a2-32ad4dd1efbf"
      },
      "cell_type": "code",
      "source": [
        "losses = [];\n",
        "for epoch in range(num_epochs):\n",
        "    if (epoch+1) % 5 == 0:\n",
        "      acc_score =  valid_score(acc_score, cnn, data_transform)\n",
        "      \n",
        "    for i, this_loader in enumerate(dataset_loader):\n",
        "        images = Variable(this_loader[\"image\"].to(device))\n",
        "        labels = Variable(this_loader[\"label\"].to(device))\n",
        "        \n",
        "        # Forward + Backward + Optimize\n",
        "        optimizer.zero_grad()\n",
        "        outputs = cnn(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.data.item());\n",
        "        with open(losses_save_path, 'wb') as fp:\n",
        "          pickle.dump(losses, fp)\n",
        "          \n",
        "        if (i+1) % 500 == 0:\n",
        "            print ('Epoch : %d/%d, Iter : %d/%d,  Loss: %.4f' \n",
        "                   %(epoch+1, num_epochs, i+1, len(train)//batch_size, loss.data.item()))\n",
        "          \n",
        "    "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : 1/300, Iter : 500/10484,  Loss: 0.9943\n",
            "Epoch : 1/300, Iter : 1000/10484,  Loss: 1.4415\n",
            "Epoch : 1/300, Iter : 1500/10484,  Loss: 0.8340\n",
            "Epoch : 1/300, Iter : 2000/10484,  Loss: 0.8880\n",
            "Epoch : 1/300, Iter : 2500/10484,  Loss: 0.9450\n",
            "Epoch : 1/300, Iter : 3000/10484,  Loss: 0.1252\n",
            "Epoch : 1/300, Iter : 3500/10484,  Loss: 0.9945\n",
            "Epoch : 1/300, Iter : 4000/10484,  Loss: 0.5936\n",
            "Epoch : 1/300, Iter : 4500/10484,  Loss: 0.2770\n",
            "Epoch : 1/300, Iter : 5000/10484,  Loss: 0.0475\n",
            "Epoch : 1/300, Iter : 5500/10484,  Loss: 0.6779\n",
            "Epoch : 1/300, Iter : 6000/10484,  Loss: 4.1608\n",
            "Epoch : 1/300, Iter : 6500/10484,  Loss: 0.7283\n",
            "Epoch : 1/300, Iter : 7000/10484,  Loss: 0.0234\n",
            "Epoch : 1/300, Iter : 7500/10484,  Loss: 0.3017\n",
            "Epoch : 1/300, Iter : 8000/10484,  Loss: 0.7415\n",
            "Epoch : 1/300, Iter : 8500/10484,  Loss: 0.2645\n",
            "Epoch : 1/300, Iter : 9000/10484,  Loss: 1.6315\n",
            "Epoch : 1/300, Iter : 9500/10484,  Loss: 0.0640\n",
            "Epoch : 1/300, Iter : 10000/10484,  Loss: 0.0453\n",
            "Epoch : 2/300, Iter : 500/10484,  Loss: 0.6262\n",
            "Epoch : 2/300, Iter : 1000/10484,  Loss: 0.5185\n",
            "Epoch : 2/300, Iter : 1500/10484,  Loss: 0.8382\n",
            "Epoch : 2/300, Iter : 2000/10484,  Loss: 2.3383\n",
            "Epoch : 2/300, Iter : 2500/10484,  Loss: 3.1815\n",
            "Epoch : 2/300, Iter : 3000/10484,  Loss: 0.7124\n",
            "Epoch : 2/300, Iter : 3500/10484,  Loss: 3.0144\n",
            "Epoch : 2/300, Iter : 4000/10484,  Loss: 0.0430\n",
            "Epoch : 2/300, Iter : 4500/10484,  Loss: 0.3055\n",
            "Epoch : 2/300, Iter : 5000/10484,  Loss: 0.0292\n",
            "Epoch : 2/300, Iter : 5500/10484,  Loss: 0.8944\n",
            "Epoch : 2/300, Iter : 6000/10484,  Loss: 1.8681\n",
            "Epoch : 2/300, Iter : 6500/10484,  Loss: 0.0223\n",
            "Epoch : 2/300, Iter : 7000/10484,  Loss: 0.9091\n",
            "Epoch : 2/300, Iter : 7500/10484,  Loss: 1.9655\n",
            "Epoch : 2/300, Iter : 8000/10484,  Loss: 2.0852\n",
            "Epoch : 2/300, Iter : 8500/10484,  Loss: 1.1558\n",
            "Epoch : 2/300, Iter : 9000/10484,  Loss: 0.0335\n",
            "Epoch : 2/300, Iter : 9500/10484,  Loss: 1.2116\n",
            "Epoch : 2/300, Iter : 10000/10484,  Loss: 0.7974\n",
            "Epoch : 3/300, Iter : 500/10484,  Loss: 0.0201\n",
            "Epoch : 3/300, Iter : 1000/10484,  Loss: 1.4986\n",
            "Epoch : 3/300, Iter : 1500/10484,  Loss: 0.2631\n",
            "Epoch : 3/300, Iter : 2000/10484,  Loss: 1.0255\n",
            "Epoch : 3/300, Iter : 2500/10484,  Loss: 0.0272\n",
            "Epoch : 3/300, Iter : 3000/10484,  Loss: 0.3146\n",
            "Epoch : 3/300, Iter : 3500/10484,  Loss: 1.8780\n",
            "Epoch : 3/300, Iter : 4000/10484,  Loss: 0.3666\n",
            "Epoch : 3/300, Iter : 4500/10484,  Loss: 0.3699\n",
            "Epoch : 3/300, Iter : 5000/10484,  Loss: 2.1790\n",
            "Epoch : 3/300, Iter : 5500/10484,  Loss: 3.5455\n",
            "Epoch : 3/300, Iter : 6000/10484,  Loss: 2.0983\n",
            "Epoch : 3/300, Iter : 6500/10484,  Loss: 0.1061\n",
            "Epoch : 3/300, Iter : 7000/10484,  Loss: 0.5854\n",
            "Epoch : 3/300, Iter : 7500/10484,  Loss: 0.2017\n",
            "Epoch : 3/300, Iter : 8000/10484,  Loss: 0.5050\n",
            "Epoch : 3/300, Iter : 8500/10484,  Loss: 0.5529\n",
            "Epoch : 3/300, Iter : 9000/10484,  Loss: 1.7734\n",
            "Epoch : 3/300, Iter : 9500/10484,  Loss: 0.6448\n",
            "Epoch : 3/300, Iter : 10000/10484,  Loss: 0.2635\n",
            "Epoch : 4/300, Iter : 500/10484,  Loss: 1.1389\n",
            "Epoch : 4/300, Iter : 1000/10484,  Loss: 0.0145\n",
            "Epoch : 4/300, Iter : 1500/10484,  Loss: 1.0752\n",
            "Epoch : 4/300, Iter : 2000/10484,  Loss: 0.3507\n",
            "Epoch : 4/300, Iter : 2500/10484,  Loss: 1.8819\n",
            "Epoch : 4/300, Iter : 3000/10484,  Loss: 0.2332\n",
            "Epoch : 4/300, Iter : 3500/10484,  Loss: 1.8273\n",
            "Epoch : 4/300, Iter : 4000/10484,  Loss: 0.5348\n",
            "Epoch : 4/300, Iter : 4500/10484,  Loss: 1.2286\n",
            "Epoch : 4/300, Iter : 5000/10484,  Loss: 0.4975\n",
            "Epoch : 4/300, Iter : 5500/10484,  Loss: 1.2155\n",
            "Epoch : 4/300, Iter : 6000/10484,  Loss: 0.2660\n",
            "Epoch : 4/300, Iter : 6500/10484,  Loss: 1.3069\n",
            "Epoch : 4/300, Iter : 7000/10484,  Loss: 0.1127\n",
            "Epoch : 4/300, Iter : 7500/10484,  Loss: 0.0282\n",
            "Epoch : 4/300, Iter : 8000/10484,  Loss: 0.0408\n",
            "Epoch : 4/300, Iter : 8500/10484,  Loss: 4.3882\n",
            "Epoch : 4/300, Iter : 9000/10484,  Loss: 0.4535\n",
            "Epoch : 4/300, Iter : 9500/10484,  Loss: 0.6938\n",
            "Epoch : 4/300, Iter : 10000/10484,  Loss: 1.3525\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:250: UserWarning: Couldn't retrieve source code for container of type CNN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saved the model to /content/gdrive/My Drive/Dal Masters/CSCI 6515 - ML for Big Data/Assignment 2/Data/train_corrected/model_file_experiment4.model\n",
            "Test Accuracy of the model on the 552 test images: 53.0000 %\n",
            "Epoch : 5/300, Iter : 500/10484,  Loss: 0.3451\n",
            "Epoch : 5/300, Iter : 1000/10484,  Loss: 2.9788\n",
            "Epoch : 5/300, Iter : 1500/10484,  Loss: 0.7164\n",
            "Epoch : 5/300, Iter : 2000/10484,  Loss: 0.8629\n",
            "Epoch : 5/300, Iter : 2500/10484,  Loss: 0.0051\n",
            "Epoch : 5/300, Iter : 3000/10484,  Loss: 0.0194\n",
            "Epoch : 5/300, Iter : 3500/10484,  Loss: 0.0008\n",
            "Epoch : 5/300, Iter : 4000/10484,  Loss: 0.6975\n",
            "Epoch : 5/300, Iter : 4500/10484,  Loss: 1.0285\n",
            "Epoch : 5/300, Iter : 5000/10484,  Loss: 0.0133\n",
            "Epoch : 5/300, Iter : 5500/10484,  Loss: 0.3173\n",
            "Epoch : 5/300, Iter : 6000/10484,  Loss: 0.0153\n",
            "Epoch : 5/300, Iter : 6500/10484,  Loss: 0.0168\n",
            "Epoch : 5/300, Iter : 7000/10484,  Loss: 0.0072\n",
            "Epoch : 5/300, Iter : 7500/10484,  Loss: 0.5422\n",
            "Epoch : 5/300, Iter : 8000/10484,  Loss: 2.2732\n",
            "Epoch : 5/300, Iter : 8500/10484,  Loss: 0.5687\n",
            "Epoch : 5/300, Iter : 9000/10484,  Loss: 0.5625\n",
            "Epoch : 5/300, Iter : 9500/10484,  Loss: 2.8823\n",
            "Epoch : 5/300, Iter : 10000/10484,  Loss: 1.4912\n",
            "Epoch : 6/300, Iter : 500/10484,  Loss: 2.0764\n",
            "Epoch : 6/300, Iter : 1000/10484,  Loss: 3.8349\n",
            "Epoch : 6/300, Iter : 1500/10484,  Loss: 0.1527\n",
            "Epoch : 6/300, Iter : 2000/10484,  Loss: 0.1678\n",
            "Epoch : 6/300, Iter : 2500/10484,  Loss: 2.2866\n",
            "Epoch : 6/300, Iter : 3000/10484,  Loss: 0.1042\n",
            "Epoch : 6/300, Iter : 3500/10484,  Loss: 0.1883\n",
            "Epoch : 6/300, Iter : 4000/10484,  Loss: 0.6150\n",
            "Epoch : 6/300, Iter : 4500/10484,  Loss: 0.0345\n",
            "Epoch : 6/300, Iter : 5000/10484,  Loss: 1.1573\n",
            "Epoch : 6/300, Iter : 5500/10484,  Loss: 2.3463\n",
            "Epoch : 6/300, Iter : 6000/10484,  Loss: 1.6467\n",
            "Epoch : 6/300, Iter : 6500/10484,  Loss: 0.0033\n",
            "Epoch : 6/300, Iter : 7000/10484,  Loss: 0.0257\n",
            "Epoch : 6/300, Iter : 7500/10484,  Loss: 1.9229\n",
            "Epoch : 6/300, Iter : 8000/10484,  Loss: 0.0004\n",
            "Epoch : 6/300, Iter : 8500/10484,  Loss: 1.1654\n",
            "Epoch : 6/300, Iter : 9000/10484,  Loss: 0.3429\n",
            "Epoch : 6/300, Iter : 9500/10484,  Loss: 2.5654\n",
            "Epoch : 6/300, Iter : 10000/10484,  Loss: 0.0701\n",
            "Epoch : 7/300, Iter : 500/10484,  Loss: 0.0053\n",
            "Epoch : 7/300, Iter : 1000/10484,  Loss: 0.0642\n",
            "Epoch : 7/300, Iter : 1500/10484,  Loss: 0.0910\n",
            "Epoch : 7/300, Iter : 2000/10484,  Loss: 0.3906\n",
            "Epoch : 7/300, Iter : 2500/10484,  Loss: 0.4854\n",
            "Epoch : 7/300, Iter : 3000/10484,  Loss: 0.5503\n",
            "Epoch : 7/300, Iter : 3500/10484,  Loss: 3.4021\n",
            "Epoch : 7/300, Iter : 4000/10484,  Loss: 0.8870\n",
            "Epoch : 7/300, Iter : 4500/10484,  Loss: 0.9967\n",
            "Epoch : 7/300, Iter : 5000/10484,  Loss: 0.0000\n",
            "Epoch : 7/300, Iter : 5500/10484,  Loss: 0.6414\n",
            "Epoch : 7/300, Iter : 6000/10484,  Loss: 0.5466\n",
            "Epoch : 7/300, Iter : 6500/10484,  Loss: 0.1106\n",
            "Epoch : 7/300, Iter : 7000/10484,  Loss: 0.0203\n",
            "Epoch : 7/300, Iter : 7500/10484,  Loss: 0.4139\n",
            "Epoch : 7/300, Iter : 8000/10484,  Loss: 0.6102\n",
            "Epoch : 7/300, Iter : 8500/10484,  Loss: 0.2066\n",
            "Epoch : 7/300, Iter : 9000/10484,  Loss: 0.6569\n",
            "Epoch : 7/300, Iter : 9500/10484,  Loss: 1.8369\n",
            "Epoch : 7/300, Iter : 10000/10484,  Loss: 1.1099\n",
            "Epoch : 8/300, Iter : 500/10484,  Loss: 1.6355\n",
            "Epoch : 8/300, Iter : 1000/10484,  Loss: 0.0875\n",
            "Epoch : 8/300, Iter : 1500/10484,  Loss: 0.9251\n",
            "Epoch : 8/300, Iter : 2000/10484,  Loss: 0.1096\n",
            "Epoch : 8/300, Iter : 2500/10484,  Loss: 6.2143\n",
            "Epoch : 8/300, Iter : 3000/10484,  Loss: 1.6787\n",
            "Epoch : 8/300, Iter : 3500/10484,  Loss: 1.3938\n",
            "Epoch : 8/300, Iter : 4000/10484,  Loss: 0.1217\n",
            "Epoch : 8/300, Iter : 4500/10484,  Loss: 0.0248\n",
            "Epoch : 8/300, Iter : 5000/10484,  Loss: 0.5017\n",
            "Epoch : 8/300, Iter : 5500/10484,  Loss: 0.0072\n",
            "Epoch : 8/300, Iter : 6000/10484,  Loss: 4.9296\n",
            "Epoch : 8/300, Iter : 6500/10484,  Loss: 0.1728\n",
            "Epoch : 8/300, Iter : 7000/10484,  Loss: 0.0174\n",
            "Epoch : 8/300, Iter : 7500/10484,  Loss: 0.0391\n",
            "Epoch : 8/300, Iter : 8000/10484,  Loss: 0.0613\n",
            "Epoch : 8/300, Iter : 8500/10484,  Loss: 0.9715\n",
            "Epoch : 8/300, Iter : 9000/10484,  Loss: 3.2020\n",
            "Epoch : 8/300, Iter : 9500/10484,  Loss: 0.6931\n",
            "Epoch : 8/300, Iter : 10000/10484,  Loss: 0.1365\n",
            "Epoch : 9/300, Iter : 500/10484,  Loss: 1.4452\n",
            "Epoch : 9/300, Iter : 1000/10484,  Loss: 1.3053\n",
            "Epoch : 9/300, Iter : 1500/10484,  Loss: 1.2774\n",
            "Epoch : 9/300, Iter : 2000/10484,  Loss: 1.2594\n",
            "Epoch : 9/300, Iter : 2500/10484,  Loss: 0.2518\n",
            "Epoch : 9/300, Iter : 3000/10484,  Loss: 0.0047\n",
            "Epoch : 9/300, Iter : 3500/10484,  Loss: 0.3309\n",
            "Epoch : 9/300, Iter : 4000/10484,  Loss: 0.6700\n",
            "Epoch : 9/300, Iter : 4500/10484,  Loss: 3.1243\n",
            "Epoch : 9/300, Iter : 5000/10484,  Loss: 0.1878\n",
            "Epoch : 9/300, Iter : 5500/10484,  Loss: 0.0270\n",
            "Epoch : 9/300, Iter : 6000/10484,  Loss: 0.0006\n",
            "Epoch : 9/300, Iter : 6500/10484,  Loss: 0.5428\n",
            "Epoch : 9/300, Iter : 7000/10484,  Loss: 0.1718\n",
            "Epoch : 9/300, Iter : 7500/10484,  Loss: 0.2627\n",
            "Epoch : 9/300, Iter : 8000/10484,  Loss: 0.0059\n",
            "Epoch : 9/300, Iter : 8500/10484,  Loss: 0.2971\n",
            "Epoch : 9/300, Iter : 9000/10484,  Loss: 1.9309\n",
            "Epoch : 9/300, Iter : 9500/10484,  Loss: 0.0002\n",
            "Epoch : 9/300, Iter : 10000/10484,  Loss: 0.0003\n",
            "Saved the model to /content/gdrive/My Drive/Dal Masters/CSCI 6515 - ML for Big Data/Assignment 2/Data/train_corrected/model_file_experiment4.model\n",
            "Test Accuracy of the model on the 552 test images: 67.0000 %\n",
            "Epoch : 10/300, Iter : 500/10484,  Loss: 0.0010\n",
            "Epoch : 10/300, Iter : 1000/10484,  Loss: 0.0002\n",
            "Epoch : 10/300, Iter : 1500/10484,  Loss: 1.5686\n",
            "Epoch : 10/300, Iter : 2000/10484,  Loss: 0.5574\n",
            "Epoch : 10/300, Iter : 2500/10484,  Loss: 0.4217\n",
            "Epoch : 10/300, Iter : 3000/10484,  Loss: 0.0181\n",
            "Epoch : 10/300, Iter : 3500/10484,  Loss: 0.0138\n",
            "Epoch : 10/300, Iter : 4000/10484,  Loss: 1.5357\n",
            "Epoch : 10/300, Iter : 4500/10484,  Loss: 2.3476\n",
            "Epoch : 10/300, Iter : 5000/10484,  Loss: 3.2243\n",
            "Epoch : 10/300, Iter : 5500/10484,  Loss: 1.0857\n",
            "Epoch : 10/300, Iter : 6000/10484,  Loss: 0.0050\n",
            "Epoch : 10/300, Iter : 6500/10484,  Loss: 0.1324\n",
            "Epoch : 10/300, Iter : 7000/10484,  Loss: 0.8332\n",
            "Epoch : 10/300, Iter : 7500/10484,  Loss: 0.8853\n",
            "Epoch : 10/300, Iter : 8000/10484,  Loss: 2.7833\n",
            "Epoch : 10/300, Iter : 8500/10484,  Loss: 1.2221\n",
            "Epoch : 10/300, Iter : 9000/10484,  Loss: 0.0019\n",
            "Epoch : 10/300, Iter : 9500/10484,  Loss: 0.0750\n",
            "Epoch : 10/300, Iter : 10000/10484,  Loss: 1.2887\n",
            "Epoch : 11/300, Iter : 500/10484,  Loss: 0.9272\n",
            "Epoch : 11/300, Iter : 1000/10484,  Loss: 2.9606\n",
            "Epoch : 11/300, Iter : 1500/10484,  Loss: 0.6282\n",
            "Epoch : 11/300, Iter : 2000/10484,  Loss: 0.9418\n",
            "Epoch : 11/300, Iter : 2500/10484,  Loss: 0.0072\n",
            "Epoch : 11/300, Iter : 3000/10484,  Loss: 0.5471\n",
            "Epoch : 11/300, Iter : 3500/10484,  Loss: 1.9108\n",
            "Epoch : 11/300, Iter : 4000/10484,  Loss: 1.3310\n",
            "Epoch : 11/300, Iter : 4500/10484,  Loss: 0.0001\n",
            "Epoch : 11/300, Iter : 5000/10484,  Loss: 1.7626\n",
            "Epoch : 11/300, Iter : 5500/10484,  Loss: 0.1259\n",
            "Epoch : 11/300, Iter : 6000/10484,  Loss: 0.0020\n",
            "Epoch : 11/300, Iter : 6500/10484,  Loss: 1.8456\n",
            "Epoch : 11/300, Iter : 7000/10484,  Loss: 3.9923\n",
            "Epoch : 11/300, Iter : 7500/10484,  Loss: 0.9587\n",
            "Epoch : 11/300, Iter : 8000/10484,  Loss: 0.4668\n",
            "Epoch : 11/300, Iter : 8500/10484,  Loss: 0.0013\n",
            "Epoch : 11/300, Iter : 9000/10484,  Loss: 0.1138\n",
            "Epoch : 11/300, Iter : 9500/10484,  Loss: 0.8274\n",
            "Epoch : 11/300, Iter : 10000/10484,  Loss: 1.4238\n",
            "Epoch : 12/300, Iter : 500/10484,  Loss: 0.8242\n",
            "Epoch : 12/300, Iter : 1000/10484,  Loss: 0.6487\n",
            "Epoch : 12/300, Iter : 1500/10484,  Loss: 0.0068\n",
            "Epoch : 12/300, Iter : 2000/10484,  Loss: 0.0727\n",
            "Epoch : 12/300, Iter : 2500/10484,  Loss: 1.1809\n",
            "Epoch : 12/300, Iter : 3000/10484,  Loss: 0.3532\n",
            "Epoch : 12/300, Iter : 3500/10484,  Loss: 0.1693\n",
            "Epoch : 12/300, Iter : 4000/10484,  Loss: 3.6769\n",
            "Epoch : 12/300, Iter : 4500/10484,  Loss: 3.8400\n",
            "Epoch : 12/300, Iter : 5000/10484,  Loss: 4.0633\n",
            "Epoch : 12/300, Iter : 5500/10484,  Loss: 0.0093\n",
            "Epoch : 12/300, Iter : 6000/10484,  Loss: 2.6436\n",
            "Epoch : 12/300, Iter : 6500/10484,  Loss: 3.7078\n",
            "Epoch : 12/300, Iter : 7000/10484,  Loss: 3.0622\n",
            "Epoch : 12/300, Iter : 7500/10484,  Loss: 1.3713\n",
            "Epoch : 12/300, Iter : 8000/10484,  Loss: 1.8450\n",
            "Epoch : 12/300, Iter : 8500/10484,  Loss: 0.0007\n",
            "Epoch : 12/300, Iter : 9000/10484,  Loss: 0.0663\n",
            "Epoch : 12/300, Iter : 9500/10484,  Loss: 5.1096\n",
            "Epoch : 12/300, Iter : 10000/10484,  Loss: 2.1197\n",
            "Epoch : 13/300, Iter : 500/10484,  Loss: 0.3006\n",
            "Epoch : 13/300, Iter : 1000/10484,  Loss: 0.0013\n",
            "Epoch : 13/300, Iter : 1500/10484,  Loss: 0.0017\n",
            "Epoch : 13/300, Iter : 2000/10484,  Loss: 2.5693\n",
            "Epoch : 13/300, Iter : 2500/10484,  Loss: 0.2467\n",
            "Epoch : 13/300, Iter : 3000/10484,  Loss: 0.0117\n",
            "Epoch : 13/300, Iter : 3500/10484,  Loss: 0.0326\n",
            "Epoch : 13/300, Iter : 4000/10484,  Loss: 0.0055\n",
            "Epoch : 13/300, Iter : 4500/10484,  Loss: 0.0003\n",
            "Epoch : 13/300, Iter : 5000/10484,  Loss: 0.3261\n",
            "Epoch : 13/300, Iter : 5500/10484,  Loss: 0.3653\n",
            "Epoch : 13/300, Iter : 6000/10484,  Loss: 1.0914\n",
            "Epoch : 13/300, Iter : 6500/10484,  Loss: 0.4233\n",
            "Epoch : 13/300, Iter : 7000/10484,  Loss: 0.0013\n",
            "Epoch : 13/300, Iter : 7500/10484,  Loss: 0.0068\n",
            "Epoch : 13/300, Iter : 8000/10484,  Loss: 0.1904\n",
            "Epoch : 13/300, Iter : 8500/10484,  Loss: 2.3405\n",
            "Epoch : 13/300, Iter : 9000/10484,  Loss: 0.3215\n",
            "Epoch : 13/300, Iter : 9500/10484,  Loss: 2.6907\n",
            "Epoch : 13/300, Iter : 10000/10484,  Loss: 3.6040\n",
            "Epoch : 14/300, Iter : 500/10484,  Loss: 2.6297\n",
            "Epoch : 14/300, Iter : 1000/10484,  Loss: 0.0162\n",
            "Epoch : 14/300, Iter : 1500/10484,  Loss: 1.1025\n",
            "Epoch : 14/300, Iter : 2000/10484,  Loss: 0.0186\n",
            "Epoch : 14/300, Iter : 2500/10484,  Loss: 0.0667\n",
            "Epoch : 14/300, Iter : 3000/10484,  Loss: 0.0131\n",
            "Epoch : 14/300, Iter : 3500/10484,  Loss: 0.6326\n",
            "Epoch : 14/300, Iter : 4000/10484,  Loss: 1.2280\n",
            "Epoch : 14/300, Iter : 4500/10484,  Loss: 2.0217\n",
            "Epoch : 14/300, Iter : 5000/10484,  Loss: 0.0029\n",
            "Epoch : 14/300, Iter : 5500/10484,  Loss: 0.2081\n",
            "Epoch : 14/300, Iter : 6000/10484,  Loss: 0.9939\n",
            "Epoch : 14/300, Iter : 6500/10484,  Loss: 0.0800\n",
            "Epoch : 14/300, Iter : 7000/10484,  Loss: 0.1004\n",
            "Epoch : 14/300, Iter : 7500/10484,  Loss: 1.2117\n",
            "Epoch : 14/300, Iter : 8000/10484,  Loss: 3.1820\n",
            "Epoch : 14/300, Iter : 8500/10484,  Loss: 0.0001\n",
            "Epoch : 14/300, Iter : 9000/10484,  Loss: 0.6255\n",
            "Epoch : 14/300, Iter : 9500/10484,  Loss: 0.0915\n",
            "Epoch : 14/300, Iter : 10000/10484,  Loss: 0.0126\n",
            "Test Accuracy of the model on the 552 test images: 67.0000 %\n",
            "Epoch : 15/300, Iter : 500/10484,  Loss: 0.4086\n",
            "Epoch : 15/300, Iter : 1000/10484,  Loss: 0.0145\n",
            "Epoch : 15/300, Iter : 1500/10484,  Loss: 0.0533\n",
            "Epoch : 15/300, Iter : 2000/10484,  Loss: 1.8535\n",
            "Epoch : 15/300, Iter : 2500/10484,  Loss: 1.4630\n",
            "Epoch : 15/300, Iter : 3000/10484,  Loss: 1.3489\n",
            "Epoch : 15/300, Iter : 3500/10484,  Loss: 1.7032\n",
            "Epoch : 15/300, Iter : 4000/10484,  Loss: 4.1074\n",
            "Epoch : 15/300, Iter : 4500/10484,  Loss: 0.0021\n",
            "Epoch : 15/300, Iter : 5000/10484,  Loss: 0.2784\n",
            "Epoch : 15/300, Iter : 5500/10484,  Loss: 1.1231\n",
            "Epoch : 15/300, Iter : 6000/10484,  Loss: 0.0189\n",
            "Epoch : 15/300, Iter : 6500/10484,  Loss: 2.2787\n",
            "Epoch : 15/300, Iter : 7000/10484,  Loss: 0.0227\n",
            "Epoch : 15/300, Iter : 7500/10484,  Loss: 0.9317\n",
            "Epoch : 15/300, Iter : 8000/10484,  Loss: 0.0052\n",
            "Epoch : 15/300, Iter : 8500/10484,  Loss: 0.2515\n",
            "Epoch : 15/300, Iter : 9000/10484,  Loss: 0.2715\n",
            "Epoch : 15/300, Iter : 9500/10484,  Loss: 1.0648\n",
            "Epoch : 15/300, Iter : 10000/10484,  Loss: 0.1377\n",
            "Epoch : 16/300, Iter : 500/10484,  Loss: 0.4682\n",
            "Epoch : 16/300, Iter : 1000/10484,  Loss: 0.2717\n",
            "Epoch : 16/300, Iter : 1500/10484,  Loss: 0.0865\n",
            "Epoch : 16/300, Iter : 2000/10484,  Loss: 1.8143\n",
            "Epoch : 16/300, Iter : 2500/10484,  Loss: 0.0037\n",
            "Epoch : 16/300, Iter : 3000/10484,  Loss: 0.7493\n",
            "Epoch : 16/300, Iter : 3500/10484,  Loss: 0.0013\n",
            "Epoch : 16/300, Iter : 4000/10484,  Loss: 0.0019\n",
            "Epoch : 16/300, Iter : 4500/10484,  Loss: 1.0299\n",
            "Epoch : 16/300, Iter : 5000/10484,  Loss: 0.0011\n",
            "Epoch : 16/300, Iter : 5500/10484,  Loss: 1.1029\n",
            "Epoch : 16/300, Iter : 6000/10484,  Loss: 0.0059\n",
            "Epoch : 16/300, Iter : 6500/10484,  Loss: 0.8235\n",
            "Epoch : 16/300, Iter : 7000/10484,  Loss: 7.1790\n",
            "Epoch : 16/300, Iter : 7500/10484,  Loss: 3.1927\n",
            "Epoch : 16/300, Iter : 8000/10484,  Loss: 0.8044\n",
            "Epoch : 16/300, Iter : 8500/10484,  Loss: 0.6141\n",
            "Epoch : 16/300, Iter : 9000/10484,  Loss: 0.0070\n",
            "Epoch : 16/300, Iter : 9500/10484,  Loss: 0.1492\n",
            "Epoch : 16/300, Iter : 10000/10484,  Loss: 1.4619\n",
            "Epoch : 17/300, Iter : 500/10484,  Loss: 0.2992\n",
            "Epoch : 17/300, Iter : 1000/10484,  Loss: 5.7988\n",
            "Epoch : 17/300, Iter : 1500/10484,  Loss: 0.1265\n",
            "Epoch : 17/300, Iter : 2000/10484,  Loss: 0.1609\n",
            "Epoch : 17/300, Iter : 2500/10484,  Loss: 0.7625\n",
            "Epoch : 17/300, Iter : 3000/10484,  Loss: 0.1449\n",
            "Epoch : 17/300, Iter : 3500/10484,  Loss: 0.4167\n",
            "Epoch : 17/300, Iter : 4000/10484,  Loss: 0.0059\n",
            "Epoch : 17/300, Iter : 4500/10484,  Loss: 0.0125\n",
            "Epoch : 17/300, Iter : 5000/10484,  Loss: 0.2687\n",
            "Epoch : 17/300, Iter : 5500/10484,  Loss: 0.1793\n",
            "Epoch : 17/300, Iter : 6000/10484,  Loss: 0.0428\n",
            "Epoch : 17/300, Iter : 6500/10484,  Loss: 0.0003\n",
            "Epoch : 17/300, Iter : 7000/10484,  Loss: 2.2276\n",
            "Epoch : 17/300, Iter : 7500/10484,  Loss: 0.0002\n",
            "Epoch : 17/300, Iter : 8000/10484,  Loss: 2.0916\n",
            "Epoch : 17/300, Iter : 8500/10484,  Loss: 1.4362\n",
            "Epoch : 17/300, Iter : 9000/10484,  Loss: 0.0236\n",
            "Epoch : 17/300, Iter : 9500/10484,  Loss: 0.0007\n",
            "Epoch : 17/300, Iter : 10000/10484,  Loss: 2.3911\n",
            "Epoch : 18/300, Iter : 500/10484,  Loss: 0.0018\n",
            "Epoch : 18/300, Iter : 1000/10484,  Loss: 0.4236\n",
            "Epoch : 18/300, Iter : 1500/10484,  Loss: 0.2609\n",
            "Epoch : 18/300, Iter : 2000/10484,  Loss: 0.9156\n",
            "Epoch : 18/300, Iter : 2500/10484,  Loss: 0.0029\n",
            "Epoch : 18/300, Iter : 3000/10484,  Loss: 0.0122\n",
            "Epoch : 18/300, Iter : 3500/10484,  Loss: 1.1860\n",
            "Epoch : 18/300, Iter : 4000/10484,  Loss: 0.3509\n",
            "Epoch : 18/300, Iter : 4500/10484,  Loss: 1.7899\n",
            "Epoch : 18/300, Iter : 5000/10484,  Loss: 0.0013\n",
            "Epoch : 18/300, Iter : 5500/10484,  Loss: 0.1358\n",
            "Epoch : 18/300, Iter : 6000/10484,  Loss: 0.4036\n",
            "Epoch : 18/300, Iter : 6500/10484,  Loss: 2.3512\n",
            "Epoch : 18/300, Iter : 7000/10484,  Loss: 2.3229\n",
            "Epoch : 18/300, Iter : 7500/10484,  Loss: 0.7881\n",
            "Epoch : 18/300, Iter : 8000/10484,  Loss: 1.3504\n",
            "Epoch : 18/300, Iter : 8500/10484,  Loss: 0.0005\n",
            "Epoch : 18/300, Iter : 9000/10484,  Loss: 0.0037\n",
            "Epoch : 18/300, Iter : 9500/10484,  Loss: 2.5080\n",
            "Epoch : 18/300, Iter : 10000/10484,  Loss: 0.1248\n",
            "Epoch : 19/300, Iter : 500/10484,  Loss: 0.0021\n",
            "Epoch : 19/300, Iter : 1000/10484,  Loss: 0.6276\n",
            "Epoch : 19/300, Iter : 1500/10484,  Loss: 0.0625\n",
            "Epoch : 19/300, Iter : 2000/10484,  Loss: 0.0011\n",
            "Epoch : 19/300, Iter : 2500/10484,  Loss: 0.0051\n",
            "Epoch : 19/300, Iter : 3000/10484,  Loss: 0.9148\n",
            "Epoch : 19/300, Iter : 3500/10484,  Loss: 1.3013\n",
            "Epoch : 19/300, Iter : 4000/10484,  Loss: 0.0033\n",
            "Epoch : 19/300, Iter : 4500/10484,  Loss: 0.9293\n",
            "Epoch : 19/300, Iter : 5000/10484,  Loss: 1.7505\n",
            "Epoch : 19/300, Iter : 5500/10484,  Loss: 0.0001\n",
            "Epoch : 19/300, Iter : 6000/10484,  Loss: 1.0247\n",
            "Epoch : 19/300, Iter : 6500/10484,  Loss: 1.8454\n",
            "Epoch : 19/300, Iter : 7000/10484,  Loss: 0.0010\n",
            "Epoch : 19/300, Iter : 7500/10484,  Loss: 0.1334\n",
            "Epoch : 19/300, Iter : 8000/10484,  Loss: 1.5011\n",
            "Epoch : 19/300, Iter : 8500/10484,  Loss: 1.9595\n",
            "Epoch : 19/300, Iter : 9000/10484,  Loss: 2.1418\n",
            "Epoch : 19/300, Iter : 9500/10484,  Loss: 0.0074\n",
            "Epoch : 19/300, Iter : 10000/10484,  Loss: 0.0018\n",
            "Test Accuracy of the model on the 552 test images: 67.0000 %\n",
            "Epoch : 20/300, Iter : 500/10484,  Loss: 1.9654\n",
            "Epoch : 20/300, Iter : 1000/10484,  Loss: 2.2863\n",
            "Epoch : 20/300, Iter : 1500/10484,  Loss: 0.0031\n",
            "Epoch : 20/300, Iter : 2000/10484,  Loss: 0.1474\n",
            "Epoch : 20/300, Iter : 2500/10484,  Loss: 0.0000\n",
            "Epoch : 20/300, Iter : 3000/10484,  Loss: 0.0370\n",
            "Epoch : 20/300, Iter : 3500/10484,  Loss: 0.7649\n",
            "Epoch : 20/300, Iter : 4000/10484,  Loss: 0.2417\n",
            "Epoch : 20/300, Iter : 4500/10484,  Loss: 0.2539\n",
            "Epoch : 20/300, Iter : 5000/10484,  Loss: 0.4792\n",
            "Epoch : 20/300, Iter : 5500/10484,  Loss: 0.8440\n",
            "Epoch : 20/300, Iter : 6000/10484,  Loss: 0.5321\n",
            "Epoch : 20/300, Iter : 6500/10484,  Loss: 0.0020\n",
            "Epoch : 20/300, Iter : 7000/10484,  Loss: 0.5986\n",
            "Epoch : 20/300, Iter : 7500/10484,  Loss: 0.0408\n",
            "Epoch : 20/300, Iter : 8000/10484,  Loss: 0.0020\n",
            "Epoch : 20/300, Iter : 8500/10484,  Loss: 1.8103\n",
            "Epoch : 20/300, Iter : 9000/10484,  Loss: 0.3107\n",
            "Epoch : 20/300, Iter : 9500/10484,  Loss: 0.5921\n",
            "Epoch : 20/300, Iter : 10000/10484,  Loss: 0.0044\n",
            "Epoch : 21/300, Iter : 500/10484,  Loss: 0.2603\n",
            "Epoch : 21/300, Iter : 1000/10484,  Loss: 0.9035\n",
            "Epoch : 21/300, Iter : 1500/10484,  Loss: 0.0902\n",
            "Epoch : 21/300, Iter : 2000/10484,  Loss: 1.2519\n",
            "Epoch : 21/300, Iter : 2500/10484,  Loss: 0.1679\n",
            "Epoch : 21/300, Iter : 3000/10484,  Loss: 0.0203\n",
            "Epoch : 21/300, Iter : 3500/10484,  Loss: 0.0007\n",
            "Epoch : 21/300, Iter : 4000/10484,  Loss: 0.1798\n",
            "Epoch : 21/300, Iter : 4500/10484,  Loss: 0.7319\n",
            "Epoch : 21/300, Iter : 5000/10484,  Loss: 2.7672\n",
            "Epoch : 21/300, Iter : 5500/10484,  Loss: 0.9518\n",
            "Epoch : 21/300, Iter : 6000/10484,  Loss: 0.1082\n",
            "Epoch : 21/300, Iter : 6500/10484,  Loss: 1.8377\n",
            "Epoch : 21/300, Iter : 7000/10484,  Loss: 0.0134\n",
            "Epoch : 21/300, Iter : 7500/10484,  Loss: 0.4545\n",
            "Epoch : 21/300, Iter : 8000/10484,  Loss: 0.3096\n",
            "Epoch : 21/300, Iter : 8500/10484,  Loss: 0.1607\n",
            "Epoch : 21/300, Iter : 9000/10484,  Loss: 1.1088\n",
            "Epoch : 21/300, Iter : 9500/10484,  Loss: 0.0237\n",
            "Epoch : 21/300, Iter : 10000/10484,  Loss: 0.4315\n",
            "Epoch : 22/300, Iter : 500/10484,  Loss: 0.4913\n",
            "Epoch : 22/300, Iter : 1000/10484,  Loss: 0.5717\n",
            "Epoch : 22/300, Iter : 1500/10484,  Loss: 0.0044\n",
            "Epoch : 22/300, Iter : 2000/10484,  Loss: 1.1042\n",
            "Epoch : 22/300, Iter : 2500/10484,  Loss: 0.9179\n",
            "Epoch : 22/300, Iter : 3000/10484,  Loss: 0.6259\n",
            "Epoch : 22/300, Iter : 3500/10484,  Loss: 0.3506\n",
            "Epoch : 22/300, Iter : 4000/10484,  Loss: 0.1853\n",
            "Epoch : 22/300, Iter : 4500/10484,  Loss: 1.9277\n",
            "Epoch : 22/300, Iter : 5000/10484,  Loss: 0.5538\n",
            "Epoch : 22/300, Iter : 5500/10484,  Loss: 0.3375\n",
            "Epoch : 22/300, Iter : 6000/10484,  Loss: 0.1513\n",
            "Epoch : 22/300, Iter : 6500/10484,  Loss: 0.0020\n",
            "Epoch : 22/300, Iter : 7000/10484,  Loss: 0.2063\n",
            "Epoch : 22/300, Iter : 7500/10484,  Loss: 0.4412\n",
            "Epoch : 22/300, Iter : 8000/10484,  Loss: 1.0454\n",
            "Epoch : 22/300, Iter : 8500/10484,  Loss: 0.0886\n",
            "Epoch : 22/300, Iter : 9000/10484,  Loss: 3.6417\n",
            "Epoch : 22/300, Iter : 9500/10484,  Loss: 0.0962\n",
            "Epoch : 22/300, Iter : 10000/10484,  Loss: 0.0246\n",
            "Epoch : 23/300, Iter : 500/10484,  Loss: 0.0386\n",
            "Epoch : 23/300, Iter : 1000/10484,  Loss: 0.7787\n",
            "Epoch : 23/300, Iter : 1500/10484,  Loss: 6.9271\n",
            "Epoch : 23/300, Iter : 2000/10484,  Loss: 0.0093\n",
            "Epoch : 23/300, Iter : 2500/10484,  Loss: 0.0259\n",
            "Epoch : 23/300, Iter : 3000/10484,  Loss: 0.0154\n",
            "Epoch : 23/300, Iter : 3500/10484,  Loss: 1.0374\n",
            "Epoch : 23/300, Iter : 4000/10484,  Loss: 0.0099\n",
            "Epoch : 23/300, Iter : 4500/10484,  Loss: 1.1182\n",
            "Epoch : 23/300, Iter : 5000/10484,  Loss: 0.0009\n",
            "Epoch : 23/300, Iter : 5500/10484,  Loss: 0.2386\n",
            "Epoch : 23/300, Iter : 6000/10484,  Loss: 0.0016\n",
            "Epoch : 23/300, Iter : 6500/10484,  Loss: 0.3795\n",
            "Epoch : 23/300, Iter : 7000/10484,  Loss: 0.0301\n",
            "Epoch : 23/300, Iter : 7500/10484,  Loss: 0.2431\n",
            "Epoch : 23/300, Iter : 8000/10484,  Loss: 0.8254\n",
            "Epoch : 23/300, Iter : 8500/10484,  Loss: 0.9605\n",
            "Epoch : 23/300, Iter : 9000/10484,  Loss: 2.4777\n",
            "Epoch : 23/300, Iter : 9500/10484,  Loss: 0.1206\n",
            "Epoch : 23/300, Iter : 10000/10484,  Loss: 0.3437\n",
            "Epoch : 24/300, Iter : 500/10484,  Loss: 2.1176\n",
            "Epoch : 24/300, Iter : 1000/10484,  Loss: 0.0014\n",
            "Epoch : 24/300, Iter : 1500/10484,  Loss: 0.2416\n",
            "Epoch : 24/300, Iter : 2000/10484,  Loss: 0.2682\n",
            "Epoch : 24/300, Iter : 2500/10484,  Loss: 1.1069\n",
            "Epoch : 24/300, Iter : 3000/10484,  Loss: 0.0068\n",
            "Epoch : 24/300, Iter : 3500/10484,  Loss: 5.6091\n",
            "Epoch : 24/300, Iter : 4000/10484,  Loss: 0.1650\n",
            "Epoch : 24/300, Iter : 4500/10484,  Loss: 0.7540\n",
            "Epoch : 24/300, Iter : 5000/10484,  Loss: 1.0742\n",
            "Epoch : 24/300, Iter : 5500/10484,  Loss: 0.0894\n",
            "Epoch : 24/300, Iter : 6000/10484,  Loss: 0.2166\n",
            "Epoch : 24/300, Iter : 6500/10484,  Loss: 2.2721\n",
            "Epoch : 24/300, Iter : 7000/10484,  Loss: 0.0033\n",
            "Epoch : 24/300, Iter : 7500/10484,  Loss: 0.0003\n",
            "Epoch : 24/300, Iter : 8000/10484,  Loss: 1.1710\n",
            "Epoch : 24/300, Iter : 8500/10484,  Loss: 1.3035\n",
            "Epoch : 24/300, Iter : 9000/10484,  Loss: 0.4463\n",
            "Epoch : 24/300, Iter : 9500/10484,  Loss: 0.0147\n",
            "Epoch : 24/300, Iter : 10000/10484,  Loss: 0.0003\n",
            "Saved the model to /content/gdrive/My Drive/Dal Masters/CSCI 6515 - ML for Big Data/Assignment 2/Data/train_corrected/model_file_experiment4.model\n",
            "Test Accuracy of the model on the 552 test images: 68.0000 %\n",
            "Epoch : 25/300, Iter : 500/10484,  Loss: 0.0005\n",
            "Epoch : 25/300, Iter : 1000/10484,  Loss: 0.2326\n",
            "Epoch : 25/300, Iter : 1500/10484,  Loss: 1.1779\n",
            "Epoch : 25/300, Iter : 2000/10484,  Loss: 0.0188\n",
            "Epoch : 25/300, Iter : 2500/10484,  Loss: 3.0180\n",
            "Epoch : 25/300, Iter : 3000/10484,  Loss: 0.7784\n",
            "Epoch : 25/300, Iter : 3500/10484,  Loss: 0.0310\n",
            "Epoch : 25/300, Iter : 4000/10484,  Loss: 0.0000\n",
            "Epoch : 25/300, Iter : 4500/10484,  Loss: 0.9706\n",
            "Epoch : 25/300, Iter : 5000/10484,  Loss: 1.1435\n",
            "Epoch : 25/300, Iter : 5500/10484,  Loss: 3.7096\n",
            "Epoch : 25/300, Iter : 6000/10484,  Loss: 1.4677\n",
            "Epoch : 25/300, Iter : 6500/10484,  Loss: 0.0011\n",
            "Epoch : 25/300, Iter : 7000/10484,  Loss: 0.0696\n",
            "Epoch : 25/300, Iter : 7500/10484,  Loss: 0.1605\n",
            "Epoch : 25/300, Iter : 8000/10484,  Loss: 6.8518\n",
            "Epoch : 25/300, Iter : 8500/10484,  Loss: 0.0059\n",
            "Epoch : 25/300, Iter : 9000/10484,  Loss: 0.4723\n",
            "Epoch : 25/300, Iter : 9500/10484,  Loss: 0.1875\n",
            "Epoch : 25/300, Iter : 10000/10484,  Loss: 0.2423\n",
            "Epoch : 26/300, Iter : 500/10484,  Loss: 2.5218\n",
            "Epoch : 26/300, Iter : 1000/10484,  Loss: 0.0002\n",
            "Epoch : 26/300, Iter : 1500/10484,  Loss: 0.4957\n",
            "Epoch : 26/300, Iter : 2000/10484,  Loss: 0.3615\n",
            "Epoch : 26/300, Iter : 2500/10484,  Loss: 0.2568\n",
            "Epoch : 26/300, Iter : 3000/10484,  Loss: 0.1358\n",
            "Epoch : 26/300, Iter : 3500/10484,  Loss: 0.7345\n",
            "Epoch : 26/300, Iter : 4000/10484,  Loss: 0.7276\n",
            "Epoch : 26/300, Iter : 4500/10484,  Loss: 0.0000\n",
            "Epoch : 26/300, Iter : 5000/10484,  Loss: 1.6663\n",
            "Epoch : 26/300, Iter : 5500/10484,  Loss: 0.0066\n",
            "Epoch : 26/300, Iter : 6000/10484,  Loss: 2.6026\n",
            "Epoch : 26/300, Iter : 6500/10484,  Loss: 0.6781\n",
            "Epoch : 26/300, Iter : 7000/10484,  Loss: 0.0008\n",
            "Epoch : 26/300, Iter : 7500/10484,  Loss: 1.2777\n",
            "Epoch : 26/300, Iter : 8000/10484,  Loss: 2.8764\n",
            "Epoch : 26/300, Iter : 8500/10484,  Loss: 2.9757\n",
            "Epoch : 26/300, Iter : 9000/10484,  Loss: 1.4824\n",
            "Epoch : 26/300, Iter : 9500/10484,  Loss: 0.0073\n",
            "Epoch : 26/300, Iter : 10000/10484,  Loss: 0.1921\n",
            "Epoch : 27/300, Iter : 500/10484,  Loss: 0.2638\n",
            "Epoch : 27/300, Iter : 1000/10484,  Loss: 0.9763\n",
            "Epoch : 27/300, Iter : 1500/10484,  Loss: 0.1855\n",
            "Epoch : 27/300, Iter : 2000/10484,  Loss: 0.0032\n",
            "Epoch : 27/300, Iter : 2500/10484,  Loss: 0.9253\n",
            "Epoch : 27/300, Iter : 3000/10484,  Loss: 1.5702\n",
            "Epoch : 27/300, Iter : 3500/10484,  Loss: 1.5960\n",
            "Epoch : 27/300, Iter : 4000/10484,  Loss: 0.4086\n",
            "Epoch : 27/300, Iter : 4500/10484,  Loss: 0.7826\n",
            "Epoch : 27/300, Iter : 5000/10484,  Loss: 0.4713\n",
            "Epoch : 27/300, Iter : 5500/10484,  Loss: 0.0109\n",
            "Epoch : 27/300, Iter : 6000/10484,  Loss: 1.0750\n",
            "Epoch : 27/300, Iter : 6500/10484,  Loss: 0.0009\n",
            "Epoch : 27/300, Iter : 7000/10484,  Loss: 0.3705\n",
            "Epoch : 27/300, Iter : 7500/10484,  Loss: 0.0011\n",
            "Epoch : 27/300, Iter : 8000/10484,  Loss: 0.2318\n",
            "Epoch : 27/300, Iter : 8500/10484,  Loss: 2.6866\n",
            "Epoch : 27/300, Iter : 9000/10484,  Loss: 1.4459\n",
            "Epoch : 27/300, Iter : 9500/10484,  Loss: 0.8177\n",
            "Epoch : 27/300, Iter : 10000/10484,  Loss: 1.1869\n",
            "Epoch : 28/300, Iter : 500/10484,  Loss: 0.3497\n",
            "Epoch : 28/300, Iter : 1000/10484,  Loss: 3.0065\n",
            "Epoch : 28/300, Iter : 1500/10484,  Loss: 0.0000\n",
            "Epoch : 28/300, Iter : 2000/10484,  Loss: 0.1367\n",
            "Epoch : 28/300, Iter : 2500/10484,  Loss: 0.9368\n",
            "Epoch : 28/300, Iter : 3000/10484,  Loss: 0.0742\n",
            "Epoch : 28/300, Iter : 3500/10484,  Loss: 0.4874\n",
            "Epoch : 28/300, Iter : 4000/10484,  Loss: 0.0024\n",
            "Epoch : 28/300, Iter : 4500/10484,  Loss: 1.5795\n",
            "Epoch : 28/300, Iter : 5000/10484,  Loss: 1.2107\n",
            "Epoch : 28/300, Iter : 5500/10484,  Loss: 0.0608\n",
            "Epoch : 28/300, Iter : 6000/10484,  Loss: 0.0004\n",
            "Epoch : 28/300, Iter : 6500/10484,  Loss: 0.5012\n",
            "Epoch : 28/300, Iter : 7000/10484,  Loss: 3.2143\n",
            "Epoch : 28/300, Iter : 7500/10484,  Loss: 1.5635\n",
            "Epoch : 28/300, Iter : 8000/10484,  Loss: 1.5936\n",
            "Epoch : 28/300, Iter : 8500/10484,  Loss: 0.2688\n",
            "Epoch : 28/300, Iter : 9000/10484,  Loss: 0.4938\n",
            "Epoch : 28/300, Iter : 9500/10484,  Loss: 0.0618\n",
            "Epoch : 28/300, Iter : 10000/10484,  Loss: 0.8592\n",
            "Epoch : 29/300, Iter : 500/10484,  Loss: 0.4184\n",
            "Epoch : 29/300, Iter : 1000/10484,  Loss: 0.2856\n",
            "Epoch : 29/300, Iter : 1500/10484,  Loss: 0.3346\n",
            "Epoch : 29/300, Iter : 2000/10484,  Loss: 0.2994\n",
            "Epoch : 29/300, Iter : 2500/10484,  Loss: 4.2289\n",
            "Epoch : 29/300, Iter : 3000/10484,  Loss: 1.1221\n",
            "Epoch : 29/300, Iter : 3500/10484,  Loss: 1.3542\n",
            "Epoch : 29/300, Iter : 4000/10484,  Loss: 4.6993\n",
            "Epoch : 29/300, Iter : 4500/10484,  Loss: 0.5209\n",
            "Epoch : 29/300, Iter : 5000/10484,  Loss: 0.0003\n",
            "Epoch : 29/300, Iter : 5500/10484,  Loss: 0.3265\n",
            "Epoch : 29/300, Iter : 6000/10484,  Loss: 0.0009\n",
            "Epoch : 29/300, Iter : 6500/10484,  Loss: 0.7233\n",
            "Epoch : 29/300, Iter : 7000/10484,  Loss: 0.0010\n",
            "Epoch : 29/300, Iter : 7500/10484,  Loss: 0.2863\n",
            "Epoch : 29/300, Iter : 8000/10484,  Loss: 0.7819\n",
            "Epoch : 29/300, Iter : 8500/10484,  Loss: 0.0000\n",
            "Epoch : 29/300, Iter : 9000/10484,  Loss: 1.2747\n",
            "Epoch : 29/300, Iter : 9500/10484,  Loss: 0.1060\n",
            "Epoch : 29/300, Iter : 10000/10484,  Loss: 0.0230\n",
            "Test Accuracy of the model on the 552 test images: 67.0000 %\n",
            "Epoch : 30/300, Iter : 500/10484,  Loss: 1.0727\n",
            "Epoch : 30/300, Iter : 1000/10484,  Loss: 0.0006\n",
            "Epoch : 30/300, Iter : 1500/10484,  Loss: 0.4847\n",
            "Epoch : 30/300, Iter : 2000/10484,  Loss: 3.8181\n",
            "Epoch : 30/300, Iter : 2500/10484,  Loss: 0.2751\n",
            "Epoch : 30/300, Iter : 3000/10484,  Loss: 0.0066\n",
            "Epoch : 30/300, Iter : 3500/10484,  Loss: 0.2242\n",
            "Epoch : 30/300, Iter : 4000/10484,  Loss: 3.0755\n",
            "Epoch : 30/300, Iter : 4500/10484,  Loss: 0.0085\n",
            "Epoch : 30/300, Iter : 5000/10484,  Loss: 0.3670\n",
            "Epoch : 30/300, Iter : 5500/10484,  Loss: 2.3824\n",
            "Epoch : 30/300, Iter : 6000/10484,  Loss: 0.2896\n",
            "Epoch : 30/300, Iter : 6500/10484,  Loss: 0.2688\n",
            "Epoch : 30/300, Iter : 7000/10484,  Loss: 0.0026\n",
            "Epoch : 30/300, Iter : 7500/10484,  Loss: 0.5178\n",
            "Epoch : 30/300, Iter : 8000/10484,  Loss: 0.1257\n",
            "Epoch : 30/300, Iter : 8500/10484,  Loss: 2.7495\n",
            "Epoch : 30/300, Iter : 9000/10484,  Loss: 0.0380\n",
            "Epoch : 30/300, Iter : 9500/10484,  Loss: 0.1283\n",
            "Epoch : 30/300, Iter : 10000/10484,  Loss: 0.0031\n",
            "Epoch : 31/300, Iter : 500/10484,  Loss: 1.1418\n",
            "Epoch : 31/300, Iter : 1000/10484,  Loss: 0.3697\n",
            "Epoch : 31/300, Iter : 1500/10484,  Loss: 0.8577\n",
            "Epoch : 31/300, Iter : 2000/10484,  Loss: 0.2358\n",
            "Epoch : 31/300, Iter : 2500/10484,  Loss: 0.1899\n",
            "Epoch : 31/300, Iter : 3000/10484,  Loss: 0.0061\n",
            "Epoch : 31/300, Iter : 3500/10484,  Loss: 0.3081\n",
            "Epoch : 31/300, Iter : 4000/10484,  Loss: 0.4048\n",
            "Epoch : 31/300, Iter : 4500/10484,  Loss: 2.5559\n",
            "Epoch : 31/300, Iter : 5000/10484,  Loss: 0.0012\n",
            "Epoch : 31/300, Iter : 5500/10484,  Loss: 0.2292\n",
            "Epoch : 31/300, Iter : 6000/10484,  Loss: 0.8526\n",
            "Epoch : 31/300, Iter : 6500/10484,  Loss: 0.0037\n",
            "Epoch : 31/300, Iter : 7000/10484,  Loss: 0.0031\n",
            "Epoch : 31/300, Iter : 7500/10484,  Loss: 0.5368\n",
            "Epoch : 31/300, Iter : 8000/10484,  Loss: 0.0040\n",
            "Epoch : 31/300, Iter : 8500/10484,  Loss: 0.4215\n",
            "Epoch : 31/300, Iter : 9000/10484,  Loss: 0.0070\n",
            "Epoch : 31/300, Iter : 9500/10484,  Loss: 0.8530\n",
            "Epoch : 31/300, Iter : 10000/10484,  Loss: 0.5037\n",
            "Epoch : 32/300, Iter : 500/10484,  Loss: 2.7685\n",
            "Epoch : 32/300, Iter : 1000/10484,  Loss: 0.0013\n",
            "Epoch : 32/300, Iter : 1500/10484,  Loss: 0.2315\n",
            "Epoch : 32/300, Iter : 2000/10484,  Loss: 0.0015\n",
            "Epoch : 32/300, Iter : 2500/10484,  Loss: 0.7946\n",
            "Epoch : 32/300, Iter : 3000/10484,  Loss: 3.7437\n",
            "Epoch : 32/300, Iter : 3500/10484,  Loss: 0.1898\n",
            "Epoch : 32/300, Iter : 4000/10484,  Loss: 0.2405\n",
            "Epoch : 32/300, Iter : 4500/10484,  Loss: 0.1899\n",
            "Epoch : 32/300, Iter : 5000/10484,  Loss: 0.3076\n",
            "Epoch : 32/300, Iter : 5500/10484,  Loss: 0.2718\n",
            "Epoch : 32/300, Iter : 6000/10484,  Loss: 1.1671\n",
            "Epoch : 32/300, Iter : 6500/10484,  Loss: 0.0005\n",
            "Epoch : 32/300, Iter : 7000/10484,  Loss: 0.0608\n",
            "Epoch : 32/300, Iter : 7500/10484,  Loss: 0.0614\n",
            "Epoch : 32/300, Iter : 8000/10484,  Loss: 1.3589\n",
            "Epoch : 32/300, Iter : 8500/10484,  Loss: 1.2457\n",
            "Epoch : 32/300, Iter : 9000/10484,  Loss: 0.0387\n",
            "Epoch : 32/300, Iter : 9500/10484,  Loss: 0.3936\n",
            "Epoch : 32/300, Iter : 10000/10484,  Loss: 0.0475\n",
            "Epoch : 33/300, Iter : 500/10484,  Loss: 0.0017\n",
            "Epoch : 33/300, Iter : 1000/10484,  Loss: 0.7564\n",
            "Epoch : 33/300, Iter : 1500/10484,  Loss: 5.0842\n",
            "Epoch : 33/300, Iter : 2000/10484,  Loss: 0.0001\n",
            "Epoch : 33/300, Iter : 2500/10484,  Loss: 0.1029\n",
            "Epoch : 33/300, Iter : 3000/10484,  Loss: 0.0000\n",
            "Epoch : 33/300, Iter : 3500/10484,  Loss: 0.4586\n",
            "Epoch : 33/300, Iter : 4000/10484,  Loss: 1.0779\n",
            "Epoch : 33/300, Iter : 4500/10484,  Loss: 2.4842\n",
            "Epoch : 33/300, Iter : 5000/10484,  Loss: 1.0807\n",
            "Epoch : 33/300, Iter : 5500/10484,  Loss: 0.5026\n",
            "Epoch : 33/300, Iter : 6000/10484,  Loss: 2.7411\n",
            "Epoch : 33/300, Iter : 6500/10484,  Loss: 0.0002\n",
            "Epoch : 33/300, Iter : 7000/10484,  Loss: 0.7445\n",
            "Epoch : 33/300, Iter : 7500/10484,  Loss: 0.9346\n",
            "Epoch : 33/300, Iter : 8000/10484,  Loss: 0.9494\n",
            "Epoch : 33/300, Iter : 8500/10484,  Loss: 0.4644\n",
            "Epoch : 33/300, Iter : 9000/10484,  Loss: 1.3765\n",
            "Epoch : 33/300, Iter : 9500/10484,  Loss: 1.3710\n",
            "Epoch : 33/300, Iter : 10000/10484,  Loss: 0.1143\n",
            "Epoch : 34/300, Iter : 500/10484,  Loss: 0.2836\n",
            "Epoch : 34/300, Iter : 1000/10484,  Loss: 0.2751\n",
            "Epoch : 34/300, Iter : 1500/10484,  Loss: 0.0001\n",
            "Epoch : 34/300, Iter : 2000/10484,  Loss: 0.0004\n",
            "Epoch : 34/300, Iter : 2500/10484,  Loss: 0.3786\n",
            "Epoch : 34/300, Iter : 3000/10484,  Loss: 0.8883\n",
            "Epoch : 34/300, Iter : 3500/10484,  Loss: 0.1452\n",
            "Epoch : 34/300, Iter : 4000/10484,  Loss: 0.6611\n",
            "Epoch : 34/300, Iter : 4500/10484,  Loss: 1.4835\n",
            "Epoch : 34/300, Iter : 5000/10484,  Loss: 0.7005\n",
            "Epoch : 34/300, Iter : 5500/10484,  Loss: 0.0153\n",
            "Epoch : 34/300, Iter : 6000/10484,  Loss: 0.0011\n",
            "Epoch : 34/300, Iter : 6500/10484,  Loss: 0.0137\n",
            "Epoch : 34/300, Iter : 7000/10484,  Loss: 0.3615\n",
            "Epoch : 34/300, Iter : 7500/10484,  Loss: 0.1813\n",
            "Epoch : 34/300, Iter : 8000/10484,  Loss: 1.1411\n",
            "Epoch : 34/300, Iter : 8500/10484,  Loss: 1.1980\n",
            "Epoch : 34/300, Iter : 9000/10484,  Loss: 0.5441\n",
            "Epoch : 34/300, Iter : 9500/10484,  Loss: 3.6671\n",
            "Epoch : 34/300, Iter : 10000/10484,  Loss: 0.2792\n",
            "Saved the model to /content/gdrive/My Drive/Dal Masters/CSCI 6515 - ML for Big Data/Assignment 2/Data/train_corrected/model_file_experiment4.model\n",
            "Test Accuracy of the model on the 552 test images: 69.0000 %\n",
            "Epoch : 35/300, Iter : 500/10484,  Loss: 0.2797\n",
            "Epoch : 35/300, Iter : 1000/10484,  Loss: 0.3701\n",
            "Epoch : 35/300, Iter : 1500/10484,  Loss: 0.3295\n",
            "Epoch : 35/300, Iter : 2000/10484,  Loss: 2.4368\n",
            "Epoch : 35/300, Iter : 2500/10484,  Loss: 0.0001\n",
            "Epoch : 35/300, Iter : 3000/10484,  Loss: 0.2555\n",
            "Epoch : 35/300, Iter : 3500/10484,  Loss: 1.1282\n",
            "Epoch : 35/300, Iter : 4000/10484,  Loss: 0.9441\n",
            "Epoch : 35/300, Iter : 4500/10484,  Loss: 0.1210\n",
            "Epoch : 35/300, Iter : 5000/10484,  Loss: 0.0000\n",
            "Epoch : 35/300, Iter : 5500/10484,  Loss: 2.6612\n",
            "Epoch : 35/300, Iter : 6000/10484,  Loss: 0.2195\n",
            "Epoch : 35/300, Iter : 6500/10484,  Loss: 0.0025\n",
            "Epoch : 35/300, Iter : 7000/10484,  Loss: 0.0045\n",
            "Epoch : 35/300, Iter : 7500/10484,  Loss: 1.8353\n",
            "Epoch : 35/300, Iter : 8000/10484,  Loss: 0.4550\n",
            "Epoch : 35/300, Iter : 8500/10484,  Loss: 0.1953\n",
            "Epoch : 35/300, Iter : 9000/10484,  Loss: 0.1900\n",
            "Epoch : 35/300, Iter : 9500/10484,  Loss: 0.0075\n",
            "Epoch : 35/300, Iter : 10000/10484,  Loss: 1.0527\n",
            "Epoch : 36/300, Iter : 500/10484,  Loss: 0.0025\n",
            "Epoch : 36/300, Iter : 1000/10484,  Loss: 0.1288\n",
            "Epoch : 36/300, Iter : 1500/10484,  Loss: 1.3514\n",
            "Epoch : 36/300, Iter : 2000/10484,  Loss: 0.0003\n",
            "Epoch : 36/300, Iter : 2500/10484,  Loss: 0.4039\n",
            "Epoch : 36/300, Iter : 3000/10484,  Loss: 0.0000\n",
            "Epoch : 36/300, Iter : 3500/10484,  Loss: 1.6647\n",
            "Epoch : 36/300, Iter : 4000/10484,  Loss: 0.3160\n",
            "Epoch : 36/300, Iter : 4500/10484,  Loss: 0.4362\n",
            "Epoch : 36/300, Iter : 5000/10484,  Loss: 0.0527\n",
            "Epoch : 36/300, Iter : 5500/10484,  Loss: 0.2070\n",
            "Epoch : 36/300, Iter : 6000/10484,  Loss: 0.4342\n",
            "Epoch : 36/300, Iter : 6500/10484,  Loss: 0.0002\n",
            "Epoch : 36/300, Iter : 7000/10484,  Loss: 0.0128\n",
            "Epoch : 36/300, Iter : 7500/10484,  Loss: 1.3229\n",
            "Epoch : 36/300, Iter : 8000/10484,  Loss: 0.0277\n",
            "Epoch : 36/300, Iter : 8500/10484,  Loss: 0.0450\n",
            "Epoch : 36/300, Iter : 9000/10484,  Loss: 0.2327\n",
            "Epoch : 36/300, Iter : 9500/10484,  Loss: 0.0015\n",
            "Epoch : 36/300, Iter : 10000/10484,  Loss: 1.2125\n",
            "Epoch : 37/300, Iter : 500/10484,  Loss: 0.0001\n",
            "Epoch : 37/300, Iter : 1000/10484,  Loss: 1.4187\n",
            "Epoch : 37/300, Iter : 1500/10484,  Loss: 0.0641\n",
            "Epoch : 37/300, Iter : 2000/10484,  Loss: 1.2566\n",
            "Epoch : 37/300, Iter : 2500/10484,  Loss: 2.5542\n",
            "Epoch : 37/300, Iter : 3000/10484,  Loss: 0.0975\n",
            "Epoch : 37/300, Iter : 3500/10484,  Loss: 0.1137\n",
            "Epoch : 37/300, Iter : 4000/10484,  Loss: 0.0030\n",
            "Epoch : 37/300, Iter : 4500/10484,  Loss: 0.4410\n",
            "Epoch : 37/300, Iter : 5000/10484,  Loss: 0.6305\n",
            "Epoch : 37/300, Iter : 5500/10484,  Loss: 0.0492\n",
            "Epoch : 37/300, Iter : 6000/10484,  Loss: 0.1253\n",
            "Epoch : 37/300, Iter : 6500/10484,  Loss: 0.8273\n",
            "Epoch : 37/300, Iter : 7000/10484,  Loss: 0.7776\n",
            "Epoch : 37/300, Iter : 7500/10484,  Loss: 0.0320\n",
            "Epoch : 37/300, Iter : 8000/10484,  Loss: 0.4500\n",
            "Epoch : 37/300, Iter : 8500/10484,  Loss: 1.6082\n",
            "Epoch : 37/300, Iter : 9000/10484,  Loss: 1.0281\n",
            "Epoch : 37/300, Iter : 9500/10484,  Loss: 0.5524\n",
            "Epoch : 37/300, Iter : 10000/10484,  Loss: 0.0026\n",
            "Epoch : 38/300, Iter : 500/10484,  Loss: 0.8050\n",
            "Epoch : 38/300, Iter : 1000/10484,  Loss: 0.5751\n",
            "Epoch : 38/300, Iter : 1500/10484,  Loss: 1.2162\n",
            "Epoch : 38/300, Iter : 2000/10484,  Loss: 0.9436\n",
            "Epoch : 38/300, Iter : 2500/10484,  Loss: 0.0000\n",
            "Epoch : 38/300, Iter : 3000/10484,  Loss: 0.0008\n",
            "Epoch : 38/300, Iter : 3500/10484,  Loss: 0.0792\n",
            "Epoch : 38/300, Iter : 4000/10484,  Loss: 0.0717\n",
            "Epoch : 38/300, Iter : 4500/10484,  Loss: 0.0249\n",
            "Epoch : 38/300, Iter : 5000/10484,  Loss: 1.1314\n",
            "Epoch : 38/300, Iter : 5500/10484,  Loss: 0.2225\n",
            "Epoch : 38/300, Iter : 6000/10484,  Loss: 3.2294\n",
            "Epoch : 38/300, Iter : 6500/10484,  Loss: 0.0001\n",
            "Epoch : 38/300, Iter : 7000/10484,  Loss: 0.0338\n",
            "Epoch : 38/300, Iter : 7500/10484,  Loss: 0.2941\n",
            "Epoch : 38/300, Iter : 8000/10484,  Loss: 0.0000\n",
            "Epoch : 38/300, Iter : 8500/10484,  Loss: 6.4020\n",
            "Epoch : 38/300, Iter : 9000/10484,  Loss: 0.0008\n",
            "Epoch : 38/300, Iter : 9500/10484,  Loss: 0.2788\n",
            "Epoch : 38/300, Iter : 10000/10484,  Loss: 2.2665\n",
            "Epoch : 39/300, Iter : 500/10484,  Loss: 0.0635\n",
            "Epoch : 39/300, Iter : 1000/10484,  Loss: 0.0002\n",
            "Epoch : 39/300, Iter : 1500/10484,  Loss: 0.0020\n",
            "Epoch : 39/300, Iter : 2000/10484,  Loss: 0.0014\n",
            "Epoch : 39/300, Iter : 2500/10484,  Loss: 0.7082\n",
            "Epoch : 39/300, Iter : 3000/10484,  Loss: 0.0095\n",
            "Epoch : 39/300, Iter : 3500/10484,  Loss: 0.3329\n",
            "Epoch : 39/300, Iter : 4000/10484,  Loss: 0.0169\n",
            "Epoch : 39/300, Iter : 4500/10484,  Loss: 0.0001\n",
            "Epoch : 39/300, Iter : 5000/10484,  Loss: 1.7629\n",
            "Epoch : 39/300, Iter : 5500/10484,  Loss: 0.2908\n",
            "Epoch : 39/300, Iter : 6000/10484,  Loss: 1.4808\n",
            "Epoch : 39/300, Iter : 6500/10484,  Loss: 0.2832\n",
            "Epoch : 39/300, Iter : 7000/10484,  Loss: 0.1400\n",
            "Epoch : 39/300, Iter : 7500/10484,  Loss: 0.2951\n",
            "Epoch : 39/300, Iter : 8000/10484,  Loss: 2.1705\n",
            "Epoch : 39/300, Iter : 8500/10484,  Loss: 0.0483\n",
            "Epoch : 39/300, Iter : 9000/10484,  Loss: 1.2111\n",
            "Epoch : 39/300, Iter : 9500/10484,  Loss: 0.1041\n",
            "Epoch : 39/300, Iter : 10000/10484,  Loss: 0.8566\n",
            "Test Accuracy of the model on the 552 test images: 69.0000 %\n",
            "Epoch : 40/300, Iter : 500/10484,  Loss: 0.0838\n",
            "Epoch : 40/300, Iter : 1000/10484,  Loss: 0.3980\n",
            "Epoch : 40/300, Iter : 1500/10484,  Loss: 0.2344\n",
            "Epoch : 40/300, Iter : 2000/10484,  Loss: 0.0426\n",
            "Epoch : 40/300, Iter : 2500/10484,  Loss: 0.4993\n",
            "Epoch : 40/300, Iter : 3000/10484,  Loss: 1.1031\n",
            "Epoch : 40/300, Iter : 3500/10484,  Loss: 0.0044\n",
            "Epoch : 40/300, Iter : 4000/10484,  Loss: 0.0020\n",
            "Epoch : 40/300, Iter : 4500/10484,  Loss: 0.2799\n",
            "Epoch : 40/300, Iter : 5000/10484,  Loss: 1.3705\n",
            "Epoch : 40/300, Iter : 5500/10484,  Loss: 1.8505\n",
            "Epoch : 40/300, Iter : 6000/10484,  Loss: 0.2534\n",
            "Epoch : 40/300, Iter : 6500/10484,  Loss: 0.6636\n",
            "Epoch : 40/300, Iter : 7000/10484,  Loss: 1.7593\n",
            "Epoch : 40/300, Iter : 7500/10484,  Loss: 0.7670\n",
            "Epoch : 40/300, Iter : 8000/10484,  Loss: 0.7756\n",
            "Epoch : 40/300, Iter : 8500/10484,  Loss: 1.3538\n",
            "Epoch : 40/300, Iter : 9000/10484,  Loss: 0.0484\n",
            "Epoch : 40/300, Iter : 9500/10484,  Loss: 0.0112\n",
            "Epoch : 40/300, Iter : 10000/10484,  Loss: 0.1482\n",
            "Epoch : 41/300, Iter : 500/10484,  Loss: 0.0103\n",
            "Epoch : 41/300, Iter : 1000/10484,  Loss: 1.7855\n",
            "Epoch : 41/300, Iter : 1500/10484,  Loss: 0.6455\n",
            "Epoch : 41/300, Iter : 2000/10484,  Loss: 0.0019\n",
            "Epoch : 41/300, Iter : 2500/10484,  Loss: 1.5127\n",
            "Epoch : 41/300, Iter : 3000/10484,  Loss: 0.0000\n",
            "Epoch : 41/300, Iter : 3500/10484,  Loss: 1.8555\n",
            "Epoch : 41/300, Iter : 4000/10484,  Loss: 0.0000\n",
            "Epoch : 41/300, Iter : 4500/10484,  Loss: 0.2873\n",
            "Epoch : 41/300, Iter : 5000/10484,  Loss: 0.0010\n",
            "Epoch : 41/300, Iter : 5500/10484,  Loss: 0.6408\n",
            "Epoch : 41/300, Iter : 6000/10484,  Loss: 0.0342\n",
            "Epoch : 41/300, Iter : 6500/10484,  Loss: 0.0051\n",
            "Epoch : 41/300, Iter : 7000/10484,  Loss: 0.0976\n",
            "Epoch : 41/300, Iter : 7500/10484,  Loss: 1.1901\n",
            "Epoch : 41/300, Iter : 8000/10484,  Loss: 0.9565\n",
            "Epoch : 41/300, Iter : 8500/10484,  Loss: 0.1105\n",
            "Epoch : 41/300, Iter : 9000/10484,  Loss: 0.2246\n",
            "Epoch : 41/300, Iter : 9500/10484,  Loss: 0.0000\n",
            "Epoch : 41/300, Iter : 10000/10484,  Loss: 1.9368\n",
            "Epoch : 42/300, Iter : 500/10484,  Loss: 1.1376\n",
            "Epoch : 42/300, Iter : 1000/10484,  Loss: 0.0002\n",
            "Epoch : 42/300, Iter : 1500/10484,  Loss: 0.3866\n",
            "Epoch : 42/300, Iter : 2000/10484,  Loss: 0.2253\n",
            "Epoch : 42/300, Iter : 2500/10484,  Loss: 0.2870\n",
            "Epoch : 42/300, Iter : 3000/10484,  Loss: 0.3455\n",
            "Epoch : 42/300, Iter : 3500/10484,  Loss: 5.4955\n",
            "Epoch : 42/300, Iter : 4000/10484,  Loss: 0.5173\n",
            "Epoch : 42/300, Iter : 4500/10484,  Loss: 0.5491\n",
            "Epoch : 42/300, Iter : 5000/10484,  Loss: 0.3669\n",
            "Epoch : 42/300, Iter : 5500/10484,  Loss: 1.0784\n",
            "Epoch : 42/300, Iter : 6000/10484,  Loss: 0.0020\n",
            "Epoch : 42/300, Iter : 6500/10484,  Loss: 1.0312\n",
            "Epoch : 42/300, Iter : 7000/10484,  Loss: 2.4553\n",
            "Epoch : 42/300, Iter : 7500/10484,  Loss: 0.0464\n",
            "Epoch : 42/300, Iter : 8000/10484,  Loss: 0.0001\n",
            "Epoch : 42/300, Iter : 8500/10484,  Loss: 1.8430\n",
            "Epoch : 42/300, Iter : 9000/10484,  Loss: 0.0084\n",
            "Epoch : 42/300, Iter : 9500/10484,  Loss: 0.0006\n",
            "Epoch : 42/300, Iter : 10000/10484,  Loss: 0.8454\n",
            "Epoch : 43/300, Iter : 500/10484,  Loss: 0.6136\n",
            "Epoch : 43/300, Iter : 1000/10484,  Loss: 0.0005\n",
            "Epoch : 43/300, Iter : 1500/10484,  Loss: 0.0048\n",
            "Epoch : 43/300, Iter : 2000/10484,  Loss: 0.4894\n",
            "Epoch : 43/300, Iter : 2500/10484,  Loss: 0.0688\n",
            "Epoch : 43/300, Iter : 3000/10484,  Loss: 3.9380\n",
            "Epoch : 43/300, Iter : 3500/10484,  Loss: 0.6155\n",
            "Epoch : 43/300, Iter : 4000/10484,  Loss: 0.3417\n",
            "Epoch : 43/300, Iter : 4500/10484,  Loss: 1.8463\n",
            "Epoch : 43/300, Iter : 5000/10484,  Loss: 0.0362\n",
            "Epoch : 43/300, Iter : 5500/10484,  Loss: 1.3085\n",
            "Epoch : 43/300, Iter : 6000/10484,  Loss: 1.4266\n",
            "Epoch : 43/300, Iter : 6500/10484,  Loss: 0.1305\n",
            "Epoch : 43/300, Iter : 7000/10484,  Loss: 0.1341\n",
            "Epoch : 43/300, Iter : 7500/10484,  Loss: 0.4351\n",
            "Epoch : 43/300, Iter : 8000/10484,  Loss: 0.0019\n",
            "Epoch : 43/300, Iter : 8500/10484,  Loss: 3.0654\n",
            "Epoch : 43/300, Iter : 9000/10484,  Loss: 1.4439\n",
            "Epoch : 43/300, Iter : 9500/10484,  Loss: 0.0917\n",
            "Epoch : 43/300, Iter : 10000/10484,  Loss: 1.2352\n",
            "Epoch : 44/300, Iter : 500/10484,  Loss: 2.7727\n",
            "Epoch : 44/300, Iter : 1000/10484,  Loss: 1.0742\n",
            "Epoch : 44/300, Iter : 1500/10484,  Loss: 1.6222\n",
            "Epoch : 44/300, Iter : 2000/10484,  Loss: 0.9221\n",
            "Epoch : 44/300, Iter : 2500/10484,  Loss: 0.1717\n",
            "Epoch : 44/300, Iter : 3000/10484,  Loss: 0.0174\n",
            "Epoch : 44/300, Iter : 3500/10484,  Loss: 0.0005\n",
            "Epoch : 44/300, Iter : 4000/10484,  Loss: 0.1495\n",
            "Epoch : 44/300, Iter : 4500/10484,  Loss: 0.6387\n",
            "Epoch : 44/300, Iter : 5000/10484,  Loss: 1.8045\n",
            "Epoch : 44/300, Iter : 5500/10484,  Loss: 1.1365\n",
            "Epoch : 44/300, Iter : 6000/10484,  Loss: 0.0007\n",
            "Epoch : 44/300, Iter : 6500/10484,  Loss: 0.0523\n",
            "Epoch : 44/300, Iter : 7000/10484,  Loss: 2.7146\n",
            "Epoch : 44/300, Iter : 7500/10484,  Loss: 0.1214\n",
            "Epoch : 44/300, Iter : 8000/10484,  Loss: 0.7364\n",
            "Epoch : 44/300, Iter : 8500/10484,  Loss: 0.2091\n",
            "Epoch : 44/300, Iter : 9000/10484,  Loss: 0.8101\n",
            "Epoch : 44/300, Iter : 9500/10484,  Loss: 0.6295\n",
            "Epoch : 44/300, Iter : 10000/10484,  Loss: 0.1633\n",
            "Saved the model to /content/gdrive/My Drive/Dal Masters/CSCI 6515 - ML for Big Data/Assignment 2/Data/train_corrected/model_file_experiment4.model\n",
            "Test Accuracy of the model on the 552 test images: 71.0000 %\n",
            "Epoch : 45/300, Iter : 500/10484,  Loss: 3.1003\n",
            "Epoch : 45/300, Iter : 1000/10484,  Loss: 2.0783\n",
            "Epoch : 45/300, Iter : 1500/10484,  Loss: 0.3152\n",
            "Epoch : 45/300, Iter : 2000/10484,  Loss: 0.6525\n",
            "Epoch : 45/300, Iter : 2500/10484,  Loss: 0.0167\n",
            "Epoch : 45/300, Iter : 3000/10484,  Loss: 0.1243\n",
            "Epoch : 45/300, Iter : 3500/10484,  Loss: 0.0223\n",
            "Epoch : 45/300, Iter : 4000/10484,  Loss: 1.3848\n",
            "Epoch : 45/300, Iter : 4500/10484,  Loss: 1.4304\n",
            "Epoch : 45/300, Iter : 5000/10484,  Loss: 0.0791\n",
            "Epoch : 45/300, Iter : 5500/10484,  Loss: 0.0002\n",
            "Epoch : 45/300, Iter : 6000/10484,  Loss: 0.0007\n",
            "Epoch : 45/300, Iter : 6500/10484,  Loss: 0.0026\n",
            "Epoch : 45/300, Iter : 7000/10484,  Loss: 1.0074\n",
            "Epoch : 45/300, Iter : 7500/10484,  Loss: 3.0433\n",
            "Epoch : 45/300, Iter : 8000/10484,  Loss: 0.0387\n",
            "Epoch : 45/300, Iter : 8500/10484,  Loss: 0.0415\n",
            "Epoch : 45/300, Iter : 9000/10484,  Loss: 0.1658\n",
            "Epoch : 45/300, Iter : 9500/10484,  Loss: 3.5130\n",
            "Epoch : 45/300, Iter : 10000/10484,  Loss: 0.5507\n",
            "Epoch : 46/300, Iter : 500/10484,  Loss: 1.2003\n",
            "Epoch : 46/300, Iter : 1000/10484,  Loss: 0.0406\n",
            "Epoch : 46/300, Iter : 1500/10484,  Loss: 0.5314\n",
            "Epoch : 46/300, Iter : 2000/10484,  Loss: 0.0117\n",
            "Epoch : 46/300, Iter : 2500/10484,  Loss: 0.0822\n",
            "Epoch : 46/300, Iter : 3000/10484,  Loss: 1.9334\n",
            "Epoch : 46/300, Iter : 3500/10484,  Loss: 0.3075\n",
            "Epoch : 46/300, Iter : 4000/10484,  Loss: 1.4425\n",
            "Epoch : 46/300, Iter : 4500/10484,  Loss: 1.9604\n",
            "Epoch : 46/300, Iter : 5000/10484,  Loss: 0.3397\n",
            "Epoch : 46/300, Iter : 5500/10484,  Loss: 1.6168\n",
            "Epoch : 46/300, Iter : 6000/10484,  Loss: 0.0001\n",
            "Epoch : 46/300, Iter : 6500/10484,  Loss: 0.8391\n",
            "Epoch : 46/300, Iter : 7000/10484,  Loss: 2.5683\n",
            "Epoch : 46/300, Iter : 7500/10484,  Loss: 0.1989\n",
            "Epoch : 46/300, Iter : 8000/10484,  Loss: 2.0150\n",
            "Epoch : 46/300, Iter : 8500/10484,  Loss: 0.1255\n",
            "Epoch : 46/300, Iter : 9000/10484,  Loss: 2.9522\n",
            "Epoch : 46/300, Iter : 9500/10484,  Loss: 0.0009\n",
            "Epoch : 46/300, Iter : 10000/10484,  Loss: 0.0000\n",
            "Epoch : 47/300, Iter : 500/10484,  Loss: 0.7885\n",
            "Epoch : 47/300, Iter : 1000/10484,  Loss: 1.0704\n",
            "Epoch : 47/300, Iter : 1500/10484,  Loss: 2.5354\n",
            "Epoch : 47/300, Iter : 2000/10484,  Loss: 3.6894\n",
            "Epoch : 47/300, Iter : 2500/10484,  Loss: 0.2451\n",
            "Epoch : 47/300, Iter : 3000/10484,  Loss: 0.0947\n",
            "Epoch : 47/300, Iter : 3500/10484,  Loss: 0.0003\n",
            "Epoch : 47/300, Iter : 4000/10484,  Loss: 2.4244\n",
            "Epoch : 47/300, Iter : 4500/10484,  Loss: 0.9288\n",
            "Epoch : 47/300, Iter : 5000/10484,  Loss: 0.0125\n",
            "Epoch : 47/300, Iter : 5500/10484,  Loss: 0.0939\n",
            "Epoch : 47/300, Iter : 6000/10484,  Loss: 1.0377\n",
            "Epoch : 47/300, Iter : 6500/10484,  Loss: 0.0724\n",
            "Epoch : 47/300, Iter : 7000/10484,  Loss: 0.0006\n",
            "Epoch : 47/300, Iter : 7500/10484,  Loss: 0.0006\n",
            "Epoch : 47/300, Iter : 8000/10484,  Loss: 0.0044\n",
            "Epoch : 47/300, Iter : 8500/10484,  Loss: 0.0103\n",
            "Epoch : 47/300, Iter : 9000/10484,  Loss: 0.0001\n",
            "Epoch : 47/300, Iter : 9500/10484,  Loss: 0.0530\n",
            "Epoch : 47/300, Iter : 10000/10484,  Loss: 0.0044\n",
            "Epoch : 48/300, Iter : 500/10484,  Loss: 0.3995\n",
            "Epoch : 48/300, Iter : 1000/10484,  Loss: 0.0179\n",
            "Epoch : 48/300, Iter : 1500/10484,  Loss: 0.0426\n",
            "Epoch : 48/300, Iter : 2000/10484,  Loss: 0.8417\n",
            "Epoch : 48/300, Iter : 2500/10484,  Loss: 0.4771\n",
            "Epoch : 48/300, Iter : 3000/10484,  Loss: 0.3642\n",
            "Epoch : 48/300, Iter : 3500/10484,  Loss: 0.0001\n",
            "Epoch : 48/300, Iter : 4000/10484,  Loss: 0.0013\n",
            "Epoch : 48/300, Iter : 4500/10484,  Loss: 0.0352\n",
            "Epoch : 48/300, Iter : 5000/10484,  Loss: 0.7084\n",
            "Epoch : 48/300, Iter : 5500/10484,  Loss: 0.5781\n",
            "Epoch : 48/300, Iter : 6000/10484,  Loss: 1.8242\n",
            "Epoch : 48/300, Iter : 6500/10484,  Loss: 0.0033\n",
            "Epoch : 48/300, Iter : 7000/10484,  Loss: 0.9584\n",
            "Epoch : 48/300, Iter : 7500/10484,  Loss: 0.9055\n",
            "Epoch : 48/300, Iter : 8000/10484,  Loss: 0.3764\n",
            "Epoch : 48/300, Iter : 8500/10484,  Loss: 0.0001\n",
            "Epoch : 48/300, Iter : 9000/10484,  Loss: 0.2113\n",
            "Epoch : 48/300, Iter : 9500/10484,  Loss: 0.0000\n",
            "Epoch : 48/300, Iter : 10000/10484,  Loss: 0.3356\n",
            "Epoch : 49/300, Iter : 500/10484,  Loss: 0.0350\n",
            "Epoch : 49/300, Iter : 1000/10484,  Loss: 0.0112\n",
            "Epoch : 49/300, Iter : 1500/10484,  Loss: 0.1868\n",
            "Epoch : 49/300, Iter : 2000/10484,  Loss: 2.9886\n",
            "Epoch : 49/300, Iter : 2500/10484,  Loss: 0.0022\n",
            "Epoch : 49/300, Iter : 3000/10484,  Loss: 0.2096\n",
            "Epoch : 49/300, Iter : 3500/10484,  Loss: 0.0000\n",
            "Epoch : 49/300, Iter : 4000/10484,  Loss: 0.0021\n",
            "Epoch : 49/300, Iter : 4500/10484,  Loss: 1.9077\n",
            "Epoch : 49/300, Iter : 5000/10484,  Loss: 0.0557\n",
            "Epoch : 49/300, Iter : 5500/10484,  Loss: 1.7521\n",
            "Epoch : 49/300, Iter : 6000/10484,  Loss: 0.0272\n",
            "Epoch : 49/300, Iter : 6500/10484,  Loss: 0.1639\n",
            "Epoch : 49/300, Iter : 7000/10484,  Loss: 1.5868\n",
            "Epoch : 49/300, Iter : 7500/10484,  Loss: 0.0100\n",
            "Epoch : 49/300, Iter : 8000/10484,  Loss: 0.4658\n",
            "Epoch : 49/300, Iter : 8500/10484,  Loss: 0.2961\n",
            "Epoch : 49/300, Iter : 9000/10484,  Loss: 0.1542\n",
            "Epoch : 49/300, Iter : 9500/10484,  Loss: 0.0122\n",
            "Epoch : 49/300, Iter : 10000/10484,  Loss: 0.3932\n",
            "Test Accuracy of the model on the 552 test images: 70.0000 %\n",
            "Epoch : 50/300, Iter : 500/10484,  Loss: 0.8282\n",
            "Epoch : 50/300, Iter : 1000/10484,  Loss: 0.1685\n",
            "Epoch : 50/300, Iter : 1500/10484,  Loss: 0.1389\n",
            "Epoch : 50/300, Iter : 2000/10484,  Loss: 0.0005\n",
            "Epoch : 50/300, Iter : 2500/10484,  Loss: 3.9978\n",
            "Epoch : 50/300, Iter : 3000/10484,  Loss: 0.0008\n",
            "Epoch : 50/300, Iter : 3500/10484,  Loss: 0.0238\n",
            "Epoch : 50/300, Iter : 4000/10484,  Loss: 0.0015\n",
            "Epoch : 50/300, Iter : 4500/10484,  Loss: 0.0208\n",
            "Epoch : 50/300, Iter : 5000/10484,  Loss: 0.5351\n",
            "Epoch : 50/300, Iter : 5500/10484,  Loss: 0.0223\n",
            "Epoch : 50/300, Iter : 6000/10484,  Loss: 2.0641\n",
            "Epoch : 50/300, Iter : 6500/10484,  Loss: 0.0002\n",
            "Epoch : 50/300, Iter : 7000/10484,  Loss: 0.0002\n",
            "Epoch : 50/300, Iter : 7500/10484,  Loss: 0.3097\n",
            "Epoch : 50/300, Iter : 8000/10484,  Loss: 2.6837\n",
            "Epoch : 50/300, Iter : 8500/10484,  Loss: 0.9137\n",
            "Epoch : 50/300, Iter : 9000/10484,  Loss: 0.0032\n",
            "Epoch : 50/300, Iter : 9500/10484,  Loss: 0.4194\n",
            "Epoch : 50/300, Iter : 10000/10484,  Loss: 1.2218\n",
            "Epoch : 51/300, Iter : 500/10484,  Loss: 0.1110\n",
            "Epoch : 51/300, Iter : 1000/10484,  Loss: 2.7638\n",
            "Epoch : 51/300, Iter : 1500/10484,  Loss: 0.1424\n",
            "Epoch : 51/300, Iter : 2000/10484,  Loss: 0.0800\n",
            "Epoch : 51/300, Iter : 2500/10484,  Loss: 1.1775\n",
            "Epoch : 51/300, Iter : 3000/10484,  Loss: 0.0644\n",
            "Epoch : 51/300, Iter : 3500/10484,  Loss: 1.9611\n",
            "Epoch : 51/300, Iter : 4000/10484,  Loss: 3.4762\n",
            "Epoch : 51/300, Iter : 4500/10484,  Loss: 0.3629\n",
            "Epoch : 51/300, Iter : 5000/10484,  Loss: 0.9516\n",
            "Epoch : 51/300, Iter : 5500/10484,  Loss: 0.3678\n",
            "Epoch : 51/300, Iter : 6000/10484,  Loss: 0.0857\n",
            "Epoch : 51/300, Iter : 6500/10484,  Loss: 0.0006\n",
            "Epoch : 51/300, Iter : 7000/10484,  Loss: 0.3965\n",
            "Epoch : 51/300, Iter : 7500/10484,  Loss: 0.1783\n",
            "Epoch : 51/300, Iter : 8000/10484,  Loss: 0.1153\n",
            "Epoch : 51/300, Iter : 8500/10484,  Loss: 0.4524\n",
            "Epoch : 51/300, Iter : 9000/10484,  Loss: 0.4185\n",
            "Epoch : 51/300, Iter : 9500/10484,  Loss: 0.3381\n",
            "Epoch : 51/300, Iter : 10000/10484,  Loss: 1.4515\n",
            "Epoch : 52/300, Iter : 500/10484,  Loss: 1.4256\n",
            "Epoch : 52/300, Iter : 1000/10484,  Loss: 4.1926\n",
            "Epoch : 52/300, Iter : 1500/10484,  Loss: 0.1994\n",
            "Epoch : 52/300, Iter : 2000/10484,  Loss: 1.1902\n",
            "Epoch : 52/300, Iter : 2500/10484,  Loss: 2.1741\n",
            "Epoch : 52/300, Iter : 3000/10484,  Loss: 0.5920\n",
            "Epoch : 52/300, Iter : 3500/10484,  Loss: 0.0000\n",
            "Epoch : 52/300, Iter : 4000/10484,  Loss: 0.1875\n",
            "Epoch : 52/300, Iter : 4500/10484,  Loss: 1.3947\n",
            "Epoch : 52/300, Iter : 5000/10484,  Loss: 0.0795\n",
            "Epoch : 52/300, Iter : 5500/10484,  Loss: 0.3451\n",
            "Epoch : 52/300, Iter : 6000/10484,  Loss: 0.0769\n",
            "Epoch : 52/300, Iter : 6500/10484,  Loss: 0.0391\n",
            "Epoch : 52/300, Iter : 7000/10484,  Loss: 0.0274\n",
            "Epoch : 52/300, Iter : 7500/10484,  Loss: 0.4780\n",
            "Epoch : 52/300, Iter : 8000/10484,  Loss: 1.8323\n",
            "Epoch : 52/300, Iter : 8500/10484,  Loss: 3.7560\n",
            "Epoch : 52/300, Iter : 9000/10484,  Loss: 0.0014\n",
            "Epoch : 52/300, Iter : 9500/10484,  Loss: 0.0018\n",
            "Epoch : 52/300, Iter : 10000/10484,  Loss: 0.0042\n",
            "Epoch : 53/300, Iter : 500/10484,  Loss: 2.0547\n",
            "Epoch : 53/300, Iter : 1000/10484,  Loss: 1.6489\n",
            "Epoch : 53/300, Iter : 1500/10484,  Loss: 0.0510\n",
            "Epoch : 53/300, Iter : 2000/10484,  Loss: 0.0017\n",
            "Epoch : 53/300, Iter : 2500/10484,  Loss: 1.3582\n",
            "Epoch : 53/300, Iter : 3000/10484,  Loss: 1.3678\n",
            "Epoch : 53/300, Iter : 3500/10484,  Loss: 0.4682\n",
            "Epoch : 53/300, Iter : 4000/10484,  Loss: 0.4317\n",
            "Epoch : 53/300, Iter : 4500/10484,  Loss: 0.2888\n",
            "Epoch : 53/300, Iter : 5000/10484,  Loss: 0.0124\n",
            "Epoch : 53/300, Iter : 5500/10484,  Loss: 0.0000\n",
            "Epoch : 53/300, Iter : 6000/10484,  Loss: 0.0656\n",
            "Epoch : 53/300, Iter : 6500/10484,  Loss: 0.0002\n",
            "Epoch : 53/300, Iter : 7000/10484,  Loss: 0.1721\n",
            "Epoch : 53/300, Iter : 7500/10484,  Loss: 0.2593\n",
            "Epoch : 53/300, Iter : 8000/10484,  Loss: 0.0236\n",
            "Epoch : 53/300, Iter : 8500/10484,  Loss: 0.0003\n",
            "Epoch : 53/300, Iter : 9000/10484,  Loss: 0.0066\n",
            "Epoch : 53/300, Iter : 9500/10484,  Loss: 0.0178\n",
            "Epoch : 53/300, Iter : 10000/10484,  Loss: 0.0001\n",
            "Epoch : 54/300, Iter : 500/10484,  Loss: 0.3288\n",
            "Epoch : 54/300, Iter : 1000/10484,  Loss: 0.1152\n",
            "Epoch : 54/300, Iter : 1500/10484,  Loss: 0.0215\n",
            "Epoch : 54/300, Iter : 2000/10484,  Loss: 0.0024\n",
            "Epoch : 54/300, Iter : 2500/10484,  Loss: 0.0006\n",
            "Epoch : 54/300, Iter : 3000/10484,  Loss: 1.8994\n",
            "Epoch : 54/300, Iter : 3500/10484,  Loss: 0.2834\n",
            "Epoch : 54/300, Iter : 4000/10484,  Loss: 0.7074\n",
            "Epoch : 54/300, Iter : 4500/10484,  Loss: 0.0438\n",
            "Epoch : 54/300, Iter : 5000/10484,  Loss: 0.0001\n",
            "Epoch : 54/300, Iter : 5500/10484,  Loss: 0.2145\n",
            "Epoch : 54/300, Iter : 6000/10484,  Loss: 0.0254\n",
            "Epoch : 54/300, Iter : 6500/10484,  Loss: 0.0002\n",
            "Epoch : 54/300, Iter : 7000/10484,  Loss: 0.0000\n",
            "Epoch : 54/300, Iter : 7500/10484,  Loss: 0.0004\n",
            "Epoch : 54/300, Iter : 8000/10484,  Loss: 0.8912\n",
            "Epoch : 54/300, Iter : 8500/10484,  Loss: 0.1145\n",
            "Epoch : 54/300, Iter : 9000/10484,  Loss: 0.1318\n",
            "Epoch : 54/300, Iter : 9500/10484,  Loss: 0.2042\n",
            "Epoch : 54/300, Iter : 10000/10484,  Loss: 1.2610\n",
            "Test Accuracy of the model on the 552 test images: 70.0000 %\n",
            "Epoch : 55/300, Iter : 500/10484,  Loss: 2.4156\n",
            "Epoch : 55/300, Iter : 1000/10484,  Loss: 0.7413\n",
            "Epoch : 55/300, Iter : 1500/10484,  Loss: 0.0163\n",
            "Epoch : 55/300, Iter : 2000/10484,  Loss: 0.4159\n",
            "Epoch : 55/300, Iter : 2500/10484,  Loss: 0.3779\n",
            "Epoch : 55/300, Iter : 3000/10484,  Loss: 0.0976\n",
            "Epoch : 55/300, Iter : 3500/10484,  Loss: 0.0001\n",
            "Epoch : 55/300, Iter : 4000/10484,  Loss: 1.6383\n",
            "Epoch : 55/300, Iter : 4500/10484,  Loss: 0.8947\n",
            "Epoch : 55/300, Iter : 5000/10484,  Loss: 0.2437\n",
            "Epoch : 55/300, Iter : 5500/10484,  Loss: 0.5021\n",
            "Epoch : 55/300, Iter : 6000/10484,  Loss: 0.0110\n",
            "Epoch : 55/300, Iter : 6500/10484,  Loss: 0.7728\n",
            "Epoch : 55/300, Iter : 7000/10484,  Loss: 0.0000\n",
            "Epoch : 55/300, Iter : 7500/10484,  Loss: 1.2001\n",
            "Epoch : 55/300, Iter : 8000/10484,  Loss: 0.8806\n",
            "Epoch : 55/300, Iter : 8500/10484,  Loss: 1.6696\n",
            "Epoch : 55/300, Iter : 9000/10484,  Loss: 0.0017\n",
            "Epoch : 55/300, Iter : 9500/10484,  Loss: 0.8730\n",
            "Epoch : 55/300, Iter : 10000/10484,  Loss: 0.0013\n",
            "Epoch : 56/300, Iter : 500/10484,  Loss: 0.0253\n",
            "Epoch : 56/300, Iter : 1000/10484,  Loss: 0.0850\n",
            "Epoch : 56/300, Iter : 1500/10484,  Loss: 0.0013\n",
            "Epoch : 56/300, Iter : 2000/10484,  Loss: 5.8077\n",
            "Epoch : 56/300, Iter : 2500/10484,  Loss: 0.0635\n",
            "Epoch : 56/300, Iter : 3000/10484,  Loss: 0.2036\n",
            "Epoch : 56/300, Iter : 3500/10484,  Loss: 0.0029\n",
            "Epoch : 56/300, Iter : 4000/10484,  Loss: 0.0002\n",
            "Epoch : 56/300, Iter : 4500/10484,  Loss: 1.0310\n",
            "Epoch : 56/300, Iter : 5000/10484,  Loss: 0.0452\n",
            "Epoch : 56/300, Iter : 5500/10484,  Loss: 0.0178\n",
            "Epoch : 56/300, Iter : 6000/10484,  Loss: 0.8063\n",
            "Epoch : 56/300, Iter : 6500/10484,  Loss: 0.2214\n",
            "Epoch : 56/300, Iter : 7000/10484,  Loss: 0.0001\n",
            "Epoch : 56/300, Iter : 7500/10484,  Loss: 0.9717\n",
            "Epoch : 56/300, Iter : 8000/10484,  Loss: 0.2599\n",
            "Epoch : 56/300, Iter : 8500/10484,  Loss: 0.4403\n",
            "Epoch : 56/300, Iter : 9000/10484,  Loss: 0.0960\n",
            "Epoch : 56/300, Iter : 9500/10484,  Loss: 1.3150\n",
            "Epoch : 56/300, Iter : 10000/10484,  Loss: 0.0628\n",
            "Epoch : 57/300, Iter : 500/10484,  Loss: 0.0000\n",
            "Epoch : 57/300, Iter : 1000/10484,  Loss: 0.0021\n",
            "Epoch : 57/300, Iter : 1500/10484,  Loss: 0.1131\n",
            "Epoch : 57/300, Iter : 2000/10484,  Loss: 0.3568\n",
            "Epoch : 57/300, Iter : 2500/10484,  Loss: 3.5082\n",
            "Epoch : 57/300, Iter : 3000/10484,  Loss: 0.1165\n",
            "Epoch : 57/300, Iter : 3500/10484,  Loss: 0.9622\n",
            "Epoch : 57/300, Iter : 4000/10484,  Loss: 0.0000\n",
            "Epoch : 57/300, Iter : 4500/10484,  Loss: 0.3704\n",
            "Epoch : 57/300, Iter : 5000/10484,  Loss: 1.5828\n",
            "Epoch : 57/300, Iter : 5500/10484,  Loss: 0.0005\n",
            "Epoch : 57/300, Iter : 6000/10484,  Loss: 0.3833\n",
            "Epoch : 57/300, Iter : 6500/10484,  Loss: 0.1131\n",
            "Epoch : 57/300, Iter : 7000/10484,  Loss: 3.3365\n",
            "Epoch : 57/300, Iter : 7500/10484,  Loss: 0.7412\n",
            "Epoch : 57/300, Iter : 8000/10484,  Loss: 0.3896\n",
            "Epoch : 57/300, Iter : 8500/10484,  Loss: 3.3663\n",
            "Epoch : 57/300, Iter : 9000/10484,  Loss: 0.0014\n",
            "Epoch : 57/300, Iter : 9500/10484,  Loss: 0.2494\n",
            "Epoch : 57/300, Iter : 10000/10484,  Loss: 2.9127\n",
            "Epoch : 58/300, Iter : 500/10484,  Loss: 0.0128\n",
            "Epoch : 58/300, Iter : 1000/10484,  Loss: 0.8488\n",
            "Epoch : 58/300, Iter : 1500/10484,  Loss: 0.0041\n",
            "Epoch : 58/300, Iter : 2000/10484,  Loss: 0.7248\n",
            "Epoch : 58/300, Iter : 2500/10484,  Loss: 0.0001\n",
            "Epoch : 58/300, Iter : 3000/10484,  Loss: 0.0997\n",
            "Epoch : 58/300, Iter : 3500/10484,  Loss: 0.3393\n",
            "Epoch : 58/300, Iter : 4000/10484,  Loss: 0.0204\n",
            "Epoch : 58/300, Iter : 4500/10484,  Loss: 0.8879\n",
            "Epoch : 58/300, Iter : 5000/10484,  Loss: 0.7493\n",
            "Epoch : 58/300, Iter : 5500/10484,  Loss: 1.4818\n",
            "Epoch : 58/300, Iter : 6000/10484,  Loss: 0.4227\n",
            "Epoch : 58/300, Iter : 6500/10484,  Loss: 0.0015\n",
            "Epoch : 58/300, Iter : 7000/10484,  Loss: 0.0023\n",
            "Epoch : 58/300, Iter : 7500/10484,  Loss: 0.0035\n",
            "Epoch : 58/300, Iter : 8000/10484,  Loss: 0.1960\n",
            "Epoch : 58/300, Iter : 8500/10484,  Loss: 0.5358\n",
            "Epoch : 58/300, Iter : 9000/10484,  Loss: 0.0001\n",
            "Epoch : 58/300, Iter : 9500/10484,  Loss: 0.9515\n",
            "Epoch : 58/300, Iter : 10000/10484,  Loss: 0.0885\n",
            "Epoch : 59/300, Iter : 500/10484,  Loss: 0.3022\n",
            "Epoch : 59/300, Iter : 1000/10484,  Loss: 0.0008\n",
            "Epoch : 59/300, Iter : 1500/10484,  Loss: 0.2554\n",
            "Epoch : 59/300, Iter : 2000/10484,  Loss: 0.0001\n",
            "Epoch : 59/300, Iter : 2500/10484,  Loss: 0.5981\n",
            "Epoch : 59/300, Iter : 3000/10484,  Loss: 0.0086\n",
            "Epoch : 59/300, Iter : 3500/10484,  Loss: 0.0223\n",
            "Epoch : 59/300, Iter : 4000/10484,  Loss: 0.0136\n",
            "Epoch : 59/300, Iter : 4500/10484,  Loss: 0.1773\n",
            "Epoch : 59/300, Iter : 5000/10484,  Loss: 2.5144\n",
            "Epoch : 59/300, Iter : 5500/10484,  Loss: 0.1523\n",
            "Epoch : 59/300, Iter : 6000/10484,  Loss: 0.5492\n",
            "Epoch : 59/300, Iter : 6500/10484,  Loss: 0.2405\n",
            "Epoch : 59/300, Iter : 7000/10484,  Loss: 0.0001\n",
            "Epoch : 59/300, Iter : 7500/10484,  Loss: 1.2910\n",
            "Epoch : 59/300, Iter : 8000/10484,  Loss: 0.1154\n",
            "Epoch : 59/300, Iter : 8500/10484,  Loss: 0.5568\n",
            "Epoch : 59/300, Iter : 9000/10484,  Loss: 0.0120\n",
            "Epoch : 59/300, Iter : 9500/10484,  Loss: 1.4158\n",
            "Epoch : 59/300, Iter : 10000/10484,  Loss: 0.5978\n",
            "Test Accuracy of the model on the 552 test images: 65.0000 %\n",
            "Epoch : 60/300, Iter : 500/10484,  Loss: 0.1104\n",
            "Epoch : 60/300, Iter : 1000/10484,  Loss: 0.3687\n",
            "Epoch : 60/300, Iter : 1500/10484,  Loss: 0.2418\n",
            "Epoch : 60/300, Iter : 2000/10484,  Loss: 0.4579\n",
            "Epoch : 60/300, Iter : 2500/10484,  Loss: 0.1142\n",
            "Epoch : 60/300, Iter : 3000/10484,  Loss: 0.4787\n",
            "Epoch : 60/300, Iter : 3500/10484,  Loss: 0.0094\n",
            "Epoch : 60/300, Iter : 4000/10484,  Loss: 0.0470\n",
            "Epoch : 60/300, Iter : 4500/10484,  Loss: 1.9710\n",
            "Epoch : 60/300, Iter : 5000/10484,  Loss: 0.1218\n",
            "Epoch : 60/300, Iter : 5500/10484,  Loss: 1.0827\n",
            "Epoch : 60/300, Iter : 6000/10484,  Loss: 0.3535\n",
            "Epoch : 60/300, Iter : 6500/10484,  Loss: 1.5178\n",
            "Epoch : 60/300, Iter : 7000/10484,  Loss: 0.8318\n",
            "Epoch : 60/300, Iter : 7500/10484,  Loss: 1.8873\n",
            "Epoch : 60/300, Iter : 8000/10484,  Loss: 0.0048\n",
            "Epoch : 60/300, Iter : 8500/10484,  Loss: 0.2474\n",
            "Epoch : 60/300, Iter : 9000/10484,  Loss: 0.0000\n",
            "Epoch : 60/300, Iter : 9500/10484,  Loss: 1.1975\n",
            "Epoch : 60/300, Iter : 10000/10484,  Loss: 0.0001\n",
            "Epoch : 61/300, Iter : 500/10484,  Loss: 0.0014\n",
            "Epoch : 61/300, Iter : 1000/10484,  Loss: 0.1544\n",
            "Epoch : 61/300, Iter : 1500/10484,  Loss: 0.7163\n",
            "Epoch : 61/300, Iter : 2000/10484,  Loss: 0.0000\n",
            "Epoch : 61/300, Iter : 2500/10484,  Loss: 0.1004\n",
            "Epoch : 61/300, Iter : 3000/10484,  Loss: 0.1498\n",
            "Epoch : 61/300, Iter : 3500/10484,  Loss: 0.1587\n",
            "Epoch : 61/300, Iter : 4000/10484,  Loss: 0.1073\n",
            "Epoch : 61/300, Iter : 4500/10484,  Loss: 0.7219\n",
            "Epoch : 61/300, Iter : 5000/10484,  Loss: 0.5942\n",
            "Epoch : 61/300, Iter : 5500/10484,  Loss: 0.7139\n",
            "Epoch : 61/300, Iter : 6000/10484,  Loss: 0.7350\n",
            "Epoch : 61/300, Iter : 6500/10484,  Loss: 0.8096\n",
            "Epoch : 61/300, Iter : 7000/10484,  Loss: 1.3341\n",
            "Epoch : 61/300, Iter : 7500/10484,  Loss: 0.7178\n",
            "Epoch : 61/300, Iter : 8000/10484,  Loss: 0.8055\n",
            "Epoch : 61/300, Iter : 8500/10484,  Loss: 0.9227\n",
            "Epoch : 61/300, Iter : 9000/10484,  Loss: 2.2158\n",
            "Epoch : 61/300, Iter : 9500/10484,  Loss: 3.6694\n",
            "Epoch : 61/300, Iter : 10000/10484,  Loss: 0.0388\n",
            "Epoch : 62/300, Iter : 500/10484,  Loss: 0.5988\n",
            "Epoch : 62/300, Iter : 1000/10484,  Loss: 0.9968\n",
            "Epoch : 62/300, Iter : 1500/10484,  Loss: 1.0916\n",
            "Epoch : 62/300, Iter : 2000/10484,  Loss: 0.5905\n",
            "Epoch : 62/300, Iter : 2500/10484,  Loss: 0.0003\n",
            "Epoch : 62/300, Iter : 3000/10484,  Loss: 0.0671\n",
            "Epoch : 62/300, Iter : 3500/10484,  Loss: 0.0001\n",
            "Epoch : 62/300, Iter : 4000/10484,  Loss: 0.0016\n",
            "Epoch : 62/300, Iter : 4500/10484,  Loss: 4.6210\n",
            "Epoch : 62/300, Iter : 5000/10484,  Loss: 0.0035\n",
            "Epoch : 62/300, Iter : 5500/10484,  Loss: 0.1673\n",
            "Epoch : 62/300, Iter : 6000/10484,  Loss: 0.0025\n",
            "Epoch : 62/300, Iter : 6500/10484,  Loss: 0.2726\n",
            "Epoch : 62/300, Iter : 7000/10484,  Loss: 0.1571\n",
            "Epoch : 62/300, Iter : 7500/10484,  Loss: 1.8420\n",
            "Epoch : 62/300, Iter : 8000/10484,  Loss: 3.3401\n",
            "Epoch : 62/300, Iter : 8500/10484,  Loss: 2.3925\n",
            "Epoch : 62/300, Iter : 9000/10484,  Loss: 1.1554\n",
            "Epoch : 62/300, Iter : 9500/10484,  Loss: 0.0814\n",
            "Epoch : 62/300, Iter : 10000/10484,  Loss: 0.4386\n",
            "Epoch : 63/300, Iter : 500/10484,  Loss: 0.5699\n",
            "Epoch : 63/300, Iter : 1000/10484,  Loss: 0.0005\n",
            "Epoch : 63/300, Iter : 1500/10484,  Loss: 2.6936\n",
            "Epoch : 63/300, Iter : 2000/10484,  Loss: 0.2666\n",
            "Epoch : 63/300, Iter : 2500/10484,  Loss: 0.8758\n",
            "Epoch : 63/300, Iter : 3000/10484,  Loss: 1.7730\n",
            "Epoch : 63/300, Iter : 3500/10484,  Loss: 0.1567\n",
            "Epoch : 63/300, Iter : 4000/10484,  Loss: 0.0211\n",
            "Epoch : 63/300, Iter : 4500/10484,  Loss: 0.9830\n",
            "Epoch : 63/300, Iter : 5000/10484,  Loss: 0.7889\n",
            "Epoch : 63/300, Iter : 5500/10484,  Loss: 1.3562\n",
            "Epoch : 63/300, Iter : 6000/10484,  Loss: 2.5412\n",
            "Epoch : 63/300, Iter : 6500/10484,  Loss: 0.0001\n",
            "Epoch : 63/300, Iter : 7000/10484,  Loss: 0.4113\n",
            "Epoch : 63/300, Iter : 7500/10484,  Loss: 2.2429\n",
            "Epoch : 63/300, Iter : 8000/10484,  Loss: 1.0048\n",
            "Epoch : 63/300, Iter : 8500/10484,  Loss: 0.3493\n",
            "Epoch : 63/300, Iter : 9000/10484,  Loss: 0.4970\n",
            "Epoch : 63/300, Iter : 9500/10484,  Loss: 0.0024\n",
            "Epoch : 63/300, Iter : 10000/10484,  Loss: 0.0095\n",
            "Epoch : 64/300, Iter : 500/10484,  Loss: 0.4004\n",
            "Epoch : 64/300, Iter : 1000/10484,  Loss: 1.2420\n",
            "Epoch : 64/300, Iter : 1500/10484,  Loss: 2.2294\n",
            "Epoch : 64/300, Iter : 2000/10484,  Loss: 0.0223\n",
            "Epoch : 64/300, Iter : 2500/10484,  Loss: 0.0089\n",
            "Epoch : 64/300, Iter : 3000/10484,  Loss: 0.0838\n",
            "Epoch : 64/300, Iter : 3500/10484,  Loss: 5.6753\n",
            "Epoch : 64/300, Iter : 4000/10484,  Loss: 0.0005\n",
            "Epoch : 64/300, Iter : 4500/10484,  Loss: 1.6346\n",
            "Epoch : 64/300, Iter : 5000/10484,  Loss: 0.0000\n",
            "Epoch : 64/300, Iter : 5500/10484,  Loss: 0.0000\n",
            "Epoch : 64/300, Iter : 6000/10484,  Loss: 0.1452\n",
            "Epoch : 64/300, Iter : 6500/10484,  Loss: 0.0004\n",
            "Epoch : 64/300, Iter : 7000/10484,  Loss: 0.0705\n",
            "Epoch : 64/300, Iter : 7500/10484,  Loss: 0.0023\n",
            "Epoch : 64/300, Iter : 8000/10484,  Loss: 0.3798\n",
            "Epoch : 64/300, Iter : 8500/10484,  Loss: 1.4868\n",
            "Epoch : 64/300, Iter : 9000/10484,  Loss: 0.0000\n",
            "Epoch : 64/300, Iter : 9500/10484,  Loss: 0.0014\n",
            "Epoch : 64/300, Iter : 10000/10484,  Loss: 1.5966\n",
            "Test Accuracy of the model on the 552 test images: 67.0000 %\n",
            "Epoch : 65/300, Iter : 500/10484,  Loss: 0.0491\n",
            "Epoch : 65/300, Iter : 1000/10484,  Loss: 1.4471\n",
            "Epoch : 65/300, Iter : 1500/10484,  Loss: 0.9404\n",
            "Epoch : 65/300, Iter : 2000/10484,  Loss: 0.7414\n",
            "Epoch : 65/300, Iter : 2500/10484,  Loss: 2.2239\n",
            "Epoch : 65/300, Iter : 3000/10484,  Loss: 2.5596\n",
            "Epoch : 65/300, Iter : 3500/10484,  Loss: 0.4010\n",
            "Epoch : 65/300, Iter : 4000/10484,  Loss: 0.6269\n",
            "Epoch : 65/300, Iter : 4500/10484,  Loss: 0.0019\n",
            "Epoch : 65/300, Iter : 5000/10484,  Loss: 0.8997\n",
            "Epoch : 65/300, Iter : 5500/10484,  Loss: 0.3959\n",
            "Epoch : 65/300, Iter : 6000/10484,  Loss: 0.2975\n",
            "Epoch : 65/300, Iter : 6500/10484,  Loss: 2.6080\n",
            "Epoch : 65/300, Iter : 7000/10484,  Loss: 2.0007\n",
            "Epoch : 65/300, Iter : 7500/10484,  Loss: 0.0342\n",
            "Epoch : 65/300, Iter : 8000/10484,  Loss: 0.9394\n",
            "Epoch : 65/300, Iter : 8500/10484,  Loss: 1.1798\n",
            "Epoch : 65/300, Iter : 9000/10484,  Loss: 1.6360\n",
            "Epoch : 65/300, Iter : 9500/10484,  Loss: 0.0047\n",
            "Epoch : 65/300, Iter : 10000/10484,  Loss: 0.1396\n",
            "Epoch : 66/300, Iter : 500/10484,  Loss: 0.3133\n",
            "Epoch : 66/300, Iter : 1000/10484,  Loss: 1.3606\n",
            "Epoch : 66/300, Iter : 1500/10484,  Loss: 2.1723\n",
            "Epoch : 66/300, Iter : 2000/10484,  Loss: 0.1598\n",
            "Epoch : 66/300, Iter : 2500/10484,  Loss: 0.0009\n",
            "Epoch : 66/300, Iter : 3000/10484,  Loss: 1.1604\n",
            "Epoch : 66/300, Iter : 3500/10484,  Loss: 0.0091\n",
            "Epoch : 66/300, Iter : 4000/10484,  Loss: 0.1364\n",
            "Epoch : 66/300, Iter : 4500/10484,  Loss: 0.3685\n",
            "Epoch : 66/300, Iter : 5000/10484,  Loss: 0.3011\n",
            "Epoch : 66/300, Iter : 5500/10484,  Loss: 0.7814\n",
            "Epoch : 66/300, Iter : 6000/10484,  Loss: 0.0006\n",
            "Epoch : 66/300, Iter : 6500/10484,  Loss: 0.0001\n",
            "Epoch : 66/300, Iter : 7000/10484,  Loss: 2.1354\n",
            "Epoch : 66/300, Iter : 7500/10484,  Loss: 1.9761\n",
            "Epoch : 66/300, Iter : 8000/10484,  Loss: 0.1368\n",
            "Epoch : 66/300, Iter : 8500/10484,  Loss: 0.0000\n",
            "Epoch : 66/300, Iter : 9000/10484,  Loss: 0.0001\n",
            "Epoch : 66/300, Iter : 9500/10484,  Loss: 0.1345\n",
            "Epoch : 66/300, Iter : 10000/10484,  Loss: 0.0000\n",
            "Epoch : 67/300, Iter : 500/10484,  Loss: 0.4582\n",
            "Epoch : 67/300, Iter : 1000/10484,  Loss: 0.0001\n",
            "Epoch : 67/300, Iter : 1500/10484,  Loss: 0.0798\n",
            "Epoch : 67/300, Iter : 2000/10484,  Loss: 0.0004\n",
            "Epoch : 67/300, Iter : 2500/10484,  Loss: 0.0006\n",
            "Epoch : 67/300, Iter : 3000/10484,  Loss: 0.0008\n",
            "Epoch : 67/300, Iter : 3500/10484,  Loss: 0.0168\n",
            "Epoch : 67/300, Iter : 4000/10484,  Loss: 0.0000\n",
            "Epoch : 67/300, Iter : 4500/10484,  Loss: 0.3348\n",
            "Epoch : 67/300, Iter : 5000/10484,  Loss: 0.0000\n",
            "Epoch : 67/300, Iter : 5500/10484,  Loss: 0.1072\n",
            "Epoch : 67/300, Iter : 6000/10484,  Loss: 0.0000\n",
            "Epoch : 67/300, Iter : 6500/10484,  Loss: 1.1623\n",
            "Epoch : 67/300, Iter : 7000/10484,  Loss: 0.7875\n",
            "Epoch : 67/300, Iter : 7500/10484,  Loss: 0.0157\n",
            "Epoch : 67/300, Iter : 8000/10484,  Loss: 0.0443\n",
            "Epoch : 67/300, Iter : 8500/10484,  Loss: 0.0004\n",
            "Epoch : 67/300, Iter : 9000/10484,  Loss: 0.8060\n",
            "Epoch : 67/300, Iter : 9500/10484,  Loss: 0.1438\n",
            "Epoch : 67/300, Iter : 10000/10484,  Loss: 1.2417\n",
            "Epoch : 68/300, Iter : 500/10484,  Loss: 0.1449\n",
            "Epoch : 68/300, Iter : 1000/10484,  Loss: 0.0053\n",
            "Epoch : 68/300, Iter : 1500/10484,  Loss: 0.7241\n",
            "Epoch : 68/300, Iter : 2000/10484,  Loss: 0.2346\n",
            "Epoch : 68/300, Iter : 2500/10484,  Loss: 1.2777\n",
            "Epoch : 68/300, Iter : 3000/10484,  Loss: 0.2997\n",
            "Epoch : 68/300, Iter : 3500/10484,  Loss: 0.5907\n",
            "Epoch : 68/300, Iter : 4000/10484,  Loss: 0.5347\n",
            "Epoch : 68/300, Iter : 4500/10484,  Loss: 1.1706\n",
            "Epoch : 68/300, Iter : 5000/10484,  Loss: 0.0489\n",
            "Epoch : 68/300, Iter : 5500/10484,  Loss: 0.7904\n",
            "Epoch : 68/300, Iter : 6000/10484,  Loss: 2.1118\n",
            "Epoch : 68/300, Iter : 6500/10484,  Loss: 0.0319\n",
            "Epoch : 68/300, Iter : 7000/10484,  Loss: 0.4742\n",
            "Epoch : 68/300, Iter : 7500/10484,  Loss: 1.5871\n",
            "Epoch : 68/300, Iter : 8000/10484,  Loss: 0.6877\n",
            "Epoch : 68/300, Iter : 8500/10484,  Loss: 2.2200\n",
            "Epoch : 68/300, Iter : 9000/10484,  Loss: 0.2847\n",
            "Epoch : 68/300, Iter : 9500/10484,  Loss: 0.5259\n",
            "Epoch : 68/300, Iter : 10000/10484,  Loss: 0.0131\n",
            "Epoch : 69/300, Iter : 500/10484,  Loss: 0.2187\n",
            "Epoch : 69/300, Iter : 1000/10484,  Loss: 2.5409\n",
            "Epoch : 69/300, Iter : 1500/10484,  Loss: 0.4572\n",
            "Epoch : 69/300, Iter : 2000/10484,  Loss: 0.0220\n",
            "Epoch : 69/300, Iter : 2500/10484,  Loss: 5.6033\n",
            "Epoch : 69/300, Iter : 3000/10484,  Loss: 1.9743\n",
            "Epoch : 69/300, Iter : 3500/10484,  Loss: 0.0032\n",
            "Epoch : 69/300, Iter : 4000/10484,  Loss: 0.1251\n",
            "Epoch : 69/300, Iter : 4500/10484,  Loss: 0.0154\n",
            "Epoch : 69/300, Iter : 5000/10484,  Loss: 0.4559\n",
            "Epoch : 69/300, Iter : 5500/10484,  Loss: 0.0044\n",
            "Epoch : 69/300, Iter : 6000/10484,  Loss: 1.7410\n",
            "Epoch : 69/300, Iter : 6500/10484,  Loss: 0.9035\n",
            "Epoch : 69/300, Iter : 7000/10484,  Loss: 0.0470\n",
            "Epoch : 69/300, Iter : 7500/10484,  Loss: 0.0005\n",
            "Epoch : 69/300, Iter : 8000/10484,  Loss: 2.2026\n",
            "Epoch : 69/300, Iter : 8500/10484,  Loss: 2.0639\n",
            "Epoch : 69/300, Iter : 9000/10484,  Loss: 0.4611\n",
            "Epoch : 69/300, Iter : 9500/10484,  Loss: 0.0054\n",
            "Epoch : 69/300, Iter : 10000/10484,  Loss: 0.4200\n",
            "Test Accuracy of the model on the 552 test images: 69.0000 %\n",
            "Epoch : 70/300, Iter : 500/10484,  Loss: 0.0042\n",
            "Epoch : 70/300, Iter : 1000/10484,  Loss: 2.0549\n",
            "Epoch : 70/300, Iter : 1500/10484,  Loss: 0.0004\n",
            "Epoch : 70/300, Iter : 2000/10484,  Loss: 0.2354\n",
            "Epoch : 70/300, Iter : 2500/10484,  Loss: 0.4138\n",
            "Epoch : 70/300, Iter : 3000/10484,  Loss: 0.0003\n",
            "Epoch : 70/300, Iter : 3500/10484,  Loss: 1.5627\n",
            "Epoch : 70/300, Iter : 4000/10484,  Loss: 0.3172\n",
            "Epoch : 70/300, Iter : 4500/10484,  Loss: 0.0163\n",
            "Epoch : 70/300, Iter : 5000/10484,  Loss: 0.8619\n",
            "Epoch : 70/300, Iter : 5500/10484,  Loss: 0.0028\n",
            "Epoch : 70/300, Iter : 6000/10484,  Loss: 2.0953\n",
            "Epoch : 70/300, Iter : 6500/10484,  Loss: 0.2525\n",
            "Epoch : 70/300, Iter : 7000/10484,  Loss: 0.0004\n",
            "Epoch : 70/300, Iter : 7500/10484,  Loss: 0.1149\n",
            "Epoch : 70/300, Iter : 8000/10484,  Loss: 0.1211\n",
            "Epoch : 70/300, Iter : 8500/10484,  Loss: 0.0063\n",
            "Epoch : 70/300, Iter : 9000/10484,  Loss: 0.2324\n",
            "Epoch : 70/300, Iter : 9500/10484,  Loss: 0.0108\n",
            "Epoch : 70/300, Iter : 10000/10484,  Loss: 4.7685\n",
            "Epoch : 71/300, Iter : 500/10484,  Loss: 0.3907\n",
            "Epoch : 71/300, Iter : 1000/10484,  Loss: 0.0057\n",
            "Epoch : 71/300, Iter : 1500/10484,  Loss: 0.1747\n",
            "Epoch : 71/300, Iter : 2000/10484,  Loss: 0.5424\n",
            "Epoch : 71/300, Iter : 2500/10484,  Loss: 1.5696\n",
            "Epoch : 71/300, Iter : 3000/10484,  Loss: 0.2977\n",
            "Epoch : 71/300, Iter : 3500/10484,  Loss: 0.5577\n",
            "Epoch : 71/300, Iter : 4000/10484,  Loss: 0.5427\n",
            "Epoch : 71/300, Iter : 4500/10484,  Loss: 0.0027\n",
            "Epoch : 71/300, Iter : 5000/10484,  Loss: 0.0003\n",
            "Epoch : 71/300, Iter : 5500/10484,  Loss: 0.2084\n",
            "Epoch : 71/300, Iter : 6000/10484,  Loss: 0.0444\n",
            "Epoch : 71/300, Iter : 6500/10484,  Loss: 0.0037\n",
            "Epoch : 71/300, Iter : 7000/10484,  Loss: 2.7451\n",
            "Epoch : 71/300, Iter : 7500/10484,  Loss: 1.8921\n",
            "Epoch : 71/300, Iter : 8000/10484,  Loss: 0.0015\n",
            "Epoch : 71/300, Iter : 8500/10484,  Loss: 0.1593\n",
            "Epoch : 71/300, Iter : 9000/10484,  Loss: 0.0002\n",
            "Epoch : 71/300, Iter : 9500/10484,  Loss: 0.0773\n",
            "Epoch : 71/300, Iter : 10000/10484,  Loss: 0.0416\n",
            "Epoch : 72/300, Iter : 500/10484,  Loss: 0.0000\n",
            "Epoch : 72/300, Iter : 1000/10484,  Loss: 1.5159\n",
            "Epoch : 72/300, Iter : 1500/10484,  Loss: 1.5659\n",
            "Epoch : 72/300, Iter : 2000/10484,  Loss: 1.3351\n",
            "Epoch : 72/300, Iter : 2500/10484,  Loss: 2.0191\n",
            "Epoch : 72/300, Iter : 3000/10484,  Loss: 0.4248\n",
            "Epoch : 72/300, Iter : 3500/10484,  Loss: 1.8145\n",
            "Epoch : 72/300, Iter : 4000/10484,  Loss: 1.2775\n",
            "Epoch : 72/300, Iter : 4500/10484,  Loss: 0.2586\n",
            "Epoch : 72/300, Iter : 5000/10484,  Loss: 2.0258\n",
            "Epoch : 72/300, Iter : 5500/10484,  Loss: 0.2324\n",
            "Epoch : 72/300, Iter : 6000/10484,  Loss: 1.9612\n",
            "Epoch : 72/300, Iter : 6500/10484,  Loss: 0.0008\n",
            "Epoch : 72/300, Iter : 7000/10484,  Loss: 0.0061\n",
            "Epoch : 72/300, Iter : 7500/10484,  Loss: 0.0574\n",
            "Epoch : 72/300, Iter : 8000/10484,  Loss: 0.0000\n",
            "Epoch : 72/300, Iter : 8500/10484,  Loss: 0.0000\n",
            "Epoch : 72/300, Iter : 9000/10484,  Loss: 0.0007\n",
            "Epoch : 72/300, Iter : 9500/10484,  Loss: 0.0251\n",
            "Epoch : 72/300, Iter : 10000/10484,  Loss: 0.1529\n",
            "Epoch : 73/300, Iter : 500/10484,  Loss: 2.2811\n",
            "Epoch : 73/300, Iter : 1000/10484,  Loss: 1.9761\n",
            "Epoch : 73/300, Iter : 1500/10484,  Loss: 0.1818\n",
            "Epoch : 73/300, Iter : 2000/10484,  Loss: 0.4021\n",
            "Epoch : 73/300, Iter : 2500/10484,  Loss: 0.8303\n",
            "Epoch : 73/300, Iter : 3000/10484,  Loss: 0.0249\n",
            "Epoch : 73/300, Iter : 3500/10484,  Loss: 0.0409\n",
            "Epoch : 73/300, Iter : 4000/10484,  Loss: 0.0001\n",
            "Epoch : 73/300, Iter : 4500/10484,  Loss: 0.6328\n",
            "Epoch : 73/300, Iter : 5000/10484,  Loss: 1.6178\n",
            "Epoch : 73/300, Iter : 5500/10484,  Loss: 0.0833\n",
            "Epoch : 73/300, Iter : 6000/10484,  Loss: 0.3940\n",
            "Epoch : 73/300, Iter : 6500/10484,  Loss: 2.6601\n",
            "Epoch : 73/300, Iter : 7000/10484,  Loss: 0.0007\n",
            "Epoch : 73/300, Iter : 7500/10484,  Loss: 0.9602\n",
            "Epoch : 73/300, Iter : 8000/10484,  Loss: 0.3983\n",
            "Epoch : 73/300, Iter : 8500/10484,  Loss: 3.4441\n",
            "Epoch : 73/300, Iter : 9000/10484,  Loss: 0.1530\n",
            "Epoch : 73/300, Iter : 9500/10484,  Loss: 0.0028\n",
            "Epoch : 73/300, Iter : 10000/10484,  Loss: 0.0007\n",
            "Epoch : 74/300, Iter : 500/10484,  Loss: 0.3353\n",
            "Epoch : 74/300, Iter : 1000/10484,  Loss: 0.1348\n",
            "Epoch : 74/300, Iter : 1500/10484,  Loss: 0.0045\n",
            "Epoch : 74/300, Iter : 2000/10484,  Loss: 0.0144\n",
            "Epoch : 74/300, Iter : 2500/10484,  Loss: 0.0007\n",
            "Epoch : 74/300, Iter : 3000/10484,  Loss: 1.1562\n",
            "Epoch : 74/300, Iter : 3500/10484,  Loss: 0.0030\n",
            "Epoch : 74/300, Iter : 4000/10484,  Loss: 0.2463\n",
            "Epoch : 74/300, Iter : 4500/10484,  Loss: 0.9671\n",
            "Epoch : 74/300, Iter : 5000/10484,  Loss: 0.2428\n",
            "Epoch : 74/300, Iter : 5500/10484,  Loss: 0.3531\n",
            "Epoch : 74/300, Iter : 6000/10484,  Loss: 0.0003\n",
            "Epoch : 74/300, Iter : 6500/10484,  Loss: 0.2475\n",
            "Epoch : 74/300, Iter : 7000/10484,  Loss: 0.7941\n",
            "Epoch : 74/300, Iter : 7500/10484,  Loss: 0.0382\n",
            "Epoch : 74/300, Iter : 8000/10484,  Loss: 0.2838\n",
            "Epoch : 74/300, Iter : 8500/10484,  Loss: 3.7632\n",
            "Epoch : 74/300, Iter : 9000/10484,  Loss: 0.0067\n",
            "Epoch : 74/300, Iter : 9500/10484,  Loss: 0.3826\n",
            "Epoch : 74/300, Iter : 10000/10484,  Loss: 0.5854\n",
            "Test Accuracy of the model on the 552 test images: 70.0000 %\n",
            "Epoch : 75/300, Iter : 500/10484,  Loss: 1.1460\n",
            "Epoch : 75/300, Iter : 1000/10484,  Loss: 0.3536\n",
            "Epoch : 75/300, Iter : 1500/10484,  Loss: 0.7297\n",
            "Epoch : 75/300, Iter : 2000/10484,  Loss: 4.0640\n",
            "Epoch : 75/300, Iter : 2500/10484,  Loss: 0.0082\n",
            "Epoch : 75/300, Iter : 3000/10484,  Loss: 0.2295\n",
            "Epoch : 75/300, Iter : 3500/10484,  Loss: 0.4191\n",
            "Epoch : 75/300, Iter : 4000/10484,  Loss: 0.3652\n",
            "Epoch : 75/300, Iter : 4500/10484,  Loss: 0.0038\n",
            "Epoch : 75/300, Iter : 5000/10484,  Loss: 1.5690\n",
            "Epoch : 75/300, Iter : 5500/10484,  Loss: 0.0005\n",
            "Epoch : 75/300, Iter : 6000/10484,  Loss: 0.0841\n",
            "Epoch : 75/300, Iter : 6500/10484,  Loss: 0.0088\n",
            "Epoch : 75/300, Iter : 7000/10484,  Loss: 0.0264\n",
            "Epoch : 75/300, Iter : 7500/10484,  Loss: 1.7661\n",
            "Epoch : 75/300, Iter : 8000/10484,  Loss: 0.3196\n",
            "Epoch : 75/300, Iter : 8500/10484,  Loss: 0.1261\n",
            "Epoch : 75/300, Iter : 9000/10484,  Loss: 0.0386\n",
            "Epoch : 75/300, Iter : 9500/10484,  Loss: 0.1157\n",
            "Epoch : 75/300, Iter : 10000/10484,  Loss: 1.1205\n",
            "Epoch : 76/300, Iter : 500/10484,  Loss: 0.4440\n",
            "Epoch : 76/300, Iter : 1000/10484,  Loss: 0.2435\n",
            "Epoch : 76/300, Iter : 1500/10484,  Loss: 1.7274\n",
            "Epoch : 76/300, Iter : 2000/10484,  Loss: 0.0281\n",
            "Epoch : 76/300, Iter : 2500/10484,  Loss: 0.0424\n",
            "Epoch : 76/300, Iter : 3000/10484,  Loss: 3.2728\n",
            "Epoch : 76/300, Iter : 3500/10484,  Loss: 2.3955\n",
            "Epoch : 76/300, Iter : 4000/10484,  Loss: 0.1758\n",
            "Epoch : 76/300, Iter : 4500/10484,  Loss: 0.5785\n",
            "Epoch : 76/300, Iter : 5000/10484,  Loss: 1.0833\n",
            "Epoch : 76/300, Iter : 5500/10484,  Loss: 0.0226\n",
            "Epoch : 76/300, Iter : 6000/10484,  Loss: 0.0002\n",
            "Epoch : 76/300, Iter : 6500/10484,  Loss: 2.3788\n",
            "Epoch : 76/300, Iter : 7000/10484,  Loss: 0.7103\n",
            "Epoch : 76/300, Iter : 7500/10484,  Loss: 0.5928\n",
            "Epoch : 76/300, Iter : 8000/10484,  Loss: 2.8978\n",
            "Epoch : 76/300, Iter : 8500/10484,  Loss: 0.0011\n",
            "Epoch : 76/300, Iter : 9000/10484,  Loss: 0.0608\n",
            "Epoch : 76/300, Iter : 9500/10484,  Loss: 0.7985\n",
            "Epoch : 76/300, Iter : 10000/10484,  Loss: 1.1340\n",
            "Epoch : 77/300, Iter : 500/10484,  Loss: 0.6033\n",
            "Epoch : 77/300, Iter : 1000/10484,  Loss: 0.4537\n",
            "Epoch : 77/300, Iter : 1500/10484,  Loss: 0.6916\n",
            "Epoch : 77/300, Iter : 2000/10484,  Loss: 0.8338\n",
            "Epoch : 77/300, Iter : 2500/10484,  Loss: 0.3753\n",
            "Epoch : 77/300, Iter : 3000/10484,  Loss: 2.3011\n",
            "Epoch : 77/300, Iter : 3500/10484,  Loss: 0.0011\n",
            "Epoch : 77/300, Iter : 4000/10484,  Loss: 0.0011\n",
            "Epoch : 77/300, Iter : 4500/10484,  Loss: 1.4266\n",
            "Epoch : 77/300, Iter : 5000/10484,  Loss: 0.0157\n",
            "Epoch : 77/300, Iter : 5500/10484,  Loss: 0.0065\n",
            "Epoch : 77/300, Iter : 6000/10484,  Loss: 1.4678\n",
            "Epoch : 77/300, Iter : 6500/10484,  Loss: 0.1748\n",
            "Epoch : 77/300, Iter : 7000/10484,  Loss: 0.3266\n",
            "Epoch : 77/300, Iter : 7500/10484,  Loss: 2.8616\n",
            "Epoch : 77/300, Iter : 8000/10484,  Loss: 1.1465\n",
            "Epoch : 77/300, Iter : 8500/10484,  Loss: 0.5602\n",
            "Epoch : 77/300, Iter : 9000/10484,  Loss: 0.0048\n",
            "Epoch : 77/300, Iter : 9500/10484,  Loss: 0.0000\n",
            "Epoch : 77/300, Iter : 10000/10484,  Loss: 1.1465\n",
            "Epoch : 78/300, Iter : 500/10484,  Loss: 0.8518\n",
            "Epoch : 78/300, Iter : 1000/10484,  Loss: 0.0001\n",
            "Epoch : 78/300, Iter : 1500/10484,  Loss: 1.4706\n",
            "Epoch : 78/300, Iter : 2000/10484,  Loss: 0.0004\n",
            "Epoch : 78/300, Iter : 2500/10484,  Loss: 8.6283\n",
            "Epoch : 78/300, Iter : 3000/10484,  Loss: 1.3044\n",
            "Epoch : 78/300, Iter : 3500/10484,  Loss: 0.0166\n",
            "Epoch : 78/300, Iter : 4000/10484,  Loss: 0.0001\n",
            "Epoch : 78/300, Iter : 4500/10484,  Loss: 0.0468\n",
            "Epoch : 78/300, Iter : 5000/10484,  Loss: 0.2102\n",
            "Epoch : 78/300, Iter : 5500/10484,  Loss: 0.1293\n",
            "Epoch : 78/300, Iter : 6000/10484,  Loss: 2.0875\n",
            "Epoch : 78/300, Iter : 6500/10484,  Loss: 0.3730\n",
            "Epoch : 78/300, Iter : 7000/10484,  Loss: 0.0740\n",
            "Epoch : 78/300, Iter : 7500/10484,  Loss: 0.0178\n",
            "Epoch : 78/300, Iter : 8000/10484,  Loss: 0.1240\n",
            "Epoch : 78/300, Iter : 8500/10484,  Loss: 0.1230\n",
            "Epoch : 78/300, Iter : 9000/10484,  Loss: 0.0049\n",
            "Epoch : 78/300, Iter : 9500/10484,  Loss: 0.6305\n",
            "Epoch : 78/300, Iter : 10000/10484,  Loss: 1.3498\n",
            "Epoch : 79/300, Iter : 500/10484,  Loss: 0.9943\n",
            "Epoch : 79/300, Iter : 1000/10484,  Loss: 0.0000\n",
            "Epoch : 79/300, Iter : 1500/10484,  Loss: 0.0000\n",
            "Epoch : 79/300, Iter : 2000/10484,  Loss: 1.2004\n",
            "Epoch : 79/300, Iter : 2500/10484,  Loss: 0.0302\n",
            "Epoch : 79/300, Iter : 3000/10484,  Loss: 0.1953\n",
            "Epoch : 79/300, Iter : 3500/10484,  Loss: 0.6809\n",
            "Epoch : 79/300, Iter : 4000/10484,  Loss: 0.4025\n",
            "Epoch : 79/300, Iter : 4500/10484,  Loss: 2.1066\n",
            "Epoch : 79/300, Iter : 5000/10484,  Loss: 0.1190\n",
            "Epoch : 79/300, Iter : 5500/10484,  Loss: 0.6843\n",
            "Epoch : 79/300, Iter : 6000/10484,  Loss: 0.8022\n",
            "Epoch : 79/300, Iter : 6500/10484,  Loss: 0.0034\n",
            "Epoch : 79/300, Iter : 7000/10484,  Loss: 0.0094\n",
            "Epoch : 79/300, Iter : 7500/10484,  Loss: 1.9997\n",
            "Epoch : 79/300, Iter : 8000/10484,  Loss: 0.0037\n",
            "Epoch : 79/300, Iter : 8500/10484,  Loss: 0.3999\n",
            "Epoch : 79/300, Iter : 9000/10484,  Loss: 1.7930\n",
            "Epoch : 79/300, Iter : 9500/10484,  Loss: 1.5168\n",
            "Epoch : 79/300, Iter : 10000/10484,  Loss: 0.0505\n",
            "Test Accuracy of the model on the 552 test images: 69.0000 %\n",
            "Epoch : 80/300, Iter : 500/10484,  Loss: 0.0230\n",
            "Epoch : 80/300, Iter : 1000/10484,  Loss: 0.1254\n",
            "Epoch : 80/300, Iter : 1500/10484,  Loss: 0.0597\n",
            "Epoch : 80/300, Iter : 2000/10484,  Loss: 0.0918\n",
            "Epoch : 80/300, Iter : 2500/10484,  Loss: 0.0001\n",
            "Epoch : 80/300, Iter : 3000/10484,  Loss: 0.5508\n",
            "Epoch : 80/300, Iter : 3500/10484,  Loss: 0.0081\n",
            "Epoch : 80/300, Iter : 4000/10484,  Loss: 0.0732\n",
            "Epoch : 80/300, Iter : 4500/10484,  Loss: 2.2993\n",
            "Epoch : 80/300, Iter : 5000/10484,  Loss: 0.2672\n",
            "Epoch : 80/300, Iter : 5500/10484,  Loss: 0.0369\n",
            "Epoch : 80/300, Iter : 6000/10484,  Loss: 0.1999\n",
            "Epoch : 80/300, Iter : 6500/10484,  Loss: 0.1902\n",
            "Epoch : 80/300, Iter : 7000/10484,  Loss: 0.3235\n",
            "Epoch : 80/300, Iter : 7500/10484,  Loss: 0.1962\n",
            "Epoch : 80/300, Iter : 8000/10484,  Loss: 2.0528\n",
            "Epoch : 80/300, Iter : 8500/10484,  Loss: 0.0163\n",
            "Epoch : 80/300, Iter : 9000/10484,  Loss: 0.0062\n",
            "Epoch : 80/300, Iter : 9500/10484,  Loss: 0.0005\n",
            "Epoch : 80/300, Iter : 10000/10484,  Loss: 3.1859\n",
            "Epoch : 81/300, Iter : 500/10484,  Loss: 0.0005\n",
            "Epoch : 81/300, Iter : 1000/10484,  Loss: 0.0009\n",
            "Epoch : 81/300, Iter : 1500/10484,  Loss: 0.0001\n",
            "Epoch : 81/300, Iter : 2000/10484,  Loss: 0.0221\n",
            "Epoch : 81/300, Iter : 2500/10484,  Loss: 0.0029\n",
            "Epoch : 81/300, Iter : 3000/10484,  Loss: 0.2007\n",
            "Epoch : 81/300, Iter : 3500/10484,  Loss: 0.1571\n",
            "Epoch : 81/300, Iter : 4000/10484,  Loss: 0.0045\n",
            "Epoch : 81/300, Iter : 4500/10484,  Loss: 0.0030\n",
            "Epoch : 81/300, Iter : 5000/10484,  Loss: 0.8981\n",
            "Epoch : 81/300, Iter : 5500/10484,  Loss: 1.4110\n",
            "Epoch : 81/300, Iter : 6000/10484,  Loss: 0.0645\n",
            "Epoch : 81/300, Iter : 6500/10484,  Loss: 0.0037\n",
            "Epoch : 81/300, Iter : 7000/10484,  Loss: 0.0005\n",
            "Epoch : 81/300, Iter : 7500/10484,  Loss: 1.0581\n",
            "Epoch : 81/300, Iter : 8000/10484,  Loss: 0.0262\n",
            "Epoch : 81/300, Iter : 8500/10484,  Loss: 0.0005\n",
            "Epoch : 81/300, Iter : 9000/10484,  Loss: 0.0007\n",
            "Epoch : 81/300, Iter : 9500/10484,  Loss: 0.0119\n",
            "Epoch : 81/300, Iter : 10000/10484,  Loss: 0.0216\n",
            "Epoch : 82/300, Iter : 500/10484,  Loss: 1.0357\n",
            "Epoch : 82/300, Iter : 1000/10484,  Loss: 0.0537\n",
            "Epoch : 82/300, Iter : 1500/10484,  Loss: 0.1036\n",
            "Epoch : 82/300, Iter : 2000/10484,  Loss: 0.3241\n",
            "Epoch : 82/300, Iter : 2500/10484,  Loss: 0.0001\n",
            "Epoch : 82/300, Iter : 3000/10484,  Loss: 1.8379\n",
            "Epoch : 82/300, Iter : 3500/10484,  Loss: 0.9608\n",
            "Epoch : 82/300, Iter : 4000/10484,  Loss: 0.0006\n",
            "Epoch : 82/300, Iter : 4500/10484,  Loss: 0.0003\n",
            "Epoch : 82/300, Iter : 5000/10484,  Loss: 3.2575\n",
            "Epoch : 82/300, Iter : 5500/10484,  Loss: 0.5913\n",
            "Epoch : 82/300, Iter : 6000/10484,  Loss: 0.0007\n",
            "Epoch : 82/300, Iter : 6500/10484,  Loss: 2.2806\n",
            "Epoch : 82/300, Iter : 7000/10484,  Loss: 0.2347\n",
            "Epoch : 82/300, Iter : 7500/10484,  Loss: 0.5842\n",
            "Epoch : 82/300, Iter : 8000/10484,  Loss: 0.4274\n",
            "Epoch : 82/300, Iter : 8500/10484,  Loss: 0.5521\n",
            "Epoch : 82/300, Iter : 9000/10484,  Loss: 0.0966\n",
            "Epoch : 82/300, Iter : 9500/10484,  Loss: 1.7625\n",
            "Epoch : 82/300, Iter : 10000/10484,  Loss: 0.9062\n",
            "Epoch : 83/300, Iter : 500/10484,  Loss: 0.0233\n",
            "Epoch : 83/300, Iter : 1000/10484,  Loss: 0.1318\n",
            "Epoch : 83/300, Iter : 1500/10484,  Loss: 0.6154\n",
            "Epoch : 83/300, Iter : 2000/10484,  Loss: 1.6734\n",
            "Epoch : 83/300, Iter : 2500/10484,  Loss: 1.8880\n",
            "Epoch : 83/300, Iter : 3000/10484,  Loss: 0.5153\n",
            "Epoch : 83/300, Iter : 3500/10484,  Loss: 0.0749\n",
            "Epoch : 83/300, Iter : 4000/10484,  Loss: 0.0535\n",
            "Epoch : 83/300, Iter : 4500/10484,  Loss: 0.7584\n",
            "Epoch : 83/300, Iter : 5000/10484,  Loss: 0.1252\n",
            "Epoch : 83/300, Iter : 5500/10484,  Loss: 0.0002\n",
            "Epoch : 83/300, Iter : 6000/10484,  Loss: 0.0163\n",
            "Epoch : 83/300, Iter : 6500/10484,  Loss: 0.0210\n",
            "Epoch : 83/300, Iter : 7000/10484,  Loss: 0.6081\n",
            "Epoch : 83/300, Iter : 7500/10484,  Loss: 0.0033\n",
            "Epoch : 83/300, Iter : 8000/10484,  Loss: 0.5262\n",
            "Epoch : 83/300, Iter : 8500/10484,  Loss: 0.4219\n",
            "Epoch : 83/300, Iter : 9000/10484,  Loss: 0.0522\n",
            "Epoch : 83/300, Iter : 9500/10484,  Loss: 1.7449\n",
            "Epoch : 83/300, Iter : 10000/10484,  Loss: 0.0001\n",
            "Epoch : 84/300, Iter : 500/10484,  Loss: 1.5741\n",
            "Epoch : 84/300, Iter : 1000/10484,  Loss: 0.2111\n",
            "Epoch : 84/300, Iter : 1500/10484,  Loss: 2.7410\n",
            "Epoch : 84/300, Iter : 2000/10484,  Loss: 0.0000\n",
            "Epoch : 84/300, Iter : 2500/10484,  Loss: 0.0000\n",
            "Epoch : 84/300, Iter : 3000/10484,  Loss: 0.6219\n",
            "Epoch : 84/300, Iter : 3500/10484,  Loss: 2.0709\n",
            "Epoch : 84/300, Iter : 4000/10484,  Loss: 0.4013\n",
            "Epoch : 84/300, Iter : 4500/10484,  Loss: 0.8161\n",
            "Epoch : 84/300, Iter : 5000/10484,  Loss: 0.3755\n",
            "Epoch : 84/300, Iter : 5500/10484,  Loss: 0.0699\n",
            "Epoch : 84/300, Iter : 6000/10484,  Loss: 0.0572\n",
            "Epoch : 84/300, Iter : 6500/10484,  Loss: 0.5083\n",
            "Epoch : 84/300, Iter : 7000/10484,  Loss: 0.0009\n",
            "Epoch : 84/300, Iter : 7500/10484,  Loss: 0.0526\n",
            "Epoch : 84/300, Iter : 8000/10484,  Loss: 3.2772\n",
            "Epoch : 84/300, Iter : 8500/10484,  Loss: 0.0667\n",
            "Epoch : 84/300, Iter : 9000/10484,  Loss: 1.9827\n",
            "Epoch : 84/300, Iter : 9500/10484,  Loss: 0.2634\n",
            "Epoch : 84/300, Iter : 10000/10484,  Loss: 0.2528\n",
            "Test Accuracy of the model on the 552 test images: 71.0000 %\n",
            "Epoch : 85/300, Iter : 500/10484,  Loss: 0.0001\n",
            "Epoch : 85/300, Iter : 1000/10484,  Loss: 0.0000\n",
            "Epoch : 85/300, Iter : 1500/10484,  Loss: 0.0002\n",
            "Epoch : 85/300, Iter : 2000/10484,  Loss: 0.4262\n",
            "Epoch : 85/300, Iter : 2500/10484,  Loss: 0.5937\n",
            "Epoch : 85/300, Iter : 3000/10484,  Loss: 1.7732\n",
            "Epoch : 85/300, Iter : 3500/10484,  Loss: 1.2528\n",
            "Epoch : 85/300, Iter : 4000/10484,  Loss: 0.0024\n",
            "Epoch : 85/300, Iter : 4500/10484,  Loss: 0.1955\n",
            "Epoch : 85/300, Iter : 5000/10484,  Loss: 0.0012\n",
            "Epoch : 85/300, Iter : 5500/10484,  Loss: 2.3589\n",
            "Epoch : 85/300, Iter : 6000/10484,  Loss: 0.0003\n",
            "Epoch : 85/300, Iter : 6500/10484,  Loss: 0.8065\n",
            "Epoch : 85/300, Iter : 7000/10484,  Loss: 0.0182\n",
            "Epoch : 85/300, Iter : 7500/10484,  Loss: 0.0666\n",
            "Epoch : 85/300, Iter : 8000/10484,  Loss: 0.1653\n",
            "Epoch : 85/300, Iter : 8500/10484,  Loss: 0.6883\n",
            "Epoch : 85/300, Iter : 9000/10484,  Loss: 0.1101\n",
            "Epoch : 85/300, Iter : 9500/10484,  Loss: 0.0001\n",
            "Epoch : 85/300, Iter : 10000/10484,  Loss: 2.3797\n",
            "Epoch : 86/300, Iter : 500/10484,  Loss: 0.3623\n",
            "Epoch : 86/300, Iter : 1000/10484,  Loss: 0.0028\n",
            "Epoch : 86/300, Iter : 1500/10484,  Loss: 0.1935\n",
            "Epoch : 86/300, Iter : 2000/10484,  Loss: 0.4691\n",
            "Epoch : 86/300, Iter : 2500/10484,  Loss: 0.2828\n",
            "Epoch : 86/300, Iter : 3000/10484,  Loss: 3.4691\n",
            "Epoch : 86/300, Iter : 3500/10484,  Loss: 1.8510\n",
            "Epoch : 86/300, Iter : 4000/10484,  Loss: 1.2354\n",
            "Epoch : 86/300, Iter : 4500/10484,  Loss: 0.3947\n",
            "Epoch : 86/300, Iter : 5000/10484,  Loss: 0.1203\n",
            "Epoch : 86/300, Iter : 5500/10484,  Loss: 1.1853\n",
            "Epoch : 86/300, Iter : 6000/10484,  Loss: 0.0780\n",
            "Epoch : 86/300, Iter : 6500/10484,  Loss: 1.6209\n",
            "Epoch : 86/300, Iter : 7000/10484,  Loss: 1.3249\n",
            "Epoch : 86/300, Iter : 7500/10484,  Loss: 0.2734\n",
            "Epoch : 86/300, Iter : 8000/10484,  Loss: 0.0051\n",
            "Epoch : 86/300, Iter : 8500/10484,  Loss: 0.5096\n",
            "Epoch : 86/300, Iter : 9000/10484,  Loss: 0.1499\n",
            "Epoch : 86/300, Iter : 9500/10484,  Loss: 3.9912\n",
            "Epoch : 86/300, Iter : 10000/10484,  Loss: 0.5994\n",
            "Epoch : 87/300, Iter : 500/10484,  Loss: 0.4805\n",
            "Epoch : 87/300, Iter : 1000/10484,  Loss: 2.7629\n",
            "Epoch : 87/300, Iter : 1500/10484,  Loss: 1.2989\n",
            "Epoch : 87/300, Iter : 2000/10484,  Loss: 0.0262\n",
            "Epoch : 87/300, Iter : 2500/10484,  Loss: 0.0000\n",
            "Epoch : 87/300, Iter : 3000/10484,  Loss: 0.0034\n",
            "Epoch : 87/300, Iter : 3500/10484,  Loss: 0.0004\n",
            "Epoch : 87/300, Iter : 4000/10484,  Loss: 0.0062\n",
            "Epoch : 87/300, Iter : 4500/10484,  Loss: 0.3448\n",
            "Epoch : 87/300, Iter : 5000/10484,  Loss: 0.8044\n",
            "Epoch : 87/300, Iter : 5500/10484,  Loss: 0.3931\n",
            "Epoch : 87/300, Iter : 6000/10484,  Loss: 0.5913\n",
            "Epoch : 87/300, Iter : 6500/10484,  Loss: 0.1286\n",
            "Epoch : 87/300, Iter : 7000/10484,  Loss: 0.0725\n",
            "Epoch : 87/300, Iter : 7500/10484,  Loss: 0.8935\n",
            "Epoch : 87/300, Iter : 8000/10484,  Loss: 0.2510\n",
            "Epoch : 87/300, Iter : 8500/10484,  Loss: 0.0000\n",
            "Epoch : 87/300, Iter : 9000/10484,  Loss: 0.0548\n",
            "Epoch : 87/300, Iter : 9500/10484,  Loss: 0.8558\n",
            "Epoch : 87/300, Iter : 10000/10484,  Loss: 0.0031\n",
            "Epoch : 88/300, Iter : 500/10484,  Loss: 0.0000\n",
            "Epoch : 88/300, Iter : 1000/10484,  Loss: 0.3259\n",
            "Epoch : 88/300, Iter : 1500/10484,  Loss: 1.6478\n",
            "Epoch : 88/300, Iter : 2000/10484,  Loss: 1.5250\n",
            "Epoch : 88/300, Iter : 2500/10484,  Loss: 0.0283\n",
            "Epoch : 88/300, Iter : 3000/10484,  Loss: 0.0057\n",
            "Epoch : 88/300, Iter : 3500/10484,  Loss: 0.2929\n",
            "Epoch : 88/300, Iter : 4000/10484,  Loss: 0.0146\n",
            "Epoch : 88/300, Iter : 4500/10484,  Loss: 0.0021\n",
            "Epoch : 88/300, Iter : 5000/10484,  Loss: 0.0006\n",
            "Epoch : 88/300, Iter : 5500/10484,  Loss: 0.2323\n",
            "Epoch : 88/300, Iter : 6000/10484,  Loss: 0.0221\n",
            "Epoch : 88/300, Iter : 6500/10484,  Loss: 1.3115\n",
            "Epoch : 88/300, Iter : 7000/10484,  Loss: 0.0013\n",
            "Epoch : 88/300, Iter : 7500/10484,  Loss: 0.0068\n",
            "Epoch : 88/300, Iter : 8000/10484,  Loss: 4.4098\n",
            "Epoch : 88/300, Iter : 8500/10484,  Loss: 1.0084\n",
            "Epoch : 88/300, Iter : 9000/10484,  Loss: 1.1386\n",
            "Epoch : 88/300, Iter : 9500/10484,  Loss: 0.0044\n",
            "Epoch : 88/300, Iter : 10000/10484,  Loss: 1.1543\n",
            "Epoch : 89/300, Iter : 500/10484,  Loss: 0.0583\n",
            "Epoch : 89/300, Iter : 1000/10484,  Loss: 0.0010\n",
            "Epoch : 89/300, Iter : 1500/10484,  Loss: 0.0000\n",
            "Epoch : 89/300, Iter : 2000/10484,  Loss: 0.2073\n",
            "Epoch : 89/300, Iter : 2500/10484,  Loss: 0.3357\n",
            "Epoch : 89/300, Iter : 3000/10484,  Loss: 0.1685\n",
            "Epoch : 89/300, Iter : 3500/10484,  Loss: 0.0005\n",
            "Epoch : 89/300, Iter : 4000/10484,  Loss: 0.2643\n",
            "Epoch : 89/300, Iter : 4500/10484,  Loss: 0.8804\n",
            "Epoch : 89/300, Iter : 5000/10484,  Loss: 0.0194\n",
            "Epoch : 89/300, Iter : 5500/10484,  Loss: 0.5430\n",
            "Epoch : 89/300, Iter : 6000/10484,  Loss: 0.0384\n",
            "Epoch : 89/300, Iter : 6500/10484,  Loss: 0.1209\n",
            "Epoch : 89/300, Iter : 7000/10484,  Loss: 0.1652\n",
            "Epoch : 89/300, Iter : 7500/10484,  Loss: 0.4453\n",
            "Epoch : 89/300, Iter : 8000/10484,  Loss: 0.1351\n",
            "Epoch : 89/300, Iter : 8500/10484,  Loss: 0.9456\n",
            "Epoch : 89/300, Iter : 9000/10484,  Loss: 0.9199\n",
            "Epoch : 89/300, Iter : 9500/10484,  Loss: 0.3410\n",
            "Epoch : 89/300, Iter : 10000/10484,  Loss: 1.0901\n",
            "Test Accuracy of the model on the 552 test images: 70.0000 %\n",
            "Epoch : 90/300, Iter : 500/10484,  Loss: 0.5965\n",
            "Epoch : 90/300, Iter : 1000/10484,  Loss: 0.4396\n",
            "Epoch : 90/300, Iter : 1500/10484,  Loss: 0.0196\n",
            "Epoch : 90/300, Iter : 2000/10484,  Loss: 0.0193\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-c2fe57a3644f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "DSVlzWTMvgZO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model_save_path = file_dir + \"model_file_\" + str(find_edges) + \"_1.model\"\n",
        "model_save_path = file_dir + \"model_file_experiment\"  + str(model_extra_char) +\"_1.model\"\n",
        "torch.save(cnn, model_save_path)\n",
        "print(\"Saved the model to \" + model_save_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zUm4UsnDxt_E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "car_type_test = CarTypeDataset(pd_dataframe=test,\n",
        "                                    root_dir=img_dir,\n",
        "                                    transform = data_transform,\n",
        "                                     sq_image = sq_image, \n",
        "                                    find_edges = find_edges\n",
        "                                    )\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(car_type_test,\n",
        "                                             batch_size=batch_size, shuffle=True,\n",
        "                                             num_workers=4)\n",
        "\n",
        "cnn.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "for i, this_loader in enumerate(test_loader):\n",
        "    images = Variable(this_loader[\"image\"])\n",
        "    outputs = cnn(images)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += this_loader[\"label\"].size(0)\n",
        "    correct += (predicted == this_loader[\"label\"]).sum()\n",
        "print('Test Accuracy of the model on the 10000 test images: %.4f %%' % (100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "usEoTQQA0j-v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "outputId": "677bc960-f943-4091-e79c-2ee821e989c6"
      },
      "cell_type": "code",
      "source": [
        "print(len(losses))\n",
        "# losses_in_epochs = losses[0::60]\n",
        "losses_in_epochs = losses\n",
        "# plt.xkcd();\n",
        "plt.rcdefaults()\n",
        "plt.figure();\n",
        "plt.title(\"Experiment \" + str(model_extra_char))\n",
        "plt.xlabel('Iterations #');\n",
        "plt.ylabel('Loss');\n",
        "plt.plot(losses_in_epochs);\n",
        "plt.show();"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "935498\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkMAAAHHCAYAAAC88FzIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUrS/AC40UFBZCpsKAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f1857d9bc50>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}